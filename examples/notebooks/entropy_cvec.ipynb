{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entropy Based Sampling With Uncertainty Tokens And Control Vectors\n",
    "Tested on T4 GPU instance :-)\n",
    "\n",
    "This notebook contains a simple example for entropy based sampling with [hfppl](https://github.com/probcomp/hfppl) and control vector steering with the [repeng]() library. \n",
    "\n",
    "The prompt and uncertainty tokens are configurable, as well as the temperature, the number of iterations `smc_steer` is run, the beam factor, the number of particles, and an entropy threshold. There is also an option to trigger control vectors when the assistant outputs a specific token, but be default the control vector is applied after the uncertainty token is inserted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "! git clone https://github.com/genlm/llamppl\n",
    "! cd hfppl && pip install .\n",
    "! pip install repeng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports, Constants, and Utils\n",
    "\n",
    "Below are imports needed, as well as some constants for text coloring and a pretty format method for easier visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from scipy.stats import entropy\n",
    "from scipy.spatial.distance import cosine\n",
    "import torch\n",
    "from typing import List, Set\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import asyncio\n",
    "from llamppl import smc_steer\n",
    "from llamppl import Model, LMContext, CachedCausalLM, TokenCategorical, Token\n",
    "from llamppl import log_softmax\n",
    "from llamppl import sample_word\n",
    "\n",
    "import re\n",
    "\n",
    "import json\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from repeng import ControlVector, ControlModel, DatasetEntry\n",
    "\n",
    "GREEN = \"\\033[92m\"\n",
    "RED = \"\\033[91m\"\n",
    "YELLOW = \"\\033[93m\"\n",
    "END = \"\\033[0m\"\n",
    "\n",
    "def pretty_format(particle):\n",
    "    context_str = str(particle.context)\n",
    "    new_context_str = re.sub(f\"({re.escape(uncertainty_token)})\", f\"{YELLOW}\\\\1{END}\", context_str)\n",
    "    return f\"{new_context_str} (weight: {RED}{particle.weight:.3f}{END})\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "\n",
    "Here are the configurable pieces of this notebook:\n",
    "\n",
    "- prompt: the prompt given to the LLM\n",
    "\n",
    "- uncertainty_token: the token to inject to induce backtracking when entropy is high - (eventually I will update this to take multiple tokens, it's diverged a little since that was part of this implementation but is very doable)\n",
    "\n",
    "- NUM_PARTICLES: number of particles to use for SMC - this is the number of different outputs you will see below\n",
    "\n",
    "- NUM_BEAMS: the beam factor used in `smc_steer`, increasing this extends the beam search replication of the SMC steering\n",
    "\n",
    "- ENTROPY_THRESHOLD: this is a floating point number > 0.0 which determines the threshold to trigger entropy after it exceeds this value. Values under 4.0 trigger more commonly, and values above 4.0 trigger much less often. I've found 2.5-3.0 is a good range for Llama-3.2-1B, though I'd love to implement variation in entropy (varentropy) as a higher confidence measure.\n",
    "\n",
    "- MAX_TOKENS: the maximum number of tokens to generate\n",
    "\n",
    "- ITERATIONS: how many times to run `smc_steer` in a single call (you probably won't change this one, was used for some eval scripts I have set up)\n",
    "\n",
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_PARTICLES = 3\n",
    "BEAM_FACTOR = 3\n",
    "ENTROPY_THRESHOLD = 2.5\n",
    "MAX_TOKENS = 200\n",
    "ITERATIONS = 1\n",
    "\n",
    "USE_TOKEN_TRIGGER = False\n",
    "TOKEN_TRIGGER = \"?\"\n",
    "\n",
    "TEMPERATURE = 1.0\n",
    "CVEC_TOKENS = 10\n",
    "CVEC_STRENGTH = 1.2\n",
    "\n",
    "prompt = \"Which number is larger, 9.9 or 9.11?\"\n",
    "uncertainty_token = \"wait...\"\n",
    "\n",
    "POSITIVE_PERSONA = \"thoughtful and genuine in your responses\"\n",
    "NEGATIVE_PERSONA = \"superficial and empty in your responses\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition\n",
    "- LLamPPL model override for entropy based sampling and steering\n",
    "- I'm including use of the `score` method here, though, to be honest I'm not totally sure if this is working or how."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NightwingEntropyControlModel(Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        lm: CachedCausalLM,\n",
    "        prompt: str,\n",
    "        uncertainty_token: str = \"Wait...\",\n",
    "        temperature: float = 1.0,\n",
    "        entropy_threshold: float = 3.0,\n",
    "        max_tokens: int = 100,\n",
    "        min_tokens_between_uncertainty: int = 50,\n",
    "        cvec_tokens: int = 10,\n",
    "        cvec_strength: float = 1.2,\n",
    "        use_token_trigger = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        print(f\"\\nInitializing model...\")\n",
    "\n",
    "        self.lm = lm\n",
    "        self.context = LMContext(lm, prompt)\n",
    "        self.uncertainty_tokens = self.lm.tokenizer.encode(\n",
    "            f\" {uncertainty_token}\", # Encode uncertainty token with a space in front\n",
    "            add_special_tokens=False\n",
    "        )\n",
    "        self.entropy_threshold = entropy_threshold\n",
    "        self.max_tokens = max_tokens\n",
    "\n",
    "        # Tracking uncertainty\n",
    "        self.min_tokens_between_uncertainty = min_tokens_between_uncertainty\n",
    "        self.generated_tokens = []\n",
    "        self.last_uncertainty_pos = -self.min_tokens_between_uncertainty\n",
    "        self.is_generating_uncertainty = False\n",
    "        self.is_generating_cvec = False\n",
    "        self.control_vector = self.train_control_vector(lm)\n",
    "        self.use_token_trigger = use_token_trigger\n",
    "        self.cvec_tokens = cvec_tokens\n",
    "        self.cvec_strength = cvec_strength\n",
    "\n",
    "    # Entropy calculation with LSE and normalization\n",
    "    def calculate_entropy(self, logprobs: np.ndarray) -> float:\n",
    "        probs = np.exp(logprobs - np.max(logprobs)) # log-sum-exp\n",
    "        probs = probs / np.sum(probs) # Normalize\n",
    "\n",
    "        return float(-np.sum(probs * np.log(probs + 1e-10))) # Add constant to avoid taking log(0)\n",
    "\n",
    "    # Step method for SMC\n",
    "    async def step(self):\n",
    "        if len(self.generated_tokens) >= self.max_tokens:\n",
    "            self.finish()\n",
    "            return\n",
    "\n",
    "        # Current position in generated output\n",
    "        current_pos = len(self.generated_tokens)\n",
    "        \n",
    "        # If we aren't currently inserting uncertainty tokens\n",
    "        # And we're outside of the minimum distance to insert tokens\n",
    "        if (not self.is_generating_uncertainty and \n",
    "            current_pos - self.last_uncertainty_pos >= self.min_tokens_between_uncertainty):\n",
    "            \n",
    "            # Calculate entropy values\n",
    "            logprobs = await self.lm.next_token_logprobs(self.context.tokens)\n",
    "            current_entropy = self.calculate_entropy(logprobs)\n",
    "            \n",
    "            # If entropy is above the threshold and it's not the first token\n",
    "            if current_entropy > self.entropy_threshold and current_pos != 0:\n",
    "                self.is_generating_uncertainty = True\n",
    "                self.last_uncertainty_pos = current_pos\n",
    "\n",
    "        # Observe the uncertainty token\n",
    "        if self.is_generating_uncertainty:\n",
    "            if len(self.generated_tokens) - self.last_uncertainty_pos < len(self.uncertainty_tokens):\n",
    "\n",
    "                # Create token\n",
    "                token_idx = len(self.generated_tokens) - self.last_uncertainty_pos\n",
    "                token = Token(self.lm, self.uncertainty_tokens[token_idx], self.lm.tokenizer.decode(self.uncertainty_tokens[token_idx]))\n",
    "\n",
    "                # Observe the token over the next token distribution\n",
    "                next_dist = self.context.next_token()\n",
    "                await self.observe(next_dist, token)\n",
    "                self.score(0.2)\n",
    "\n",
    "                self.generated_tokens.append(token)\n",
    "\n",
    "                if token_idx == len(self.uncertainty_tokens) - 1:\n",
    "                    self.is_generating_uncertainty = False\n",
    "\n",
    "                if not self.use_token_trigger:\n",
    "                    self.is_generating_cvec = True\n",
    "            return\n",
    "\n",
    "        if self.is_generating_cvec:\n",
    "            self.lm.model.set_control(self.control_vector, self.cvec_strength)\n",
    "            tokens_remaining = self.cvec_tokens\n",
    "            while tokens_remaining > 0:\n",
    "                next_token = self.context.next_token()\n",
    "                await self.sample(next_token)\n",
    "                tokens_remaining -= 1\n",
    "            #self.condition(tokens_remaining == 0)\n",
    "            self.lm.model.set_control(self.control_vector, 0.0)  \n",
    "            self.is_generating_cvec = False\n",
    "            return      \n",
    "\n",
    "        # Normal sampling behavior\n",
    "        next_dist = self.context.next_token()\n",
    "        token = await self.sample(next_dist)\n",
    "        self.score(1.0)\n",
    "        self.generated_tokens.append(token)\n",
    "\n",
    "        if token.token_id == self.lm.tokenizer.eos_token_id:\n",
    "            self.finish()\n",
    "\n",
    "    def make_dataset(self, template: str, pos_personas: list[str], neg_personas: list[str], suffixes: list[str]):\n",
    "        dataset = []\n",
    "        user_tag, asst_tag = \"\", \"\"\n",
    "        for suffix in suffixes:\n",
    "            for positive_persona, negative_persona in zip(pos_personas, neg_personas):\n",
    "                positive_template = template.format(persona=positive_persona)\n",
    "                negative_template = template.format(persona=negative_persona)\n",
    "                dataset.append(\n",
    "                    DatasetEntry(\n",
    "                        positive=f\"{user_tag} {positive_template} {asst_tag} {suffix}\",\n",
    "                        negative=f\"{user_tag} {negative_template} {asst_tag} {suffix}\",\n",
    "                    )\n",
    "                )\n",
    "        return dataset\n",
    "\n",
    "    def train_control_vector(self, lm):\n",
    "        print(\"Training control vector...\")\n",
    "        #user_tag, asst_tag = \"[INST]\", \"[/INST]\"\n",
    "        user_tag, asst_tag = \"\", \"\"\n",
    "        lm.tokenizer.pad_token_id = 0\n",
    "\n",
    "        with open(\"/home/nightwing/Desktop/Projects/repeng/notebooks/data/all_truncated_outputs.json\") as f:\n",
    "            output_suffixes = json.load(f)\n",
    "\n",
    "        truncated_output_suffixes = [\n",
    "            lm.tokenizer.convert_tokens_to_string(tokens[:i])\n",
    "            for tokens in (lm.tokenizer.tokenize(s) for s in output_suffixes)\n",
    "            for i in range(1, len(tokens))\n",
    "        ]\n",
    "\n",
    "        cvec_dataset = self.make_dataset(\n",
    "            \"Act as if you're extremely {persona}.\",\n",
    "            [POSITIVE_PERSONA],\n",
    "            [NEGATIVE_PERSONA],\n",
    "            truncated_output_suffixes,\n",
    "        )\n",
    "\n",
    "        control_vector = ControlVector.train(lm.model, lm.tokenizer, cvec_dataset)\n",
    "\n",
    "        return control_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run SMC\n",
    "- Running the below cell will run `smc_steer` ITERATIONS number of times, likely once, with the parameters set above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    lm = CachedCausalLM.from_pretrained(\"NousResearch/Hermes-3-Llama-3.2-3B\", backend='hf')\n",
    "\n",
    "    lm.model = ControlModel(lm.model, list(range(-5, -12, -1)))\n",
    "\n",
    "    lm.batch_size = 8\n",
    "\n",
    "    model = NightwingEntropyControlModel(\n",
    "        lm=lm,\n",
    "        prompt=prompt,\n",
    "        uncertainty_token=uncertainty_token,\n",
    "        temperature=TEMPERATURE,\n",
    "        entropy_threshold=ENTROPY_THRESHOLD,\n",
    "        max_tokens=MAX_TOKENS,\n",
    "        use_token_trigger=USE_TOKEN_TRIGGER\n",
    "    )\n",
    "\n",
    "    print(f\"\\nSteering with smc_steer with {GREEN}{NUM_PARTICLES}{END} particles and beam factor {GREEN}{BEAM_FACTOR}{END}\")\n",
    "\n",
    "    for i in range(ITERATIONS):\n",
    "        particles = await smc_steer(model, NUM_PARTICLES, BEAM_FACTOR)\n",
    "\n",
    "        print(f\"\\n{GREEN}{prompt}{END}\")\n",
    "        for particle in particles:\n",
    "            print(pretty_format(particle))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
