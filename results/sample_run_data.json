{
    "results": {
        "mmlu_smc_regex": {
            "alias": "mmlu_smc_regex"
        },
        "mmlu_llama_humanities": {
            "alias": " - humanities"
        },
        "mmlu_llama_formal_logic": {
            "alias": "  - formal logic",
            "exact_match,flexible-extract": 0.0,
            "exact_match_stderr,flexible-extract": "N/A"
        },
        "mmlu_llama_high_school_european_history": {
            "alias": "  - high school european history",
            "exact_match,flexible-extract": 0.0,
            "exact_match_stderr,flexible-extract": "N/A"
        },
        "mmlu_llama_high_school_us_history": {
            "alias": "  - high school us history",
            "exact_match,flexible-extract": 0.0,
            "exact_match_stderr,flexible-extract": "N/A"
        },
        "mmlu_llama_high_school_world_history": {
            "alias": "  - high school world history",
            "exact_match,flexible-extract": 0.0,
            "exact_match_stderr,flexible-extract": "N/A"
        },
        "mmlu_llama_international_law": {
            "alias": "  - international law",
            "exact_match,flexible-extract": 1.0,
            "exact_match_stderr,flexible-extract": "N/A"
        },
        "mmlu_llama_jurisprudence": {
            "alias": "  - jurisprudence",
            "exact_match,flexible-extract": 0.0,
            "exact_match_stderr,flexible-extract": "N/A"
        },
        "mmlu_llama_logical_fallacies": {
            "alias": "  - logical fallacies",
            "exact_match,flexible-extract": 0.0,
            "exact_match_stderr,flexible-extract": "N/A"
        },
        "mmlu_llama_moral_disputes": {
            "alias": "  - moral disputes",
            "exact_match,flexible-extract": 1.0,
            "exact_match_stderr,flexible-extract": "N/A"
        },
        "mmlu_llama_moral_scenarios": {
            "alias": "  - moral scenarios",
            "exact_match,flexible-extract": 0.0,
            "exact_match_stderr,flexible-extract": "N/A"
        },
        "mmlu_llama_philosophy": {
            "alias": "  - philosophy",
            "exact_match,flexible-extract": 0.0,
            "exact_match_stderr,flexible-extract": "N/A"
        },
        "mmlu_llama_prehistory": {
            "alias": "  - prehistory",
            "exact_match,flexible-extract": 0.0,
            "exact_match_stderr,flexible-extract": "N/A"
        },
        "mmlu_llama_professional_law": {
            "alias": "  - professional law",
            "exact_match,flexible-extract": 0.0,
            "exact_match_stderr,flexible-extract": "N/A"
        },
        "mmlu_llama_world_religions": {
            "alias": "  - world religions",
            "exact_match,flexible-extract": 0.0,
            "exact_match_stderr,flexible-extract": "N/A"
        },
        "mmlu_llama_other": {
            "alias": " - other"
        },
        "mmlu_llama_business_ethics": {
            "alias": "  - business ethics",
            "exact_match,flexible-extract": 0.0,
            "exact_match_stderr,flexible-extract": "N/A"
        },
        "mmlu_llama_clinical_knowledge": {
            "alias": "  - clinical knowledge",
            "exact_match,flexible-extract": 0.0,
            "exact_match_stderr,flexible-extract": "N/A"
        },
        "mmlu_llama_college_medicine": {
            "alias": "  - college medicine",
            "exact_match,flexible-extract": 1.0,
            "exact_match_stderr,flexible-extract": "N/A"
        },
        "mmlu_llama_global_facts": {
            "alias": "  - global facts",
            "exact_match,flexible-extract": 0.0,
            "exact_match_stderr,flexible-extract": "N/A"
        },
        "mmlu_llama_human_aging": {
            "alias": "  - human aging",
            "exact_match,flexible-extract": 0.0,
            "exact_match_stderr,flexible-extract": "N/A"
        },
        "mmlu_llama_management": {
            "alias": "  - management",
            "exact_match,flexible-extract": 0.0,
            "exact_match_stderr,flexible-extract": "N/A"
        },
        "mmlu_llama_marketing": {
            "alias": "  - marketing",
            "exact_match,flexible-extract": 0.0,
            "exact_match_stderr,flexible-extract": "N/A"
        },
        "mmlu_llama_medical_genetics": {
            "alias": "  - medical genetics",
            "exact_match,flexible-extract": 0.0,
            "exact_match_stderr,flexible-extract": "N/A"
        },
        "mmlu_llama_miscellaneous": {
            "alias": "  - miscellaneous",
            "exact_match,flexible-extract": 1.0,
            "exact_match_stderr,flexible-extract": "N/A"
        },
        "mmlu_llama_nutrition": {
            "alias": "  - nutrition",
            "exact_match,flexible-extract": 0.0,
            "exact_match_stderr,flexible-extract": "N/A"
        },
        "mmlu_llama_professional_accounting": {
            "alias": "  - professional accounting",
            "exact_match,flexible-extract": 0.0,
            "exact_match_stderr,flexible-extract": "N/A"
        },
        "mmlu_llama_professional_medicine": {
            "alias": "  - professional medicine",
            "exact_match,flexible-extract": 1.0,
            "exact_match_stderr,flexible-extract": "N/A"
        },
        "mmlu_llama_virology": {
            "alias": "  - virology",
            "exact_match,flexible-extract": 0.0,
            "exact_match_stderr,flexible-extract": "N/A"
        },
        "mmlu_llama_social_sciences": {
            "alias": " - social sciences"
        },
        "mmlu_llama_econometrics": {
            "alias": "  - econometrics",
            "exact_match,flexible-extract": 0.0,
            "exact_match_stderr,flexible-extract": "N/A"
        },
        "mmlu_llama_high_school_geography": {
            "alias": "  - high school geography",
            "exact_match,flexible-extract": 0.0,
            "exact_match_stderr,flexible-extract": "N/A"
        },
        "mmlu_llama_high_school_government_and_politics": {
            "alias": "  - high school government and politics",
            "exact_match,flexible-extract": 0.0,
            "exact_match_stderr,flexible-extract": "N/A"
        },
        "mmlu_llama_high_school_macroeconomics": {
            "alias": "  - high school macroeconomics",
            "exact_match,flexible-extract": 0.0,
            "exact_match_stderr,flexible-extract": "N/A"
        },
        "mmlu_llama_high_school_microeconomics": {
            "alias": "  - high school microeconomics",
            "exact_match,flexible-extract": 0.0,
            "exact_match_stderr,flexible-extract": "N/A"
        },
        "mmlu_llama_high_school_psychology": {
            "alias": "  - high school psychology",
            "exact_match,flexible-extract": 0.0,
            "exact_match_stderr,flexible-extract": "N/A"
        },
        "mmlu_llama_human_sexuality": {
            "alias": "  - human sexuality",
            "exact_match,flexible-extract": 0.0,
            "exact_match_stderr,flexible-extract": "N/A"
        },
        "mmlu_llama_professional_psychology": {
            "alias": "  - professional psychology",
            "exact_match,flexible-extract": 0.0,
            "exact_match_stderr,flexible-extract": "N/A"
        },
        "mmlu_llama_public_relations": {
            "alias": "  - public relations",
            "exact_match,flexible-extract": 1.0,
            "exact_match_stderr,flexible-extract": "N/A"
        },
        "mmlu_llama_security_studies": {
            "alias": "  - security studies",
            "exact_match,flexible-extract": 0.0,
            "exact_match_stderr,flexible-extract": "N/A"
        },
        "mmlu_llama_sociology": {
            "alias": "  - sociology",
            "exact_match,flexible-extract": 0.0,
            "exact_match_stderr,flexible-extract": "N/A"
        },
        "mmlu_llama_us_foreign_policy": {
            "alias": "  - us foreign policy",
            "exact_match,flexible-extract": 0.0,
            "exact_match_stderr,flexible-extract": "N/A"
        },
        "mmlu_llama_stem": {
            "alias": " - stem"
        },
        "mmlu_llama_abstract_algebra": {
            "alias": "  - abstract algebra",
            "exact_match,flexible-extract": 0.0,
            "exact_match_stderr,flexible-extract": "N/A"
        },
        "mmlu_llama_anatomy": {
            "alias": "  - anatomy",
            "exact_match,flexible-extract": 1.0,
            "exact_match_stderr,flexible-extract": "N/A"
        },
        "mmlu_llama_astronomy": {
            "alias": "  - astronomy",
            "exact_match,flexible-extract": 0.0,
            "exact_match_stderr,flexible-extract": "N/A"
        },
        "mmlu_llama_college_biology": {
            "alias": "  - college biology",
            "exact_match,flexible-extract": 0.0,
            "exact_match_stderr,flexible-extract": "N/A"
        },
        "mmlu_llama_college_chemistry": {
            "alias": "  - college chemistry",
            "exact_match,flexible-extract": 0.0,
            "exact_match_stderr,flexible-extract": "N/A"
        },
        "mmlu_llama_college_computer_science": {
            "alias": "  - college computer science",
            "exact_match,flexible-extract": 0.0,
            "exact_match_stderr,flexible-extract": "N/A"
        },
        "mmlu_llama_college_mathematics": {
            "alias": "  - college mathematics",
            "exact_match,flexible-extract": 0.0,
            "exact_match_stderr,flexible-extract": "N/A"
        },
        "mmlu_llama_college_physics": {
            "alias": "  - college physics",
            "exact_match,flexible-extract": 0.0,
            "exact_match_stderr,flexible-extract": "N/A"
        },
        "mmlu_llama_computer_security": {
            "alias": "  - computer security",
            "exact_match,flexible-extract": 1.0,
            "exact_match_stderr,flexible-extract": "N/A"
        },
        "mmlu_llama_conceptual_physics": {
            "alias": "  - conceptual physics",
            "exact_match,flexible-extract": 0.0,
            "exact_match_stderr,flexible-extract": "N/A"
        },
        "mmlu_llama_electrical_engineering": {
            "alias": "  - electrical engineering",
            "exact_match,flexible-extract": 0.0,
            "exact_match_stderr,flexible-extract": "N/A"
        },
        "mmlu_llama_elementary_mathematics": {
            "alias": "  - elementary mathematics",
            "exact_match,flexible-extract": 1.0,
            "exact_match_stderr,flexible-extract": "N/A"
        },
        "mmlu_llama_high_school_biology": {
            "alias": "  - high school biology",
            "exact_match,flexible-extract": 0.0,
            "exact_match_stderr,flexible-extract": "N/A"
        },
        "mmlu_llama_high_school_chemistry": {
            "alias": "  - high school chemistry",
            "exact_match,flexible-extract": 1.0,
            "exact_match_stderr,flexible-extract": "N/A"
        },
        "mmlu_llama_high_school_computer_science": {
            "alias": "  - high school computer science",
            "exact_match,flexible-extract": 0.0,
            "exact_match_stderr,flexible-extract": "N/A"
        },
        "mmlu_llama_high_school_mathematics": {
            "alias": "  - high school mathematics",
            "exact_match,flexible-extract": 0.0,
            "exact_match_stderr,flexible-extract": "N/A"
        },
        "mmlu_llama_high_school_physics": {
            "alias": "  - high school physics",
            "exact_match,flexible-extract": 0.0,
            "exact_match_stderr,flexible-extract": "N/A"
        },
        "mmlu_llama_high_school_statistics": {
            "alias": "  - high school statistics",
            "exact_match,flexible-extract": 0.0,
            "exact_match_stderr,flexible-extract": "N/A"
        },
        "mmlu_llama_machine_learning": {
            "alias": "  - machine learning",
            "exact_match,flexible-extract": 0.0,
            "exact_match_stderr,flexible-extract": "N/A"
        }
    },
    "groups": {
        "mmlu_smc_regex": {
            "alias": "mmlu_smc_regex"
        },
        "mmlu_llama_humanities": {
            "alias": " - humanities"
        },
        "mmlu_llama_other": {
            "alias": " - other"
        },
        "mmlu_llama_social_sciences": {
            "alias": " - social sciences"
        },
        "mmlu_llama_stem": {
            "alias": " - stem"
        }
    },
    "group_subtasks": {
        "mmlu_llama_humanities": [
            "mmlu_llama_logical_fallacies",
            "mmlu_llama_professional_law",
            "mmlu_llama_jurisprudence",
            "mmlu_llama_high_school_world_history",
            "mmlu_llama_high_school_european_history",
            "mmlu_llama_high_school_us_history",
            "mmlu_llama_formal_logic",
            "mmlu_llama_international_law",
            "mmlu_llama_moral_disputes",
            "mmlu_llama_world_religions",
            "mmlu_llama_prehistory",
            "mmlu_llama_moral_scenarios",
            "mmlu_llama_philosophy"
        ],
        "mmlu_llama_social_sciences": [
            "mmlu_llama_high_school_psychology",
            "mmlu_llama_professional_psychology",
            "mmlu_llama_high_school_geography",
            "mmlu_llama_sociology",
            "mmlu_llama_security_studies",
            "mmlu_llama_high_school_macroeconomics",
            "mmlu_llama_high_school_government_and_politics",
            "mmlu_llama_public_relations",
            "mmlu_llama_econometrics",
            "mmlu_llama_human_sexuality",
            "mmlu_llama_us_foreign_policy",
            "mmlu_llama_high_school_microeconomics"
        ],
        "mmlu_llama_other": [
            "mmlu_llama_professional_accounting",
            "mmlu_llama_professional_medicine",
            "mmlu_llama_marketing",
            "mmlu_llama_virology",
            "mmlu_llama_miscellaneous",
            "mmlu_llama_clinical_knowledge",
            "mmlu_llama_nutrition",
            "mmlu_llama_medical_genetics",
            "mmlu_llama_business_ethics",
            "mmlu_llama_management",
            "mmlu_llama_global_facts",
            "mmlu_llama_college_medicine",
            "mmlu_llama_human_aging"
        ],
        "mmlu_llama_stem": [
            "mmlu_llama_anatomy",
            "mmlu_llama_computer_security",
            "mmlu_llama_high_school_physics",
            "mmlu_llama_high_school_biology",
            "mmlu_llama_college_biology",
            "mmlu_llama_high_school_computer_science",
            "mmlu_llama_conceptual_physics",
            "mmlu_llama_astronomy",
            "mmlu_llama_college_physics",
            "mmlu_llama_elementary_mathematics",
            "mmlu_llama_high_school_statistics",
            "mmlu_llama_electrical_engineering",
            "mmlu_llama_machine_learning",
            "mmlu_llama_abstract_algebra",
            "mmlu_llama_high_school_mathematics",
            "mmlu_llama_college_computer_science",
            "mmlu_llama_high_school_chemistry",
            "mmlu_llama_college_chemistry",
            "mmlu_llama_college_mathematics"
        ],
        "mmlu_smc_regex": [
            "mmlu_llama_stem",
            "mmlu_llama_other",
            "mmlu_llama_social_sciences",
            "mmlu_llama_humanities"
        ]
    },
    "configs": {
        "mmlu_llama_abstract_algebra": {
            "task": "mmlu_llama_abstract_algebra",
            "task_alias": "abstract algebra",
            "tag": "mmlu_llama_stem_tasks",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "abstract_algebra",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Given the following question and four candidate answers (A, B, C and D), choose the best answer.\nQuestion: {{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nYour response should end with \"The best answer is [the_answer_letter]\" where the [the_answer_letter] is one of A, B, C or D.",
            "doc_to_target": "{{['A.','B.','C.','D.'][answer]}}",
            "unsafe_code": false,
            "description": "",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "exact_match",
                    "aggregation": "mean",
                    "higher_is_better": true,
                    "ignore_case": true,
                    "ignore_punctuation": true,
                    "regexes_to_ignore": [
                        "\\$",
                        "\\.$"
                    ]
                }
            ],
            "output_type": "generate_until",
            "generation_kwargs": {
                "do_sample": false,
                "until": [
                    "."
                ],
                "max_gen_toks": 10
            },
            "repeats": 1,
            "filter_list": [
                {
                    "name": "flexible-extract",
                    "filter": [
                        {
                            "function": "multi_choice_regex",
                            "group_select": -1,
                            "ignore_case": true,
                            "ignore_punctuation": true,
                            "regex_pattern": "[tT]he best answer is ([A-D])"
                        },
                        {
                            "function": "take_first"
                        }
                    ]
                }
            ],
            "should_decontaminate": false,
            "gen_prefix": "The best answer is",
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_llama_anatomy": {
            "task": "mmlu_llama_anatomy",
            "task_alias": "anatomy",
            "tag": "mmlu_llama_stem_tasks",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "anatomy",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Given the following question and four candidate answers (A, B, C and D), choose the best answer.\nQuestion: {{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nYour response should end with \"The best answer is [the_answer_letter]\" where the [the_answer_letter] is one of A, B, C or D.",
            "doc_to_target": "{{['A.','B.','C.','D.'][answer]}}",
            "unsafe_code": false,
            "description": "",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "exact_match",
                    "aggregation": "mean",
                    "higher_is_better": true,
                    "ignore_case": true,
                    "ignore_punctuation": true,
                    "regexes_to_ignore": [
                        "\\$",
                        "\\.$"
                    ]
                }
            ],
            "output_type": "generate_until",
            "generation_kwargs": {
                "do_sample": false,
                "until": [
                    "."
                ],
                "max_gen_toks": 10
            },
            "repeats": 1,
            "filter_list": [
                {
                    "name": "flexible-extract",
                    "filter": [
                        {
                            "function": "multi_choice_regex",
                            "group_select": -1,
                            "ignore_case": true,
                            "ignore_punctuation": true,
                            "regex_pattern": "[tT]he best answer is ([A-D])"
                        },
                        {
                            "function": "take_first"
                        }
                    ]
                }
            ],
            "should_decontaminate": false,
            "gen_prefix": "The best answer is",
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_llama_astronomy": {
            "task": "mmlu_llama_astronomy",
            "task_alias": "astronomy",
            "tag": "mmlu_llama_stem_tasks",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "astronomy",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Given the following question and four candidate answers (A, B, C and D), choose the best answer.\nQuestion: {{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nYour response should end with \"The best answer is [the_answer_letter]\" where the [the_answer_letter] is one of A, B, C or D.",
            "doc_to_target": "{{['A.','B.','C.','D.'][answer]}}",
            "unsafe_code": false,
            "description": "",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "exact_match",
                    "aggregation": "mean",
                    "higher_is_better": true,
                    "ignore_case": true,
                    "ignore_punctuation": true,
                    "regexes_to_ignore": [
                        "\\$",
                        "\\.$"
                    ]
                }
            ],
            "output_type": "generate_until",
            "generation_kwargs": {
                "do_sample": false,
                "until": [
                    "."
                ],
                "max_gen_toks": 10
            },
            "repeats": 1,
            "filter_list": [
                {
                    "name": "flexible-extract",
                    "filter": [
                        {
                            "function": "multi_choice_regex",
                            "group_select": -1,
                            "ignore_case": true,
                            "ignore_punctuation": true,
                            "regex_pattern": "[tT]he best answer is ([A-D])"
                        },
                        {
                            "function": "take_first"
                        }
                    ]
                }
            ],
            "should_decontaminate": false,
            "gen_prefix": "The best answer is",
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_llama_business_ethics": {
            "task": "mmlu_llama_business_ethics",
            "task_alias": "business ethics",
            "tag": "mmlu_llama_other_tasks",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "business_ethics",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Given the following question and four candidate answers (A, B, C and D), choose the best answer.\nQuestion: {{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nYour response should end with \"The best answer is [the_answer_letter]\" where the [the_answer_letter] is one of A, B, C or D.",
            "doc_to_target": "{{['A.','B.','C.','D.'][answer]}}",
            "unsafe_code": false,
            "description": "",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "exact_match",
                    "aggregation": "mean",
                    "higher_is_better": true,
                    "ignore_case": true,
                    "ignore_punctuation": true,
                    "regexes_to_ignore": [
                        "\\$",
                        "\\.$"
                    ]
                }
            ],
            "output_type": "generate_until",
            "generation_kwargs": {
                "do_sample": false,
                "until": [
                    "."
                ],
                "max_gen_toks": 10
            },
            "repeats": 1,
            "filter_list": [
                {
                    "name": "flexible-extract",
                    "filter": [
                        {
                            "function": "multi_choice_regex",
                            "group_select": -1,
                            "ignore_case": true,
                            "ignore_punctuation": true,
                            "regex_pattern": "[tT]he best answer is ([A-D])"
                        },
                        {
                            "function": "take_first"
                        }
                    ]
                }
            ],
            "should_decontaminate": false,
            "gen_prefix": "The best answer is",
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_llama_clinical_knowledge": {
            "task": "mmlu_llama_clinical_knowledge",
            "task_alias": "clinical knowledge",
            "tag": "mmlu_llama_other_tasks",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "clinical_knowledge",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Given the following question and four candidate answers (A, B, C and D), choose the best answer.\nQuestion: {{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nYour response should end with \"The best answer is [the_answer_letter]\" where the [the_answer_letter] is one of A, B, C or D.",
            "doc_to_target": "{{['A.','B.','C.','D.'][answer]}}",
            "unsafe_code": false,
            "description": "",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "exact_match",
                    "aggregation": "mean",
                    "higher_is_better": true,
                    "ignore_case": true,
                    "ignore_punctuation": true,
                    "regexes_to_ignore": [
                        "\\$",
                        "\\.$"
                    ]
                }
            ],
            "output_type": "generate_until",
            "generation_kwargs": {
                "do_sample": false,
                "until": [
                    "."
                ],
                "max_gen_toks": 10
            },
            "repeats": 1,
            "filter_list": [
                {
                    "name": "flexible-extract",
                    "filter": [
                        {
                            "function": "multi_choice_regex",
                            "group_select": -1,
                            "ignore_case": true,
                            "ignore_punctuation": true,
                            "regex_pattern": "[tT]he best answer is ([A-D])"
                        },
                        {
                            "function": "take_first"
                        }
                    ]
                }
            ],
            "should_decontaminate": false,
            "gen_prefix": "The best answer is",
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_llama_college_biology": {
            "task": "mmlu_llama_college_biology",
            "task_alias": "college biology",
            "tag": "mmlu_llama_stem_tasks",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "college_biology",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Given the following question and four candidate answers (A, B, C and D), choose the best answer.\nQuestion: {{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nYour response should end with \"The best answer is [the_answer_letter]\" where the [the_answer_letter] is one of A, B, C or D.",
            "doc_to_target": "{{['A.','B.','C.','D.'][answer]}}",
            "unsafe_code": false,
            "description": "",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "exact_match",
                    "aggregation": "mean",
                    "higher_is_better": true,
                    "ignore_case": true,
                    "ignore_punctuation": true,
                    "regexes_to_ignore": [
                        "\\$",
                        "\\.$"
                    ]
                }
            ],
            "output_type": "generate_until",
            "generation_kwargs": {
                "do_sample": false,
                "until": [
                    "."
                ],
                "max_gen_toks": 10
            },
            "repeats": 1,
            "filter_list": [
                {
                    "name": "flexible-extract",
                    "filter": [
                        {
                            "function": "multi_choice_regex",
                            "group_select": -1,
                            "ignore_case": true,
                            "ignore_punctuation": true,
                            "regex_pattern": "[tT]he best answer is ([A-D])"
                        },
                        {
                            "function": "take_first"
                        }
                    ]
                }
            ],
            "should_decontaminate": false,
            "gen_prefix": "The best answer is",
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_llama_college_chemistry": {
            "task": "mmlu_llama_college_chemistry",
            "task_alias": "college chemistry",
            "tag": "mmlu_llama_stem_tasks",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "college_chemistry",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Given the following question and four candidate answers (A, B, C and D), choose the best answer.\nQuestion: {{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nYour response should end with \"The best answer is [the_answer_letter]\" where the [the_answer_letter] is one of A, B, C or D.",
            "doc_to_target": "{{['A.','B.','C.','D.'][answer]}}",
            "unsafe_code": false,
            "description": "",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "exact_match",
                    "aggregation": "mean",
                    "higher_is_better": true,
                    "ignore_case": true,
                    "ignore_punctuation": true,
                    "regexes_to_ignore": [
                        "\\$",
                        "\\.$"
                    ]
                }
            ],
            "output_type": "generate_until",
            "generation_kwargs": {
                "do_sample": false,
                "until": [
                    "."
                ],
                "max_gen_toks": 10
            },
            "repeats": 1,
            "filter_list": [
                {
                    "name": "flexible-extract",
                    "filter": [
                        {
                            "function": "multi_choice_regex",
                            "group_select": -1,
                            "ignore_case": true,
                            "ignore_punctuation": true,
                            "regex_pattern": "[tT]he best answer is ([A-D])"
                        },
                        {
                            "function": "take_first"
                        }
                    ]
                }
            ],
            "should_decontaminate": false,
            "gen_prefix": "The best answer is",
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_llama_college_computer_science": {
            "task": "mmlu_llama_college_computer_science",
            "task_alias": "college computer science",
            "tag": "mmlu_llama_stem_tasks",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "college_computer_science",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Given the following question and four candidate answers (A, B, C and D), choose the best answer.\nQuestion: {{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nYour response should end with \"The best answer is [the_answer_letter]\" where the [the_answer_letter] is one of A, B, C or D.",
            "doc_to_target": "{{['A.','B.','C.','D.'][answer]}}",
            "unsafe_code": false,
            "description": "",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "exact_match",
                    "aggregation": "mean",
                    "higher_is_better": true,
                    "ignore_case": true,
                    "ignore_punctuation": true,
                    "regexes_to_ignore": [
                        "\\$",
                        "\\.$"
                    ]
                }
            ],
            "output_type": "generate_until",
            "generation_kwargs": {
                "do_sample": false,
                "until": [
                    "."
                ],
                "max_gen_toks": 10
            },
            "repeats": 1,
            "filter_list": [
                {
                    "name": "flexible-extract",
                    "filter": [
                        {
                            "function": "multi_choice_regex",
                            "group_select": -1,
                            "ignore_case": true,
                            "ignore_punctuation": true,
                            "regex_pattern": "[tT]he best answer is ([A-D])"
                        },
                        {
                            "function": "take_first"
                        }
                    ]
                }
            ],
            "should_decontaminate": false,
            "gen_prefix": "The best answer is",
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_llama_college_mathematics": {
            "task": "mmlu_llama_college_mathematics",
            "task_alias": "college mathematics",
            "tag": "mmlu_llama_stem_tasks",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "college_mathematics",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Given the following question and four candidate answers (A, B, C and D), choose the best answer.\nQuestion: {{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nYour response should end with \"The best answer is [the_answer_letter]\" where the [the_answer_letter] is one of A, B, C or D.",
            "doc_to_target": "{{['A.','B.','C.','D.'][answer]}}",
            "unsafe_code": false,
            "description": "",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "exact_match",
                    "aggregation": "mean",
                    "higher_is_better": true,
                    "ignore_case": true,
                    "ignore_punctuation": true,
                    "regexes_to_ignore": [
                        "\\$",
                        "\\.$"
                    ]
                }
            ],
            "output_type": "generate_until",
            "generation_kwargs": {
                "do_sample": false,
                "until": [
                    "."
                ],
                "max_gen_toks": 10
            },
            "repeats": 1,
            "filter_list": [
                {
                    "name": "flexible-extract",
                    "filter": [
                        {
                            "function": "multi_choice_regex",
                            "group_select": -1,
                            "ignore_case": true,
                            "ignore_punctuation": true,
                            "regex_pattern": "[tT]he best answer is ([A-D])"
                        },
                        {
                            "function": "take_first"
                        }
                    ]
                }
            ],
            "should_decontaminate": false,
            "gen_prefix": "The best answer is",
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_llama_college_medicine": {
            "task": "mmlu_llama_college_medicine",
            "task_alias": "college medicine",
            "tag": "mmlu_llama_other_tasks",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "college_medicine",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Given the following question and four candidate answers (A, B, C and D), choose the best answer.\nQuestion: {{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nYour response should end with \"The best answer is [the_answer_letter]\" where the [the_answer_letter] is one of A, B, C or D.",
            "doc_to_target": "{{['A.','B.','C.','D.'][answer]}}",
            "unsafe_code": false,
            "description": "",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "exact_match",
                    "aggregation": "mean",
                    "higher_is_better": true,
                    "ignore_case": true,
                    "ignore_punctuation": true,
                    "regexes_to_ignore": [
                        "\\$",
                        "\\.$"
                    ]
                }
            ],
            "output_type": "generate_until",
            "generation_kwargs": {
                "do_sample": false,
                "until": [
                    "."
                ],
                "max_gen_toks": 10
            },
            "repeats": 1,
            "filter_list": [
                {
                    "name": "flexible-extract",
                    "filter": [
                        {
                            "function": "multi_choice_regex",
                            "group_select": -1,
                            "ignore_case": true,
                            "ignore_punctuation": true,
                            "regex_pattern": "[tT]he best answer is ([A-D])"
                        },
                        {
                            "function": "take_first"
                        }
                    ]
                }
            ],
            "should_decontaminate": false,
            "gen_prefix": "The best answer is",
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_llama_college_physics": {
            "task": "mmlu_llama_college_physics",
            "task_alias": "college physics",
            "tag": "mmlu_llama_stem_tasks",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "college_physics",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Given the following question and four candidate answers (A, B, C and D), choose the best answer.\nQuestion: {{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nYour response should end with \"The best answer is [the_answer_letter]\" where the [the_answer_letter] is one of A, B, C or D.",
            "doc_to_target": "{{['A.','B.','C.','D.'][answer]}}",
            "unsafe_code": false,
            "description": "",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "exact_match",
                    "aggregation": "mean",
                    "higher_is_better": true,
                    "ignore_case": true,
                    "ignore_punctuation": true,
                    "regexes_to_ignore": [
                        "\\$",
                        "\\.$"
                    ]
                }
            ],
            "output_type": "generate_until",
            "generation_kwargs": {
                "do_sample": false,
                "until": [
                    "."
                ],
                "max_gen_toks": 10
            },
            "repeats": 1,
            "filter_list": [
                {
                    "name": "flexible-extract",
                    "filter": [
                        {
                            "function": "multi_choice_regex",
                            "group_select": -1,
                            "ignore_case": true,
                            "ignore_punctuation": true,
                            "regex_pattern": "[tT]he best answer is ([A-D])"
                        },
                        {
                            "function": "take_first"
                        }
                    ]
                }
            ],
            "should_decontaminate": false,
            "gen_prefix": "The best answer is",
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_llama_computer_security": {
            "task": "mmlu_llama_computer_security",
            "task_alias": "computer security",
            "tag": "mmlu_llama_stem_tasks",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "computer_security",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Given the following question and four candidate answers (A, B, C and D), choose the best answer.\nQuestion: {{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nYour response should end with \"The best answer is [the_answer_letter]\" where the [the_answer_letter] is one of A, B, C or D.",
            "doc_to_target": "{{['A.','B.','C.','D.'][answer]}}",
            "unsafe_code": false,
            "description": "",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "exact_match",
                    "aggregation": "mean",
                    "higher_is_better": true,
                    "ignore_case": true,
                    "ignore_punctuation": true,
                    "regexes_to_ignore": [
                        "\\$",
                        "\\.$"
                    ]
                }
            ],
            "output_type": "generate_until",
            "generation_kwargs": {
                "do_sample": false,
                "until": [
                    "."
                ],
                "max_gen_toks": 10
            },
            "repeats": 1,
            "filter_list": [
                {
                    "name": "flexible-extract",
                    "filter": [
                        {
                            "function": "multi_choice_regex",
                            "group_select": -1,
                            "ignore_case": true,
                            "ignore_punctuation": true,
                            "regex_pattern": "[tT]he best answer is ([A-D])"
                        },
                        {
                            "function": "take_first"
                        }
                    ]
                }
            ],
            "should_decontaminate": false,
            "gen_prefix": "The best answer is",
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_llama_conceptual_physics": {
            "task": "mmlu_llama_conceptual_physics",
            "task_alias": "conceptual physics",
            "tag": "mmlu_llama_stem_tasks",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "conceptual_physics",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Given the following question and four candidate answers (A, B, C and D), choose the best answer.\nQuestion: {{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nYour response should end with \"The best answer is [the_answer_letter]\" where the [the_answer_letter] is one of A, B, C or D.",
            "doc_to_target": "{{['A.','B.','C.','D.'][answer]}}",
            "unsafe_code": false,
            "description": "",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "exact_match",
                    "aggregation": "mean",
                    "higher_is_better": true,
                    "ignore_case": true,
                    "ignore_punctuation": true,
                    "regexes_to_ignore": [
                        "\\$",
                        "\\.$"
                    ]
                }
            ],
            "output_type": "generate_until",
            "generation_kwargs": {
                "do_sample": false,
                "until": [
                    "."
                ],
                "max_gen_toks": 10
            },
            "repeats": 1,
            "filter_list": [
                {
                    "name": "flexible-extract",
                    "filter": [
                        {
                            "function": "multi_choice_regex",
                            "group_select": -1,
                            "ignore_case": true,
                            "ignore_punctuation": true,
                            "regex_pattern": "[tT]he best answer is ([A-D])"
                        },
                        {
                            "function": "take_first"
                        }
                    ]
                }
            ],
            "should_decontaminate": false,
            "gen_prefix": "The best answer is",
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_llama_econometrics": {
            "task": "mmlu_llama_econometrics",
            "task_alias": "econometrics",
            "tag": "mmlu_llama_social_sciences_tasks",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "econometrics",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Given the following question and four candidate answers (A, B, C and D), choose the best answer.\nQuestion: {{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nYour response should end with \"The best answer is [the_answer_letter]\" where the [the_answer_letter] is one of A, B, C or D.",
            "doc_to_target": "{{['A.','B.','C.','D.'][answer]}}",
            "unsafe_code": false,
            "description": "",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "exact_match",
                    "aggregation": "mean",
                    "higher_is_better": true,
                    "ignore_case": true,
                    "ignore_punctuation": true,
                    "regexes_to_ignore": [
                        "\\$",
                        "\\.$"
                    ]
                }
            ],
            "output_type": "generate_until",
            "generation_kwargs": {
                "do_sample": false,
                "until": [
                    "."
                ],
                "max_gen_toks": 10
            },
            "repeats": 1,
            "filter_list": [
                {
                    "name": "flexible-extract",
                    "filter": [
                        {
                            "function": "multi_choice_regex",
                            "group_select": -1,
                            "ignore_case": true,
                            "ignore_punctuation": true,
                            "regex_pattern": "[tT]he best answer is ([A-D])"
                        },
                        {
                            "function": "take_first"
                        }
                    ]
                }
            ],
            "should_decontaminate": false,
            "gen_prefix": "The best answer is",
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_llama_electrical_engineering": {
            "task": "mmlu_llama_electrical_engineering",
            "task_alias": "electrical engineering",
            "tag": "mmlu_llama_stem_tasks",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "electrical_engineering",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Given the following question and four candidate answers (A, B, C and D), choose the best answer.\nQuestion: {{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nYour response should end with \"The best answer is [the_answer_letter]\" where the [the_answer_letter] is one of A, B, C or D.",
            "doc_to_target": "{{['A.','B.','C.','D.'][answer]}}",
            "unsafe_code": false,
            "description": "",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "exact_match",
                    "aggregation": "mean",
                    "higher_is_better": true,
                    "ignore_case": true,
                    "ignore_punctuation": true,
                    "regexes_to_ignore": [
                        "\\$",
                        "\\.$"
                    ]
                }
            ],
            "output_type": "generate_until",
            "generation_kwargs": {
                "do_sample": false,
                "until": [
                    "."
                ],
                "max_gen_toks": 10
            },
            "repeats": 1,
            "filter_list": [
                {
                    "name": "flexible-extract",
                    "filter": [
                        {
                            "function": "multi_choice_regex",
                            "group_select": -1,
                            "ignore_case": true,
                            "ignore_punctuation": true,
                            "regex_pattern": "[tT]he best answer is ([A-D])"
                        },
                        {
                            "function": "take_first"
                        }
                    ]
                }
            ],
            "should_decontaminate": false,
            "gen_prefix": "The best answer is",
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_llama_elementary_mathematics": {
            "task": "mmlu_llama_elementary_mathematics",
            "task_alias": "elementary mathematics",
            "tag": "mmlu_llama_stem_tasks",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "elementary_mathematics",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Given the following question and four candidate answers (A, B, C and D), choose the best answer.\nQuestion: {{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nYour response should end with \"The best answer is [the_answer_letter]\" where the [the_answer_letter] is one of A, B, C or D.",
            "doc_to_target": "{{['A.','B.','C.','D.'][answer]}}",
            "unsafe_code": false,
            "description": "",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "exact_match",
                    "aggregation": "mean",
                    "higher_is_better": true,
                    "ignore_case": true,
                    "ignore_punctuation": true,
                    "regexes_to_ignore": [
                        "\\$",
                        "\\.$"
                    ]
                }
            ],
            "output_type": "generate_until",
            "generation_kwargs": {
                "do_sample": false,
                "until": [
                    "."
                ],
                "max_gen_toks": 10
            },
            "repeats": 1,
            "filter_list": [
                {
                    "name": "flexible-extract",
                    "filter": [
                        {
                            "function": "multi_choice_regex",
                            "group_select": -1,
                            "ignore_case": true,
                            "ignore_punctuation": true,
                            "regex_pattern": "[tT]he best answer is ([A-D])"
                        },
                        {
                            "function": "take_first"
                        }
                    ]
                }
            ],
            "should_decontaminate": false,
            "gen_prefix": "The best answer is",
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_llama_formal_logic": {
            "task": "mmlu_llama_formal_logic",
            "task_alias": "formal logic",
            "tag": "mmlu_llama_humanities_tasks",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "formal_logic",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Given the following question and four candidate answers (A, B, C and D), choose the best answer.\nQuestion: {{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nYour response should end with \"The best answer is [the_answer_letter]\" where the [the_answer_letter] is one of A, B, C or D.",
            "doc_to_target": "{{['A.','B.','C.','D.'][answer]}}",
            "unsafe_code": false,
            "description": "",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "exact_match",
                    "aggregation": "mean",
                    "higher_is_better": true,
                    "ignore_case": true,
                    "ignore_punctuation": true,
                    "regexes_to_ignore": [
                        "\\$",
                        "\\.$"
                    ]
                }
            ],
            "output_type": "generate_until",
            "generation_kwargs": {
                "do_sample": false,
                "until": [
                    "."
                ],
                "max_gen_toks": 10
            },
            "repeats": 1,
            "filter_list": [
                {
                    "name": "flexible-extract",
                    "filter": [
                        {
                            "function": "multi_choice_regex",
                            "group_select": -1,
                            "ignore_case": true,
                            "ignore_punctuation": true,
                            "regex_pattern": "[tT]he best answer is ([A-D])"
                        },
                        {
                            "function": "take_first"
                        }
                    ]
                }
            ],
            "should_decontaminate": false,
            "gen_prefix": "The best answer is",
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_llama_global_facts": {
            "task": "mmlu_llama_global_facts",
            "task_alias": "global facts",
            "tag": "mmlu_llama_other_tasks",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "global_facts",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Given the following question and four candidate answers (A, B, C and D), choose the best answer.\nQuestion: {{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nYour response should end with \"The best answer is [the_answer_letter]\" where the [the_answer_letter] is one of A, B, C or D.",
            "doc_to_target": "{{['A.','B.','C.','D.'][answer]}}",
            "unsafe_code": false,
            "description": "",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "exact_match",
                    "aggregation": "mean",
                    "higher_is_better": true,
                    "ignore_case": true,
                    "ignore_punctuation": true,
                    "regexes_to_ignore": [
                        "\\$",
                        "\\.$"
                    ]
                }
            ],
            "output_type": "generate_until",
            "generation_kwargs": {
                "do_sample": false,
                "until": [
                    "."
                ],
                "max_gen_toks": 10
            },
            "repeats": 1,
            "filter_list": [
                {
                    "name": "flexible-extract",
                    "filter": [
                        {
                            "function": "multi_choice_regex",
                            "group_select": -1,
                            "ignore_case": true,
                            "ignore_punctuation": true,
                            "regex_pattern": "[tT]he best answer is ([A-D])"
                        },
                        {
                            "function": "take_first"
                        }
                    ]
                }
            ],
            "should_decontaminate": false,
            "gen_prefix": "The best answer is",
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_llama_high_school_biology": {
            "task": "mmlu_llama_high_school_biology",
            "task_alias": "high school biology",
            "tag": "mmlu_llama_stem_tasks",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "high_school_biology",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Given the following question and four candidate answers (A, B, C and D), choose the best answer.\nQuestion: {{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nYour response should end with \"The best answer is [the_answer_letter]\" where the [the_answer_letter] is one of A, B, C or D.",
            "doc_to_target": "{{['A.','B.','C.','D.'][answer]}}",
            "unsafe_code": false,
            "description": "",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "exact_match",
                    "aggregation": "mean",
                    "higher_is_better": true,
                    "ignore_case": true,
                    "ignore_punctuation": true,
                    "regexes_to_ignore": [
                        "\\$",
                        "\\.$"
                    ]
                }
            ],
            "output_type": "generate_until",
            "generation_kwargs": {
                "do_sample": false,
                "until": [
                    "."
                ],
                "max_gen_toks": 10
            },
            "repeats": 1,
            "filter_list": [
                {
                    "name": "flexible-extract",
                    "filter": [
                        {
                            "function": "multi_choice_regex",
                            "group_select": -1,
                            "ignore_case": true,
                            "ignore_punctuation": true,
                            "regex_pattern": "[tT]he best answer is ([A-D])"
                        },
                        {
                            "function": "take_first"
                        }
                    ]
                }
            ],
            "should_decontaminate": false,
            "gen_prefix": "The best answer is",
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_llama_high_school_chemistry": {
            "task": "mmlu_llama_high_school_chemistry",
            "task_alias": "high school chemistry",
            "tag": "mmlu_llama_stem_tasks",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "high_school_chemistry",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Given the following question and four candidate answers (A, B, C and D), choose the best answer.\nQuestion: {{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nYour response should end with \"The best answer is [the_answer_letter]\" where the [the_answer_letter] is one of A, B, C or D.",
            "doc_to_target": "{{['A.','B.','C.','D.'][answer]}}",
            "unsafe_code": false,
            "description": "",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "exact_match",
                    "aggregation": "mean",
                    "higher_is_better": true,
                    "ignore_case": true,
                    "ignore_punctuation": true,
                    "regexes_to_ignore": [
                        "\\$",
                        "\\.$"
                    ]
                }
            ],
            "output_type": "generate_until",
            "generation_kwargs": {
                "do_sample": false,
                "until": [
                    "."
                ],
                "max_gen_toks": 10
            },
            "repeats": 1,
            "filter_list": [
                {
                    "name": "flexible-extract",
                    "filter": [
                        {
                            "function": "multi_choice_regex",
                            "group_select": -1,
                            "ignore_case": true,
                            "ignore_punctuation": true,
                            "regex_pattern": "[tT]he best answer is ([A-D])"
                        },
                        {
                            "function": "take_first"
                        }
                    ]
                }
            ],
            "should_decontaminate": false,
            "gen_prefix": "The best answer is",
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_llama_high_school_computer_science": {
            "task": "mmlu_llama_high_school_computer_science",
            "task_alias": "high school computer science",
            "tag": "mmlu_llama_stem_tasks",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "high_school_computer_science",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Given the following question and four candidate answers (A, B, C and D), choose the best answer.\nQuestion: {{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nYour response should end with \"The best answer is [the_answer_letter]\" where the [the_answer_letter] is one of A, B, C or D.",
            "doc_to_target": "{{['A.','B.','C.','D.'][answer]}}",
            "unsafe_code": false,
            "description": "",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "exact_match",
                    "aggregation": "mean",
                    "higher_is_better": true,
                    "ignore_case": true,
                    "ignore_punctuation": true,
                    "regexes_to_ignore": [
                        "\\$",
                        "\\.$"
                    ]
                }
            ],
            "output_type": "generate_until",
            "generation_kwargs": {
                "do_sample": false,
                "until": [
                    "."
                ],
                "max_gen_toks": 10
            },
            "repeats": 1,
            "filter_list": [
                {
                    "name": "flexible-extract",
                    "filter": [
                        {
                            "function": "multi_choice_regex",
                            "group_select": -1,
                            "ignore_case": true,
                            "ignore_punctuation": true,
                            "regex_pattern": "[tT]he best answer is ([A-D])"
                        },
                        {
                            "function": "take_first"
                        }
                    ]
                }
            ],
            "should_decontaminate": false,
            "gen_prefix": "The best answer is",
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_llama_high_school_european_history": {
            "task": "mmlu_llama_high_school_european_history",
            "task_alias": "high school european history",
            "tag": "mmlu_llama_humanities_tasks",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "high_school_european_history",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Given the following question and four candidate answers (A, B, C and D), choose the best answer.\nQuestion: {{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nYour response should end with \"The best answer is [the_answer_letter]\" where the [the_answer_letter] is one of A, B, C or D.",
            "doc_to_target": "{{['A.','B.','C.','D.'][answer]}}",
            "unsafe_code": false,
            "description": "",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "exact_match",
                    "aggregation": "mean",
                    "higher_is_better": true,
                    "ignore_case": true,
                    "ignore_punctuation": true,
                    "regexes_to_ignore": [
                        "\\$",
                        "\\.$"
                    ]
                }
            ],
            "output_type": "generate_until",
            "generation_kwargs": {
                "do_sample": false,
                "until": [
                    "."
                ],
                "max_gen_toks": 10
            },
            "repeats": 1,
            "filter_list": [
                {
                    "name": "flexible-extract",
                    "filter": [
                        {
                            "function": "multi_choice_regex",
                            "group_select": -1,
                            "ignore_case": true,
                            "ignore_punctuation": true,
                            "regex_pattern": "[tT]he best answer is ([A-D])"
                        },
                        {
                            "function": "take_first"
                        }
                    ]
                }
            ],
            "should_decontaminate": false,
            "gen_prefix": "The best answer is",
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_llama_high_school_geography": {
            "task": "mmlu_llama_high_school_geography",
            "task_alias": "high school geography",
            "tag": "mmlu_llama_social_sciences_tasks",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "high_school_geography",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Given the following question and four candidate answers (A, B, C and D), choose the best answer.\nQuestion: {{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nYour response should end with \"The best answer is [the_answer_letter]\" where the [the_answer_letter] is one of A, B, C or D.",
            "doc_to_target": "{{['A.','B.','C.','D.'][answer]}}",
            "unsafe_code": false,
            "description": "",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "exact_match",
                    "aggregation": "mean",
                    "higher_is_better": true,
                    "ignore_case": true,
                    "ignore_punctuation": true,
                    "regexes_to_ignore": [
                        "\\$",
                        "\\.$"
                    ]
                }
            ],
            "output_type": "generate_until",
            "generation_kwargs": {
                "do_sample": false,
                "until": [
                    "."
                ],
                "max_gen_toks": 10
            },
            "repeats": 1,
            "filter_list": [
                {
                    "name": "flexible-extract",
                    "filter": [
                        {
                            "function": "multi_choice_regex",
                            "group_select": -1,
                            "ignore_case": true,
                            "ignore_punctuation": true,
                            "regex_pattern": "[tT]he best answer is ([A-D])"
                        },
                        {
                            "function": "take_first"
                        }
                    ]
                }
            ],
            "should_decontaminate": false,
            "gen_prefix": "The best answer is",
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_llama_high_school_government_and_politics": {
            "task": "mmlu_llama_high_school_government_and_politics",
            "task_alias": "high school government and politics",
            "tag": "mmlu_llama_social_sciences_tasks",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "high_school_government_and_politics",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Given the following question and four candidate answers (A, B, C and D), choose the best answer.\nQuestion: {{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nYour response should end with \"The best answer is [the_answer_letter]\" where the [the_answer_letter] is one of A, B, C or D.",
            "doc_to_target": "{{['A.','B.','C.','D.'][answer]}}",
            "unsafe_code": false,
            "description": "",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "exact_match",
                    "aggregation": "mean",
                    "higher_is_better": true,
                    "ignore_case": true,
                    "ignore_punctuation": true,
                    "regexes_to_ignore": [
                        "\\$",
                        "\\.$"
                    ]
                }
            ],
            "output_type": "generate_until",
            "generation_kwargs": {
                "do_sample": false,
                "until": [
                    "."
                ],
                "max_gen_toks": 10
            },
            "repeats": 1,
            "filter_list": [
                {
                    "name": "flexible-extract",
                    "filter": [
                        {
                            "function": "multi_choice_regex",
                            "group_select": -1,
                            "ignore_case": true,
                            "ignore_punctuation": true,
                            "regex_pattern": "[tT]he best answer is ([A-D])"
                        },
                        {
                            "function": "take_first"
                        }
                    ]
                }
            ],
            "should_decontaminate": false,
            "gen_prefix": "The best answer is",
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_llama_high_school_macroeconomics": {
            "task": "mmlu_llama_high_school_macroeconomics",
            "task_alias": "high school macroeconomics",
            "tag": "mmlu_llama_social_sciences_tasks",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "high_school_macroeconomics",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Given the following question and four candidate answers (A, B, C and D), choose the best answer.\nQuestion: {{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nYour response should end with \"The best answer is [the_answer_letter]\" where the [the_answer_letter] is one of A, B, C or D.",
            "doc_to_target": "{{['A.','B.','C.','D.'][answer]}}",
            "unsafe_code": false,
            "description": "",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "exact_match",
                    "aggregation": "mean",
                    "higher_is_better": true,
                    "ignore_case": true,
                    "ignore_punctuation": true,
                    "regexes_to_ignore": [
                        "\\$",
                        "\\.$"
                    ]
                }
            ],
            "output_type": "generate_until",
            "generation_kwargs": {
                "do_sample": false,
                "until": [
                    "."
                ],
                "max_gen_toks": 10
            },
            "repeats": 1,
            "filter_list": [
                {
                    "name": "flexible-extract",
                    "filter": [
                        {
                            "function": "multi_choice_regex",
                            "group_select": -1,
                            "ignore_case": true,
                            "ignore_punctuation": true,
                            "regex_pattern": "[tT]he best answer is ([A-D])"
                        },
                        {
                            "function": "take_first"
                        }
                    ]
                }
            ],
            "should_decontaminate": false,
            "gen_prefix": "The best answer is",
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_llama_high_school_mathematics": {
            "task": "mmlu_llama_high_school_mathematics",
            "task_alias": "high school mathematics",
            "tag": "mmlu_llama_stem_tasks",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "high_school_mathematics",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Given the following question and four candidate answers (A, B, C and D), choose the best answer.\nQuestion: {{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nYour response should end with \"The best answer is [the_answer_letter]\" where the [the_answer_letter] is one of A, B, C or D.",
            "doc_to_target": "{{['A.','B.','C.','D.'][answer]}}",
            "unsafe_code": false,
            "description": "",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "exact_match",
                    "aggregation": "mean",
                    "higher_is_better": true,
                    "ignore_case": true,
                    "ignore_punctuation": true,
                    "regexes_to_ignore": [
                        "\\$",
                        "\\.$"
                    ]
                }
            ],
            "output_type": "generate_until",
            "generation_kwargs": {
                "do_sample": false,
                "until": [
                    "."
                ],
                "max_gen_toks": 10
            },
            "repeats": 1,
            "filter_list": [
                {
                    "name": "flexible-extract",
                    "filter": [
                        {
                            "function": "multi_choice_regex",
                            "group_select": -1,
                            "ignore_case": true,
                            "ignore_punctuation": true,
                            "regex_pattern": "[tT]he best answer is ([A-D])"
                        },
                        {
                            "function": "take_first"
                        }
                    ]
                }
            ],
            "should_decontaminate": false,
            "gen_prefix": "The best answer is",
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_llama_high_school_microeconomics": {
            "task": "mmlu_llama_high_school_microeconomics",
            "task_alias": "high school microeconomics",
            "tag": "mmlu_llama_social_sciences_tasks",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "high_school_microeconomics",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Given the following question and four candidate answers (A, B, C and D), choose the best answer.\nQuestion: {{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nYour response should end with \"The best answer is [the_answer_letter]\" where the [the_answer_letter] is one of A, B, C or D.",
            "doc_to_target": "{{['A.','B.','C.','D.'][answer]}}",
            "unsafe_code": false,
            "description": "",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "exact_match",
                    "aggregation": "mean",
                    "higher_is_better": true,
                    "ignore_case": true,
                    "ignore_punctuation": true,
                    "regexes_to_ignore": [
                        "\\$",
                        "\\.$"
                    ]
                }
            ],
            "output_type": "generate_until",
            "generation_kwargs": {
                "do_sample": false,
                "until": [
                    "."
                ],
                "max_gen_toks": 10
            },
            "repeats": 1,
            "filter_list": [
                {
                    "name": "flexible-extract",
                    "filter": [
                        {
                            "function": "multi_choice_regex",
                            "group_select": -1,
                            "ignore_case": true,
                            "ignore_punctuation": true,
                            "regex_pattern": "[tT]he best answer is ([A-D])"
                        },
                        {
                            "function": "take_first"
                        }
                    ]
                }
            ],
            "should_decontaminate": false,
            "gen_prefix": "The best answer is",
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_llama_high_school_physics": {
            "task": "mmlu_llama_high_school_physics",
            "task_alias": "high school physics",
            "tag": "mmlu_llama_stem_tasks",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "high_school_physics",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Given the following question and four candidate answers (A, B, C and D), choose the best answer.\nQuestion: {{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nYour response should end with \"The best answer is [the_answer_letter]\" where the [the_answer_letter] is one of A, B, C or D.",
            "doc_to_target": "{{['A.','B.','C.','D.'][answer]}}",
            "unsafe_code": false,
            "description": "",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "exact_match",
                    "aggregation": "mean",
                    "higher_is_better": true,
                    "ignore_case": true,
                    "ignore_punctuation": true,
                    "regexes_to_ignore": [
                        "\\$",
                        "\\.$"
                    ]
                }
            ],
            "output_type": "generate_until",
            "generation_kwargs": {
                "do_sample": false,
                "until": [
                    "."
                ],
                "max_gen_toks": 10
            },
            "repeats": 1,
            "filter_list": [
                {
                    "name": "flexible-extract",
                    "filter": [
                        {
                            "function": "multi_choice_regex",
                            "group_select": -1,
                            "ignore_case": true,
                            "ignore_punctuation": true,
                            "regex_pattern": "[tT]he best answer is ([A-D])"
                        },
                        {
                            "function": "take_first"
                        }
                    ]
                }
            ],
            "should_decontaminate": false,
            "gen_prefix": "The best answer is",
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_llama_high_school_psychology": {
            "task": "mmlu_llama_high_school_psychology",
            "task_alias": "high school psychology",
            "tag": "mmlu_llama_social_sciences_tasks",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "high_school_psychology",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Given the following question and four candidate answers (A, B, C and D), choose the best answer.\nQuestion: {{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nYour response should end with \"The best answer is [the_answer_letter]\" where the [the_answer_letter] is one of A, B, C or D.",
            "doc_to_target": "{{['A.','B.','C.','D.'][answer]}}",
            "unsafe_code": false,
            "description": "",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "exact_match",
                    "aggregation": "mean",
                    "higher_is_better": true,
                    "ignore_case": true,
                    "ignore_punctuation": true,
                    "regexes_to_ignore": [
                        "\\$",
                        "\\.$"
                    ]
                }
            ],
            "output_type": "generate_until",
            "generation_kwargs": {
                "do_sample": false,
                "until": [
                    "."
                ],
                "max_gen_toks": 10
            },
            "repeats": 1,
            "filter_list": [
                {
                    "name": "flexible-extract",
                    "filter": [
                        {
                            "function": "multi_choice_regex",
                            "group_select": -1,
                            "ignore_case": true,
                            "ignore_punctuation": true,
                            "regex_pattern": "[tT]he best answer is ([A-D])"
                        },
                        {
                            "function": "take_first"
                        }
                    ]
                }
            ],
            "should_decontaminate": false,
            "gen_prefix": "The best answer is",
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_llama_high_school_statistics": {
            "task": "mmlu_llama_high_school_statistics",
            "task_alias": "high school statistics",
            "tag": "mmlu_llama_stem_tasks",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "high_school_statistics",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Given the following question and four candidate answers (A, B, C and D), choose the best answer.\nQuestion: {{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nYour response should end with \"The best answer is [the_answer_letter]\" where the [the_answer_letter] is one of A, B, C or D.",
            "doc_to_target": "{{['A.','B.','C.','D.'][answer]}}",
            "unsafe_code": false,
            "description": "",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "exact_match",
                    "aggregation": "mean",
                    "higher_is_better": true,
                    "ignore_case": true,
                    "ignore_punctuation": true,
                    "regexes_to_ignore": [
                        "\\$",
                        "\\.$"
                    ]
                }
            ],
            "output_type": "generate_until",
            "generation_kwargs": {
                "do_sample": false,
                "until": [
                    "."
                ],
                "max_gen_toks": 10
            },
            "repeats": 1,
            "filter_list": [
                {
                    "name": "flexible-extract",
                    "filter": [
                        {
                            "function": "multi_choice_regex",
                            "group_select": -1,
                            "ignore_case": true,
                            "ignore_punctuation": true,
                            "regex_pattern": "[tT]he best answer is ([A-D])"
                        },
                        {
                            "function": "take_first"
                        }
                    ]
                }
            ],
            "should_decontaminate": false,
            "gen_prefix": "The best answer is",
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_llama_high_school_us_history": {
            "task": "mmlu_llama_high_school_us_history",
            "task_alias": "high school us history",
            "tag": "mmlu_llama_humanities_tasks",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "high_school_us_history",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Given the following question and four candidate answers (A, B, C and D), choose the best answer.\nQuestion: {{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nYour response should end with \"The best answer is [the_answer_letter]\" where the [the_answer_letter] is one of A, B, C or D.",
            "doc_to_target": "{{['A.','B.','C.','D.'][answer]}}",
            "unsafe_code": false,
            "description": "",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "exact_match",
                    "aggregation": "mean",
                    "higher_is_better": true,
                    "ignore_case": true,
                    "ignore_punctuation": true,
                    "regexes_to_ignore": [
                        "\\$",
                        "\\.$"
                    ]
                }
            ],
            "output_type": "generate_until",
            "generation_kwargs": {
                "do_sample": false,
                "until": [
                    "."
                ],
                "max_gen_toks": 10
            },
            "repeats": 1,
            "filter_list": [
                {
                    "name": "flexible-extract",
                    "filter": [
                        {
                            "function": "multi_choice_regex",
                            "group_select": -1,
                            "ignore_case": true,
                            "ignore_punctuation": true,
                            "regex_pattern": "[tT]he best answer is ([A-D])"
                        },
                        {
                            "function": "take_first"
                        }
                    ]
                }
            ],
            "should_decontaminate": false,
            "gen_prefix": "The best answer is",
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_llama_high_school_world_history": {
            "task": "mmlu_llama_high_school_world_history",
            "task_alias": "high school world history",
            "tag": "mmlu_llama_humanities_tasks",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "high_school_world_history",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Given the following question and four candidate answers (A, B, C and D), choose the best answer.\nQuestion: {{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nYour response should end with \"The best answer is [the_answer_letter]\" where the [the_answer_letter] is one of A, B, C or D.",
            "doc_to_target": "{{['A.','B.','C.','D.'][answer]}}",
            "unsafe_code": false,
            "description": "",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "exact_match",
                    "aggregation": "mean",
                    "higher_is_better": true,
                    "ignore_case": true,
                    "ignore_punctuation": true,
                    "regexes_to_ignore": [
                        "\\$",
                        "\\.$"
                    ]
                }
            ],
            "output_type": "generate_until",
            "generation_kwargs": {
                "do_sample": false,
                "until": [
                    "."
                ],
                "max_gen_toks": 10
            },
            "repeats": 1,
            "filter_list": [
                {
                    "name": "flexible-extract",
                    "filter": [
                        {
                            "function": "multi_choice_regex",
                            "group_select": -1,
                            "ignore_case": true,
                            "ignore_punctuation": true,
                            "regex_pattern": "[tT]he best answer is ([A-D])"
                        },
                        {
                            "function": "take_first"
                        }
                    ]
                }
            ],
            "should_decontaminate": false,
            "gen_prefix": "The best answer is",
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_llama_human_aging": {
            "task": "mmlu_llama_human_aging",
            "task_alias": "human aging",
            "tag": "mmlu_llama_other_tasks",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "human_aging",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Given the following question and four candidate answers (A, B, C and D), choose the best answer.\nQuestion: {{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nYour response should end with \"The best answer is [the_answer_letter]\" where the [the_answer_letter] is one of A, B, C or D.",
            "doc_to_target": "{{['A.','B.','C.','D.'][answer]}}",
            "unsafe_code": false,
            "description": "",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "exact_match",
                    "aggregation": "mean",
                    "higher_is_better": true,
                    "ignore_case": true,
                    "ignore_punctuation": true,
                    "regexes_to_ignore": [
                        "\\$",
                        "\\.$"
                    ]
                }
            ],
            "output_type": "generate_until",
            "generation_kwargs": {
                "do_sample": false,
                "until": [
                    "."
                ],
                "max_gen_toks": 10
            },
            "repeats": 1,
            "filter_list": [
                {
                    "name": "flexible-extract",
                    "filter": [
                        {
                            "function": "multi_choice_regex",
                            "group_select": -1,
                            "ignore_case": true,
                            "ignore_punctuation": true,
                            "regex_pattern": "[tT]he best answer is ([A-D])"
                        },
                        {
                            "function": "take_first"
                        }
                    ]
                }
            ],
            "should_decontaminate": false,
            "gen_prefix": "The best answer is",
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_llama_human_sexuality": {
            "task": "mmlu_llama_human_sexuality",
            "task_alias": "human sexuality",
            "tag": "mmlu_llama_social_sciences_tasks",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "human_sexuality",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Given the following question and four candidate answers (A, B, C and D), choose the best answer.\nQuestion: {{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nYour response should end with \"The best answer is [the_answer_letter]\" where the [the_answer_letter] is one of A, B, C or D.",
            "doc_to_target": "{{['A.','B.','C.','D.'][answer]}}",
            "unsafe_code": false,
            "description": "",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "exact_match",
                    "aggregation": "mean",
                    "higher_is_better": true,
                    "ignore_case": true,
                    "ignore_punctuation": true,
                    "regexes_to_ignore": [
                        "\\$",
                        "\\.$"
                    ]
                }
            ],
            "output_type": "generate_until",
            "generation_kwargs": {
                "do_sample": false,
                "until": [
                    "."
                ],
                "max_gen_toks": 10
            },
            "repeats": 1,
            "filter_list": [
                {
                    "name": "flexible-extract",
                    "filter": [
                        {
                            "function": "multi_choice_regex",
                            "group_select": -1,
                            "ignore_case": true,
                            "ignore_punctuation": true,
                            "regex_pattern": "[tT]he best answer is ([A-D])"
                        },
                        {
                            "function": "take_first"
                        }
                    ]
                }
            ],
            "should_decontaminate": false,
            "gen_prefix": "The best answer is",
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_llama_international_law": {
            "task": "mmlu_llama_international_law",
            "task_alias": "international law",
            "tag": "mmlu_llama_humanities_tasks",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "international_law",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Given the following question and four candidate answers (A, B, C and D), choose the best answer.\nQuestion: {{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nYour response should end with \"The best answer is [the_answer_letter]\" where the [the_answer_letter] is one of A, B, C or D.",
            "doc_to_target": "{{['A.','B.','C.','D.'][answer]}}",
            "unsafe_code": false,
            "description": "",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "exact_match",
                    "aggregation": "mean",
                    "higher_is_better": true,
                    "ignore_case": true,
                    "ignore_punctuation": true,
                    "regexes_to_ignore": [
                        "\\$",
                        "\\.$"
                    ]
                }
            ],
            "output_type": "generate_until",
            "generation_kwargs": {
                "do_sample": false,
                "until": [
                    "."
                ],
                "max_gen_toks": 10
            },
            "repeats": 1,
            "filter_list": [
                {
                    "name": "flexible-extract",
                    "filter": [
                        {
                            "function": "multi_choice_regex",
                            "group_select": -1,
                            "ignore_case": true,
                            "ignore_punctuation": true,
                            "regex_pattern": "[tT]he best answer is ([A-D])"
                        },
                        {
                            "function": "take_first"
                        }
                    ]
                }
            ],
            "should_decontaminate": false,
            "gen_prefix": "The best answer is",
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_llama_jurisprudence": {
            "task": "mmlu_llama_jurisprudence",
            "task_alias": "jurisprudence",
            "tag": "mmlu_llama_humanities_tasks",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "jurisprudence",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Given the following question and four candidate answers (A, B, C and D), choose the best answer.\nQuestion: {{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nYour response should end with \"The best answer is [the_answer_letter]\" where the [the_answer_letter] is one of A, B, C or D.",
            "doc_to_target": "{{['A.','B.','C.','D.'][answer]}}",
            "unsafe_code": false,
            "description": "",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "exact_match",
                    "aggregation": "mean",
                    "higher_is_better": true,
                    "ignore_case": true,
                    "ignore_punctuation": true,
                    "regexes_to_ignore": [
                        "\\$",
                        "\\.$"
                    ]
                }
            ],
            "output_type": "generate_until",
            "generation_kwargs": {
                "do_sample": false,
                "until": [
                    "."
                ],
                "max_gen_toks": 10
            },
            "repeats": 1,
            "filter_list": [
                {
                    "name": "flexible-extract",
                    "filter": [
                        {
                            "function": "multi_choice_regex",
                            "group_select": -1,
                            "ignore_case": true,
                            "ignore_punctuation": true,
                            "regex_pattern": "[tT]he best answer is ([A-D])"
                        },
                        {
                            "function": "take_first"
                        }
                    ]
                }
            ],
            "should_decontaminate": false,
            "gen_prefix": "The best answer is",
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_llama_logical_fallacies": {
            "task": "mmlu_llama_logical_fallacies",
            "task_alias": "logical fallacies",
            "tag": "mmlu_llama_humanities_tasks",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "logical_fallacies",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Given the following question and four candidate answers (A, B, C and D), choose the best answer.\nQuestion: {{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nYour response should end with \"The best answer is [the_answer_letter]\" where the [the_answer_letter] is one of A, B, C or D.",
            "doc_to_target": "{{['A.','B.','C.','D.'][answer]}}",
            "unsafe_code": false,
            "description": "",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "exact_match",
                    "aggregation": "mean",
                    "higher_is_better": true,
                    "ignore_case": true,
                    "ignore_punctuation": true,
                    "regexes_to_ignore": [
                        "\\$",
                        "\\.$"
                    ]
                }
            ],
            "output_type": "generate_until",
            "generation_kwargs": {
                "do_sample": false,
                "until": [
                    "."
                ],
                "max_gen_toks": 10
            },
            "repeats": 1,
            "filter_list": [
                {
                    "name": "flexible-extract",
                    "filter": [
                        {
                            "function": "multi_choice_regex",
                            "group_select": -1,
                            "ignore_case": true,
                            "ignore_punctuation": true,
                            "regex_pattern": "[tT]he best answer is ([A-D])"
                        },
                        {
                            "function": "take_first"
                        }
                    ]
                }
            ],
            "should_decontaminate": false,
            "gen_prefix": "The best answer is",
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_llama_machine_learning": {
            "task": "mmlu_llama_machine_learning",
            "task_alias": "machine learning",
            "tag": "mmlu_llama_stem_tasks",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "machine_learning",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Given the following question and four candidate answers (A, B, C and D), choose the best answer.\nQuestion: {{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nYour response should end with \"The best answer is [the_answer_letter]\" where the [the_answer_letter] is one of A, B, C or D.",
            "doc_to_target": "{{['A.','B.','C.','D.'][answer]}}",
            "unsafe_code": false,
            "description": "",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "exact_match",
                    "aggregation": "mean",
                    "higher_is_better": true,
                    "ignore_case": true,
                    "ignore_punctuation": true,
                    "regexes_to_ignore": [
                        "\\$",
                        "\\.$"
                    ]
                }
            ],
            "output_type": "generate_until",
            "generation_kwargs": {
                "do_sample": false,
                "until": [
                    "."
                ],
                "max_gen_toks": 10
            },
            "repeats": 1,
            "filter_list": [
                {
                    "name": "flexible-extract",
                    "filter": [
                        {
                            "function": "multi_choice_regex",
                            "group_select": -1,
                            "ignore_case": true,
                            "ignore_punctuation": true,
                            "regex_pattern": "[tT]he best answer is ([A-D])"
                        },
                        {
                            "function": "take_first"
                        }
                    ]
                }
            ],
            "should_decontaminate": false,
            "gen_prefix": "The best answer is",
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_llama_management": {
            "task": "mmlu_llama_management",
            "task_alias": "management",
            "tag": "mmlu_llama_other_tasks",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "management",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Given the following question and four candidate answers (A, B, C and D), choose the best answer.\nQuestion: {{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nYour response should end with \"The best answer is [the_answer_letter]\" where the [the_answer_letter] is one of A, B, C or D.",
            "doc_to_target": "{{['A.','B.','C.','D.'][answer]}}",
            "unsafe_code": false,
            "description": "",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "exact_match",
                    "aggregation": "mean",
                    "higher_is_better": true,
                    "ignore_case": true,
                    "ignore_punctuation": true,
                    "regexes_to_ignore": [
                        "\\$",
                        "\\.$"
                    ]
                }
            ],
            "output_type": "generate_until",
            "generation_kwargs": {
                "do_sample": false,
                "until": [
                    "."
                ],
                "max_gen_toks": 10
            },
            "repeats": 1,
            "filter_list": [
                {
                    "name": "flexible-extract",
                    "filter": [
                        {
                            "function": "multi_choice_regex",
                            "group_select": -1,
                            "ignore_case": true,
                            "ignore_punctuation": true,
                            "regex_pattern": "[tT]he best answer is ([A-D])"
                        },
                        {
                            "function": "take_first"
                        }
                    ]
                }
            ],
            "should_decontaminate": false,
            "gen_prefix": "The best answer is",
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_llama_marketing": {
            "task": "mmlu_llama_marketing",
            "task_alias": "marketing",
            "tag": "mmlu_llama_other_tasks",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "marketing",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Given the following question and four candidate answers (A, B, C and D), choose the best answer.\nQuestion: {{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nYour response should end with \"The best answer is [the_answer_letter]\" where the [the_answer_letter] is one of A, B, C or D.",
            "doc_to_target": "{{['A.','B.','C.','D.'][answer]}}",
            "unsafe_code": false,
            "description": "",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "exact_match",
                    "aggregation": "mean",
                    "higher_is_better": true,
                    "ignore_case": true,
                    "ignore_punctuation": true,
                    "regexes_to_ignore": [
                        "\\$",
                        "\\.$"
                    ]
                }
            ],
            "output_type": "generate_until",
            "generation_kwargs": {
                "do_sample": false,
                "until": [
                    "."
                ],
                "max_gen_toks": 10
            },
            "repeats": 1,
            "filter_list": [
                {
                    "name": "flexible-extract",
                    "filter": [
                        {
                            "function": "multi_choice_regex",
                            "group_select": -1,
                            "ignore_case": true,
                            "ignore_punctuation": true,
                            "regex_pattern": "[tT]he best answer is ([A-D])"
                        },
                        {
                            "function": "take_first"
                        }
                    ]
                }
            ],
            "should_decontaminate": false,
            "gen_prefix": "The best answer is",
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_llama_medical_genetics": {
            "task": "mmlu_llama_medical_genetics",
            "task_alias": "medical genetics",
            "tag": "mmlu_llama_other_tasks",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "medical_genetics",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Given the following question and four candidate answers (A, B, C and D), choose the best answer.\nQuestion: {{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nYour response should end with \"The best answer is [the_answer_letter]\" where the [the_answer_letter] is one of A, B, C or D.",
            "doc_to_target": "{{['A.','B.','C.','D.'][answer]}}",
            "unsafe_code": false,
            "description": "",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "exact_match",
                    "aggregation": "mean",
                    "higher_is_better": true,
                    "ignore_case": true,
                    "ignore_punctuation": true,
                    "regexes_to_ignore": [
                        "\\$",
                        "\\.$"
                    ]
                }
            ],
            "output_type": "generate_until",
            "generation_kwargs": {
                "do_sample": false,
                "until": [
                    "."
                ],
                "max_gen_toks": 10
            },
            "repeats": 1,
            "filter_list": [
                {
                    "name": "flexible-extract",
                    "filter": [
                        {
                            "function": "multi_choice_regex",
                            "group_select": -1,
                            "ignore_case": true,
                            "ignore_punctuation": true,
                            "regex_pattern": "[tT]he best answer is ([A-D])"
                        },
                        {
                            "function": "take_first"
                        }
                    ]
                }
            ],
            "should_decontaminate": false,
            "gen_prefix": "The best answer is",
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_llama_miscellaneous": {
            "task": "mmlu_llama_miscellaneous",
            "task_alias": "miscellaneous",
            "tag": "mmlu_llama_other_tasks",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "miscellaneous",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Given the following question and four candidate answers (A, B, C and D), choose the best answer.\nQuestion: {{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nYour response should end with \"The best answer is [the_answer_letter]\" where the [the_answer_letter] is one of A, B, C or D.",
            "doc_to_target": "{{['A.','B.','C.','D.'][answer]}}",
            "unsafe_code": false,
            "description": "",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "exact_match",
                    "aggregation": "mean",
                    "higher_is_better": true,
                    "ignore_case": true,
                    "ignore_punctuation": true,
                    "regexes_to_ignore": [
                        "\\$",
                        "\\.$"
                    ]
                }
            ],
            "output_type": "generate_until",
            "generation_kwargs": {
                "do_sample": false,
                "until": [
                    "."
                ],
                "max_gen_toks": 10
            },
            "repeats": 1,
            "filter_list": [
                {
                    "name": "flexible-extract",
                    "filter": [
                        {
                            "function": "multi_choice_regex",
                            "group_select": -1,
                            "ignore_case": true,
                            "ignore_punctuation": true,
                            "regex_pattern": "[tT]he best answer is ([A-D])"
                        },
                        {
                            "function": "take_first"
                        }
                    ]
                }
            ],
            "should_decontaminate": false,
            "gen_prefix": "The best answer is",
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_llama_moral_disputes": {
            "task": "mmlu_llama_moral_disputes",
            "task_alias": "moral disputes",
            "tag": "mmlu_llama_humanities_tasks",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "moral_disputes",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Given the following question and four candidate answers (A, B, C and D), choose the best answer.\nQuestion: {{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nYour response should end with \"The best answer is [the_answer_letter]\" where the [the_answer_letter] is one of A, B, C or D.",
            "doc_to_target": "{{['A.','B.','C.','D.'][answer]}}",
            "unsafe_code": false,
            "description": "",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "exact_match",
                    "aggregation": "mean",
                    "higher_is_better": true,
                    "ignore_case": true,
                    "ignore_punctuation": true,
                    "regexes_to_ignore": [
                        "\\$",
                        "\\.$"
                    ]
                }
            ],
            "output_type": "generate_until",
            "generation_kwargs": {
                "do_sample": false,
                "until": [
                    "."
                ],
                "max_gen_toks": 10
            },
            "repeats": 1,
            "filter_list": [
                {
                    "name": "flexible-extract",
                    "filter": [
                        {
                            "function": "multi_choice_regex",
                            "group_select": -1,
                            "ignore_case": true,
                            "ignore_punctuation": true,
                            "regex_pattern": "[tT]he best answer is ([A-D])"
                        },
                        {
                            "function": "take_first"
                        }
                    ]
                }
            ],
            "should_decontaminate": false,
            "gen_prefix": "The best answer is",
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_llama_moral_scenarios": {
            "task": "mmlu_llama_moral_scenarios",
            "task_alias": "moral scenarios",
            "tag": "mmlu_llama_humanities_tasks",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "moral_scenarios",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Given the following question and four candidate answers (A, B, C and D), choose the best answer.\nQuestion: {{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nYour response should end with \"The best answer is [the_answer_letter]\" where the [the_answer_letter] is one of A, B, C or D.",
            "doc_to_target": "{{['A.','B.','C.','D.'][answer]}}",
            "unsafe_code": false,
            "description": "",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "exact_match",
                    "aggregation": "mean",
                    "higher_is_better": true,
                    "ignore_case": true,
                    "ignore_punctuation": true,
                    "regexes_to_ignore": [
                        "\\$",
                        "\\.$"
                    ]
                }
            ],
            "output_type": "generate_until",
            "generation_kwargs": {
                "do_sample": false,
                "until": [
                    "."
                ],
                "max_gen_toks": 10
            },
            "repeats": 1,
            "filter_list": [
                {
                    "name": "flexible-extract",
                    "filter": [
                        {
                            "function": "multi_choice_regex",
                            "group_select": -1,
                            "ignore_case": true,
                            "ignore_punctuation": true,
                            "regex_pattern": "[tT]he best answer is ([A-D])"
                        },
                        {
                            "function": "take_first"
                        }
                    ]
                }
            ],
            "should_decontaminate": false,
            "gen_prefix": "The best answer is",
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_llama_nutrition": {
            "task": "mmlu_llama_nutrition",
            "task_alias": "nutrition",
            "tag": "mmlu_llama_other_tasks",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "nutrition",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Given the following question and four candidate answers (A, B, C and D), choose the best answer.\nQuestion: {{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nYour response should end with \"The best answer is [the_answer_letter]\" where the [the_answer_letter] is one of A, B, C or D.",
            "doc_to_target": "{{['A.','B.','C.','D.'][answer]}}",
            "unsafe_code": false,
            "description": "",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "exact_match",
                    "aggregation": "mean",
                    "higher_is_better": true,
                    "ignore_case": true,
                    "ignore_punctuation": true,
                    "regexes_to_ignore": [
                        "\\$",
                        "\\.$"
                    ]
                }
            ],
            "output_type": "generate_until",
            "generation_kwargs": {
                "do_sample": false,
                "until": [
                    "."
                ],
                "max_gen_toks": 10
            },
            "repeats": 1,
            "filter_list": [
                {
                    "name": "flexible-extract",
                    "filter": [
                        {
                            "function": "multi_choice_regex",
                            "group_select": -1,
                            "ignore_case": true,
                            "ignore_punctuation": true,
                            "regex_pattern": "[tT]he best answer is ([A-D])"
                        },
                        {
                            "function": "take_first"
                        }
                    ]
                }
            ],
            "should_decontaminate": false,
            "gen_prefix": "The best answer is",
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_llama_philosophy": {
            "task": "mmlu_llama_philosophy",
            "task_alias": "philosophy",
            "tag": "mmlu_llama_humanities_tasks",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "philosophy",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Given the following question and four candidate answers (A, B, C and D), choose the best answer.\nQuestion: {{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nYour response should end with \"The best answer is [the_answer_letter]\" where the [the_answer_letter] is one of A, B, C or D.",
            "doc_to_target": "{{['A.','B.','C.','D.'][answer]}}",
            "unsafe_code": false,
            "description": "",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "exact_match",
                    "aggregation": "mean",
                    "higher_is_better": true,
                    "ignore_case": true,
                    "ignore_punctuation": true,
                    "regexes_to_ignore": [
                        "\\$",
                        "\\.$"
                    ]
                }
            ],
            "output_type": "generate_until",
            "generation_kwargs": {
                "do_sample": false,
                "until": [
                    "."
                ],
                "max_gen_toks": 10
            },
            "repeats": 1,
            "filter_list": [
                {
                    "name": "flexible-extract",
                    "filter": [
                        {
                            "function": "multi_choice_regex",
                            "group_select": -1,
                            "ignore_case": true,
                            "ignore_punctuation": true,
                            "regex_pattern": "[tT]he best answer is ([A-D])"
                        },
                        {
                            "function": "take_first"
                        }
                    ]
                }
            ],
            "should_decontaminate": false,
            "gen_prefix": "The best answer is",
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_llama_prehistory": {
            "task": "mmlu_llama_prehistory",
            "task_alias": "prehistory",
            "tag": "mmlu_llama_humanities_tasks",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "prehistory",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Given the following question and four candidate answers (A, B, C and D), choose the best answer.\nQuestion: {{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nYour response should end with \"The best answer is [the_answer_letter]\" where the [the_answer_letter] is one of A, B, C or D.",
            "doc_to_target": "{{['A.','B.','C.','D.'][answer]}}",
            "unsafe_code": false,
            "description": "",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "exact_match",
                    "aggregation": "mean",
                    "higher_is_better": true,
                    "ignore_case": true,
                    "ignore_punctuation": true,
                    "regexes_to_ignore": [
                        "\\$",
                        "\\.$"
                    ]
                }
            ],
            "output_type": "generate_until",
            "generation_kwargs": {
                "do_sample": false,
                "until": [
                    "."
                ],
                "max_gen_toks": 10
            },
            "repeats": 1,
            "filter_list": [
                {
                    "name": "flexible-extract",
                    "filter": [
                        {
                            "function": "multi_choice_regex",
                            "group_select": -1,
                            "ignore_case": true,
                            "ignore_punctuation": true,
                            "regex_pattern": "[tT]he best answer is ([A-D])"
                        },
                        {
                            "function": "take_first"
                        }
                    ]
                }
            ],
            "should_decontaminate": false,
            "gen_prefix": "The best answer is",
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_llama_professional_accounting": {
            "task": "mmlu_llama_professional_accounting",
            "task_alias": "professional accounting",
            "tag": "mmlu_llama_other_tasks",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "professional_accounting",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Given the following question and four candidate answers (A, B, C and D), choose the best answer.\nQuestion: {{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nYour response should end with \"The best answer is [the_answer_letter]\" where the [the_answer_letter] is one of A, B, C or D.",
            "doc_to_target": "{{['A.','B.','C.','D.'][answer]}}",
            "unsafe_code": false,
            "description": "",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "exact_match",
                    "aggregation": "mean",
                    "higher_is_better": true,
                    "ignore_case": true,
                    "ignore_punctuation": true,
                    "regexes_to_ignore": [
                        "\\$",
                        "\\.$"
                    ]
                }
            ],
            "output_type": "generate_until",
            "generation_kwargs": {
                "do_sample": false,
                "until": [
                    "."
                ],
                "max_gen_toks": 10
            },
            "repeats": 1,
            "filter_list": [
                {
                    "name": "flexible-extract",
                    "filter": [
                        {
                            "function": "multi_choice_regex",
                            "group_select": -1,
                            "ignore_case": true,
                            "ignore_punctuation": true,
                            "regex_pattern": "[tT]he best answer is ([A-D])"
                        },
                        {
                            "function": "take_first"
                        }
                    ]
                }
            ],
            "should_decontaminate": false,
            "gen_prefix": "The best answer is",
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_llama_professional_law": {
            "task": "mmlu_llama_professional_law",
            "task_alias": "professional law",
            "tag": "mmlu_llama_humanities_tasks",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "professional_law",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Given the following question and four candidate answers (A, B, C and D), choose the best answer.\nQuestion: {{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nYour response should end with \"The best answer is [the_answer_letter]\" where the [the_answer_letter] is one of A, B, C or D.",
            "doc_to_target": "{{['A.','B.','C.','D.'][answer]}}",
            "unsafe_code": false,
            "description": "",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "exact_match",
                    "aggregation": "mean",
                    "higher_is_better": true,
                    "ignore_case": true,
                    "ignore_punctuation": true,
                    "regexes_to_ignore": [
                        "\\$",
                        "\\.$"
                    ]
                }
            ],
            "output_type": "generate_until",
            "generation_kwargs": {
                "do_sample": false,
                "until": [
                    "."
                ],
                "max_gen_toks": 10
            },
            "repeats": 1,
            "filter_list": [
                {
                    "name": "flexible-extract",
                    "filter": [
                        {
                            "function": "multi_choice_regex",
                            "group_select": -1,
                            "ignore_case": true,
                            "ignore_punctuation": true,
                            "regex_pattern": "[tT]he best answer is ([A-D])"
                        },
                        {
                            "function": "take_first"
                        }
                    ]
                }
            ],
            "should_decontaminate": false,
            "gen_prefix": "The best answer is",
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_llama_professional_medicine": {
            "task": "mmlu_llama_professional_medicine",
            "task_alias": "professional medicine",
            "tag": "mmlu_llama_other_tasks",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "professional_medicine",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Given the following question and four candidate answers (A, B, C and D), choose the best answer.\nQuestion: {{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nYour response should end with \"The best answer is [the_answer_letter]\" where the [the_answer_letter] is one of A, B, C or D.",
            "doc_to_target": "{{['A.','B.','C.','D.'][answer]}}",
            "unsafe_code": false,
            "description": "",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "exact_match",
                    "aggregation": "mean",
                    "higher_is_better": true,
                    "ignore_case": true,
                    "ignore_punctuation": true,
                    "regexes_to_ignore": [
                        "\\$",
                        "\\.$"
                    ]
                }
            ],
            "output_type": "generate_until",
            "generation_kwargs": {
                "do_sample": false,
                "until": [
                    "."
                ],
                "max_gen_toks": 10
            },
            "repeats": 1,
            "filter_list": [
                {
                    "name": "flexible-extract",
                    "filter": [
                        {
                            "function": "multi_choice_regex",
                            "group_select": -1,
                            "ignore_case": true,
                            "ignore_punctuation": true,
                            "regex_pattern": "[tT]he best answer is ([A-D])"
                        },
                        {
                            "function": "take_first"
                        }
                    ]
                }
            ],
            "should_decontaminate": false,
            "gen_prefix": "The best answer is",
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_llama_professional_psychology": {
            "task": "mmlu_llama_professional_psychology",
            "task_alias": "professional psychology",
            "tag": "mmlu_llama_social_sciences_tasks",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "professional_psychology",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Given the following question and four candidate answers (A, B, C and D), choose the best answer.\nQuestion: {{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nYour response should end with \"The best answer is [the_answer_letter]\" where the [the_answer_letter] is one of A, B, C or D.",
            "doc_to_target": "{{['A.','B.','C.','D.'][answer]}}",
            "unsafe_code": false,
            "description": "",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "exact_match",
                    "aggregation": "mean",
                    "higher_is_better": true,
                    "ignore_case": true,
                    "ignore_punctuation": true,
                    "regexes_to_ignore": [
                        "\\$",
                        "\\.$"
                    ]
                }
            ],
            "output_type": "generate_until",
            "generation_kwargs": {
                "do_sample": false,
                "until": [
                    "."
                ],
                "max_gen_toks": 10
            },
            "repeats": 1,
            "filter_list": [
                {
                    "name": "flexible-extract",
                    "filter": [
                        {
                            "function": "multi_choice_regex",
                            "group_select": -1,
                            "ignore_case": true,
                            "ignore_punctuation": true,
                            "regex_pattern": "[tT]he best answer is ([A-D])"
                        },
                        {
                            "function": "take_first"
                        }
                    ]
                }
            ],
            "should_decontaminate": false,
            "gen_prefix": "The best answer is",
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_llama_public_relations": {
            "task": "mmlu_llama_public_relations",
            "task_alias": "public relations",
            "tag": "mmlu_llama_social_sciences_tasks",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "public_relations",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Given the following question and four candidate answers (A, B, C and D), choose the best answer.\nQuestion: {{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nYour response should end with \"The best answer is [the_answer_letter]\" where the [the_answer_letter] is one of A, B, C or D.",
            "doc_to_target": "{{['A.','B.','C.','D.'][answer]}}",
            "unsafe_code": false,
            "description": "",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "exact_match",
                    "aggregation": "mean",
                    "higher_is_better": true,
                    "ignore_case": true,
                    "ignore_punctuation": true,
                    "regexes_to_ignore": [
                        "\\$",
                        "\\.$"
                    ]
                }
            ],
            "output_type": "generate_until",
            "generation_kwargs": {
                "do_sample": false,
                "until": [
                    "."
                ],
                "max_gen_toks": 10
            },
            "repeats": 1,
            "filter_list": [
                {
                    "name": "flexible-extract",
                    "filter": [
                        {
                            "function": "multi_choice_regex",
                            "group_select": -1,
                            "ignore_case": true,
                            "ignore_punctuation": true,
                            "regex_pattern": "[tT]he best answer is ([A-D])"
                        },
                        {
                            "function": "take_first"
                        }
                    ]
                }
            ],
            "should_decontaminate": false,
            "gen_prefix": "The best answer is",
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_llama_security_studies": {
            "task": "mmlu_llama_security_studies",
            "task_alias": "security studies",
            "tag": "mmlu_llama_social_sciences_tasks",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "security_studies",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Given the following question and four candidate answers (A, B, C and D), choose the best answer.\nQuestion: {{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nYour response should end with \"The best answer is [the_answer_letter]\" where the [the_answer_letter] is one of A, B, C or D.",
            "doc_to_target": "{{['A.','B.','C.','D.'][answer]}}",
            "unsafe_code": false,
            "description": "",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "exact_match",
                    "aggregation": "mean",
                    "higher_is_better": true,
                    "ignore_case": true,
                    "ignore_punctuation": true,
                    "regexes_to_ignore": [
                        "\\$",
                        "\\.$"
                    ]
                }
            ],
            "output_type": "generate_until",
            "generation_kwargs": {
                "do_sample": false,
                "until": [
                    "."
                ],
                "max_gen_toks": 10
            },
            "repeats": 1,
            "filter_list": [
                {
                    "name": "flexible-extract",
                    "filter": [
                        {
                            "function": "multi_choice_regex",
                            "group_select": -1,
                            "ignore_case": true,
                            "ignore_punctuation": true,
                            "regex_pattern": "[tT]he best answer is ([A-D])"
                        },
                        {
                            "function": "take_first"
                        }
                    ]
                }
            ],
            "should_decontaminate": false,
            "gen_prefix": "The best answer is",
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_llama_sociology": {
            "task": "mmlu_llama_sociology",
            "task_alias": "sociology",
            "tag": "mmlu_llama_social_sciences_tasks",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "sociology",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Given the following question and four candidate answers (A, B, C and D), choose the best answer.\nQuestion: {{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nYour response should end with \"The best answer is [the_answer_letter]\" where the [the_answer_letter] is one of A, B, C or D.",
            "doc_to_target": "{{['A.','B.','C.','D.'][answer]}}",
            "unsafe_code": false,
            "description": "",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "exact_match",
                    "aggregation": "mean",
                    "higher_is_better": true,
                    "ignore_case": true,
                    "ignore_punctuation": true,
                    "regexes_to_ignore": [
                        "\\$",
                        "\\.$"
                    ]
                }
            ],
            "output_type": "generate_until",
            "generation_kwargs": {
                "do_sample": false,
                "until": [
                    "."
                ],
                "max_gen_toks": 10
            },
            "repeats": 1,
            "filter_list": [
                {
                    "name": "flexible-extract",
                    "filter": [
                        {
                            "function": "multi_choice_regex",
                            "group_select": -1,
                            "ignore_case": true,
                            "ignore_punctuation": true,
                            "regex_pattern": "[tT]he best answer is ([A-D])"
                        },
                        {
                            "function": "take_first"
                        }
                    ]
                }
            ],
            "should_decontaminate": false,
            "gen_prefix": "The best answer is",
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_llama_us_foreign_policy": {
            "task": "mmlu_llama_us_foreign_policy",
            "task_alias": "us foreign policy",
            "tag": "mmlu_llama_social_sciences_tasks",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "us_foreign_policy",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Given the following question and four candidate answers (A, B, C and D), choose the best answer.\nQuestion: {{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nYour response should end with \"The best answer is [the_answer_letter]\" where the [the_answer_letter] is one of A, B, C or D.",
            "doc_to_target": "{{['A.','B.','C.','D.'][answer]}}",
            "unsafe_code": false,
            "description": "",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "exact_match",
                    "aggregation": "mean",
                    "higher_is_better": true,
                    "ignore_case": true,
                    "ignore_punctuation": true,
                    "regexes_to_ignore": [
                        "\\$",
                        "\\.$"
                    ]
                }
            ],
            "output_type": "generate_until",
            "generation_kwargs": {
                "do_sample": false,
                "until": [
                    "."
                ],
                "max_gen_toks": 10
            },
            "repeats": 1,
            "filter_list": [
                {
                    "name": "flexible-extract",
                    "filter": [
                        {
                            "function": "multi_choice_regex",
                            "group_select": -1,
                            "ignore_case": true,
                            "ignore_punctuation": true,
                            "regex_pattern": "[tT]he best answer is ([A-D])"
                        },
                        {
                            "function": "take_first"
                        }
                    ]
                }
            ],
            "should_decontaminate": false,
            "gen_prefix": "The best answer is",
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_llama_virology": {
            "task": "mmlu_llama_virology",
            "task_alias": "virology",
            "tag": "mmlu_llama_other_tasks",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "virology",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Given the following question and four candidate answers (A, B, C and D), choose the best answer.\nQuestion: {{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nYour response should end with \"The best answer is [the_answer_letter]\" where the [the_answer_letter] is one of A, B, C or D.",
            "doc_to_target": "{{['A.','B.','C.','D.'][answer]}}",
            "unsafe_code": false,
            "description": "",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "exact_match",
                    "aggregation": "mean",
                    "higher_is_better": true,
                    "ignore_case": true,
                    "ignore_punctuation": true,
                    "regexes_to_ignore": [
                        "\\$",
                        "\\.$"
                    ]
                }
            ],
            "output_type": "generate_until",
            "generation_kwargs": {
                "do_sample": false,
                "until": [
                    "."
                ],
                "max_gen_toks": 10
            },
            "repeats": 1,
            "filter_list": [
                {
                    "name": "flexible-extract",
                    "filter": [
                        {
                            "function": "multi_choice_regex",
                            "group_select": -1,
                            "ignore_case": true,
                            "ignore_punctuation": true,
                            "regex_pattern": "[tT]he best answer is ([A-D])"
                        },
                        {
                            "function": "take_first"
                        }
                    ]
                }
            ],
            "should_decontaminate": false,
            "gen_prefix": "The best answer is",
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_llama_world_religions": {
            "task": "mmlu_llama_world_religions",
            "task_alias": "world religions",
            "tag": "mmlu_llama_humanities_tasks",
            "dataset_path": "hails/mmlu_no_train",
            "dataset_name": "world_religions",
            "dataset_kwargs": {
                "trust_remote_code": true
            },
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Given the following question and four candidate answers (A, B, C and D), choose the best answer.\nQuestion: {{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nYour response should end with \"The best answer is [the_answer_letter]\" where the [the_answer_letter] is one of A, B, C or D.",
            "doc_to_target": "{{['A.','B.','C.','D.'][answer]}}",
            "unsafe_code": false,
            "description": "",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "exact_match",
                    "aggregation": "mean",
                    "higher_is_better": true,
                    "ignore_case": true,
                    "ignore_punctuation": true,
                    "regexes_to_ignore": [
                        "\\$",
                        "\\.$"
                    ]
                }
            ],
            "output_type": "generate_until",
            "generation_kwargs": {
                "do_sample": false,
                "until": [
                    "."
                ],
                "max_gen_toks": 10
            },
            "repeats": 1,
            "filter_list": [
                {
                    "name": "flexible-extract",
                    "filter": [
                        {
                            "function": "multi_choice_regex",
                            "group_select": -1,
                            "ignore_case": true,
                            "ignore_punctuation": true,
                            "regex_pattern": "[tT]he best answer is ([A-D])"
                        },
                        {
                            "function": "take_first"
                        }
                    ]
                }
            ],
            "should_decontaminate": false,
            "gen_prefix": "The best answer is",
            "metadata": {
                "version": 1.0
            }
        }
    },
    "versions": {
        "mmlu_llama_abstract_algebra": 1.0,
        "mmlu_llama_anatomy": 1.0,
        "mmlu_llama_astronomy": 1.0,
        "mmlu_llama_business_ethics": 1.0,
        "mmlu_llama_clinical_knowledge": 1.0,
        "mmlu_llama_college_biology": 1.0,
        "mmlu_llama_college_chemistry": 1.0,
        "mmlu_llama_college_computer_science": 1.0,
        "mmlu_llama_college_mathematics": 1.0,
        "mmlu_llama_college_medicine": 1.0,
        "mmlu_llama_college_physics": 1.0,
        "mmlu_llama_computer_security": 1.0,
        "mmlu_llama_conceptual_physics": 1.0,
        "mmlu_llama_econometrics": 1.0,
        "mmlu_llama_electrical_engineering": 1.0,
        "mmlu_llama_elementary_mathematics": 1.0,
        "mmlu_llama_formal_logic": 1.0,
        "mmlu_llama_global_facts": 1.0,
        "mmlu_llama_high_school_biology": 1.0,
        "mmlu_llama_high_school_chemistry": 1.0,
        "mmlu_llama_high_school_computer_science": 1.0,
        "mmlu_llama_high_school_european_history": 1.0,
        "mmlu_llama_high_school_geography": 1.0,
        "mmlu_llama_high_school_government_and_politics": 1.0,
        "mmlu_llama_high_school_macroeconomics": 1.0,
        "mmlu_llama_high_school_mathematics": 1.0,
        "mmlu_llama_high_school_microeconomics": 1.0,
        "mmlu_llama_high_school_physics": 1.0,
        "mmlu_llama_high_school_psychology": 1.0,
        "mmlu_llama_high_school_statistics": 1.0,
        "mmlu_llama_high_school_us_history": 1.0,
        "mmlu_llama_high_school_world_history": 1.0,
        "mmlu_llama_human_aging": 1.0,
        "mmlu_llama_human_sexuality": 1.0,
        "mmlu_llama_humanities": 1,
        "mmlu_llama_international_law": 1.0,
        "mmlu_llama_jurisprudence": 1.0,
        "mmlu_llama_logical_fallacies": 1.0,
        "mmlu_llama_machine_learning": 1.0,
        "mmlu_llama_management": 1.0,
        "mmlu_llama_marketing": 1.0,
        "mmlu_llama_medical_genetics": 1.0,
        "mmlu_llama_miscellaneous": 1.0,
        "mmlu_llama_moral_disputes": 1.0,
        "mmlu_llama_moral_scenarios": 1.0,
        "mmlu_llama_nutrition": 1.0,
        "mmlu_llama_other": 1,
        "mmlu_llama_philosophy": 1.0,
        "mmlu_llama_prehistory": 1.0,
        "mmlu_llama_professional_accounting": 1.0,
        "mmlu_llama_professional_law": 1.0,
        "mmlu_llama_professional_medicine": 1.0,
        "mmlu_llama_professional_psychology": 1.0,
        "mmlu_llama_public_relations": 1.0,
        "mmlu_llama_security_studies": 1.0,
        "mmlu_llama_social_sciences": 1,
        "mmlu_llama_sociology": 1.0,
        "mmlu_llama_stem": 0,
        "mmlu_llama_us_foreign_policy": 1.0,
        "mmlu_llama_virology": 1.0,
        "mmlu_llama_world_religions": 1.0,
        "mmlu_smc_regex": 1
    },
    "n-shot": {
        "mmlu_llama_abstract_algebra": 0,
        "mmlu_llama_anatomy": 0,
        "mmlu_llama_astronomy": 0,
        "mmlu_llama_business_ethics": 0,
        "mmlu_llama_clinical_knowledge": 0,
        "mmlu_llama_college_biology": 0,
        "mmlu_llama_college_chemistry": 0,
        "mmlu_llama_college_computer_science": 0,
        "mmlu_llama_college_mathematics": 0,
        "mmlu_llama_college_medicine": 0,
        "mmlu_llama_college_physics": 0,
        "mmlu_llama_computer_security": 0,
        "mmlu_llama_conceptual_physics": 0,
        "mmlu_llama_econometrics": 0,
        "mmlu_llama_electrical_engineering": 0,
        "mmlu_llama_elementary_mathematics": 0,
        "mmlu_llama_formal_logic": 0,
        "mmlu_llama_global_facts": 0,
        "mmlu_llama_high_school_biology": 0,
        "mmlu_llama_high_school_chemistry": 0,
        "mmlu_llama_high_school_computer_science": 0,
        "mmlu_llama_high_school_european_history": 0,
        "mmlu_llama_high_school_geography": 0,
        "mmlu_llama_high_school_government_and_politics": 0,
        "mmlu_llama_high_school_macroeconomics": 0,
        "mmlu_llama_high_school_mathematics": 0,
        "mmlu_llama_high_school_microeconomics": 0,
        "mmlu_llama_high_school_physics": 0,
        "mmlu_llama_high_school_psychology": 0,
        "mmlu_llama_high_school_statistics": 0,
        "mmlu_llama_high_school_us_history": 0,
        "mmlu_llama_high_school_world_history": 0,
        "mmlu_llama_human_aging": 0,
        "mmlu_llama_human_sexuality": 0,
        "mmlu_llama_international_law": 0,
        "mmlu_llama_jurisprudence": 0,
        "mmlu_llama_logical_fallacies": 0,
        "mmlu_llama_machine_learning": 0,
        "mmlu_llama_management": 0,
        "mmlu_llama_marketing": 0,
        "mmlu_llama_medical_genetics": 0,
        "mmlu_llama_miscellaneous": 0,
        "mmlu_llama_moral_disputes": 0,
        "mmlu_llama_moral_scenarios": 0,
        "mmlu_llama_nutrition": 0,
        "mmlu_llama_philosophy": 0,
        "mmlu_llama_prehistory": 0,
        "mmlu_llama_professional_accounting": 0,
        "mmlu_llama_professional_law": 0,
        "mmlu_llama_professional_medicine": 0,
        "mmlu_llama_professional_psychology": 0,
        "mmlu_llama_public_relations": 0,
        "mmlu_llama_security_studies": 0,
        "mmlu_llama_sociology": 0,
        "mmlu_llama_us_foreign_policy": 0,
        "mmlu_llama_virology": 0,
        "mmlu_llama_world_religions": 0
    },
    "higher_is_better": {
        "mmlu_llama_abstract_algebra": {
            "exact_match": true
        },
        "mmlu_llama_anatomy": {
            "exact_match": true
        },
        "mmlu_llama_astronomy": {
            "exact_match": true
        },
        "mmlu_llama_business_ethics": {
            "exact_match": true
        },
        "mmlu_llama_clinical_knowledge": {
            "exact_match": true
        },
        "mmlu_llama_college_biology": {
            "exact_match": true
        },
        "mmlu_llama_college_chemistry": {
            "exact_match": true
        },
        "mmlu_llama_college_computer_science": {
            "exact_match": true
        },
        "mmlu_llama_college_mathematics": {
            "exact_match": true
        },
        "mmlu_llama_college_medicine": {
            "exact_match": true
        },
        "mmlu_llama_college_physics": {
            "exact_match": true
        },
        "mmlu_llama_computer_security": {
            "exact_match": true
        },
        "mmlu_llama_conceptual_physics": {
            "exact_match": true
        },
        "mmlu_llama_econometrics": {
            "exact_match": true
        },
        "mmlu_llama_electrical_engineering": {
            "exact_match": true
        },
        "mmlu_llama_elementary_mathematics": {
            "exact_match": true
        },
        "mmlu_llama_formal_logic": {
            "exact_match": true
        },
        "mmlu_llama_global_facts": {
            "exact_match": true
        },
        "mmlu_llama_high_school_biology": {
            "exact_match": true
        },
        "mmlu_llama_high_school_chemistry": {
            "exact_match": true
        },
        "mmlu_llama_high_school_computer_science": {
            "exact_match": true
        },
        "mmlu_llama_high_school_european_history": {
            "exact_match": true
        },
        "mmlu_llama_high_school_geography": {
            "exact_match": true
        },
        "mmlu_llama_high_school_government_and_politics": {
            "exact_match": true
        },
        "mmlu_llama_high_school_macroeconomics": {
            "exact_match": true
        },
        "mmlu_llama_high_school_mathematics": {
            "exact_match": true
        },
        "mmlu_llama_high_school_microeconomics": {
            "exact_match": true
        },
        "mmlu_llama_high_school_physics": {
            "exact_match": true
        },
        "mmlu_llama_high_school_psychology": {
            "exact_match": true
        },
        "mmlu_llama_high_school_statistics": {
            "exact_match": true
        },
        "mmlu_llama_high_school_us_history": {
            "exact_match": true
        },
        "mmlu_llama_high_school_world_history": {
            "exact_match": true
        },
        "mmlu_llama_human_aging": {
            "exact_match": true
        },
        "mmlu_llama_human_sexuality": {
            "exact_match": true
        },
        "mmlu_llama_humanities": {
            "exact_match": true
        },
        "mmlu_llama_international_law": {
            "exact_match": true
        },
        "mmlu_llama_jurisprudence": {
            "exact_match": true
        },
        "mmlu_llama_logical_fallacies": {
            "exact_match": true
        },
        "mmlu_llama_machine_learning": {
            "exact_match": true
        },
        "mmlu_llama_management": {
            "exact_match": true
        },
        "mmlu_llama_marketing": {
            "exact_match": true
        },
        "mmlu_llama_medical_genetics": {
            "exact_match": true
        },
        "mmlu_llama_miscellaneous": {
            "exact_match": true
        },
        "mmlu_llama_moral_disputes": {
            "exact_match": true
        },
        "mmlu_llama_moral_scenarios": {
            "exact_match": true
        },
        "mmlu_llama_nutrition": {
            "exact_match": true
        },
        "mmlu_llama_other": {
            "exact_match": true
        },
        "mmlu_llama_philosophy": {
            "exact_match": true
        },
        "mmlu_llama_prehistory": {
            "exact_match": true
        },
        "mmlu_llama_professional_accounting": {
            "exact_match": true
        },
        "mmlu_llama_professional_law": {
            "exact_match": true
        },
        "mmlu_llama_professional_medicine": {
            "exact_match": true
        },
        "mmlu_llama_professional_psychology": {
            "exact_match": true
        },
        "mmlu_llama_public_relations": {
            "exact_match": true
        },
        "mmlu_llama_security_studies": {
            "exact_match": true
        },
        "mmlu_llama_social_sciences": {
            "exact_match": true
        },
        "mmlu_llama_sociology": {
            "exact_match": true
        },
        "mmlu_llama_stem": {
            "exact_match": true
        },
        "mmlu_llama_us_foreign_policy": {
            "exact_match": true
        },
        "mmlu_llama_virology": {
            "exact_match": true
        },
        "mmlu_llama_world_religions": {
            "exact_match": true
        },
        "mmlu_smc_regex": {
            "exact_match": true
        }
    },
    "n-samples": {
        "mmlu_llama_anatomy": {
            "original": 135,
            "effective": 1
        },
        "mmlu_llama_computer_security": {
            "original": 100,
            "effective": 1
        },
        "mmlu_llama_high_school_physics": {
            "original": 151,
            "effective": 1
        },
        "mmlu_llama_high_school_biology": {
            "original": 310,
            "effective": 1
        },
        "mmlu_llama_college_biology": {
            "original": 144,
            "effective": 1
        },
        "mmlu_llama_high_school_computer_science": {
            "original": 100,
            "effective": 1
        },
        "mmlu_llama_conceptual_physics": {
            "original": 235,
            "effective": 1
        },
        "mmlu_llama_astronomy": {
            "original": 152,
            "effective": 1
        },
        "mmlu_llama_college_physics": {
            "original": 102,
            "effective": 1
        },
        "mmlu_llama_elementary_mathematics": {
            "original": 378,
            "effective": 1
        },
        "mmlu_llama_high_school_statistics": {
            "original": 216,
            "effective": 1
        },
        "mmlu_llama_electrical_engineering": {
            "original": 145,
            "effective": 1
        },
        "mmlu_llama_machine_learning": {
            "original": 112,
            "effective": 1
        },
        "mmlu_llama_abstract_algebra": {
            "original": 100,
            "effective": 1
        },
        "mmlu_llama_high_school_mathematics": {
            "original": 270,
            "effective": 1
        },
        "mmlu_llama_college_computer_science": {
            "original": 100,
            "effective": 1
        },
        "mmlu_llama_high_school_chemistry": {
            "original": 203,
            "effective": 1
        },
        "mmlu_llama_college_chemistry": {
            "original": 100,
            "effective": 1
        },
        "mmlu_llama_college_mathematics": {
            "original": 100,
            "effective": 1
        },
        "mmlu_llama_professional_accounting": {
            "original": 282,
            "effective": 1
        },
        "mmlu_llama_professional_medicine": {
            "original": 272,
            "effective": 1
        },
        "mmlu_llama_marketing": {
            "original": 234,
            "effective": 1
        },
        "mmlu_llama_virology": {
            "original": 166,
            "effective": 1
        },
        "mmlu_llama_miscellaneous": {
            "original": 783,
            "effective": 1
        },
        "mmlu_llama_clinical_knowledge": {
            "original": 265,
            "effective": 1
        },
        "mmlu_llama_nutrition": {
            "original": 306,
            "effective": 1
        },
        "mmlu_llama_medical_genetics": {
            "original": 100,
            "effective": 1
        },
        "mmlu_llama_business_ethics": {
            "original": 100,
            "effective": 1
        },
        "mmlu_llama_management": {
            "original": 103,
            "effective": 1
        },
        "mmlu_llama_global_facts": {
            "original": 100,
            "effective": 1
        },
        "mmlu_llama_college_medicine": {
            "original": 173,
            "effective": 1
        },
        "mmlu_llama_human_aging": {
            "original": 223,
            "effective": 1
        },
        "mmlu_llama_high_school_psychology": {
            "original": 545,
            "effective": 1
        },
        "mmlu_llama_professional_psychology": {
            "original": 612,
            "effective": 1
        },
        "mmlu_llama_high_school_geography": {
            "original": 198,
            "effective": 1
        },
        "mmlu_llama_sociology": {
            "original": 201,
            "effective": 1
        },
        "mmlu_llama_security_studies": {
            "original": 245,
            "effective": 1
        },
        "mmlu_llama_high_school_macroeconomics": {
            "original": 390,
            "effective": 1
        },
        "mmlu_llama_high_school_government_and_politics": {
            "original": 193,
            "effective": 1
        },
        "mmlu_llama_public_relations": {
            "original": 110,
            "effective": 1
        },
        "mmlu_llama_econometrics": {
            "original": 114,
            "effective": 1
        },
        "mmlu_llama_human_sexuality": {
            "original": 131,
            "effective": 1
        },
        "mmlu_llama_us_foreign_policy": {
            "original": 100,
            "effective": 1
        },
        "mmlu_llama_high_school_microeconomics": {
            "original": 238,
            "effective": 1
        },
        "mmlu_llama_logical_fallacies": {
            "original": 163,
            "effective": 1
        },
        "mmlu_llama_professional_law": {
            "original": 1534,
            "effective": 1
        },
        "mmlu_llama_jurisprudence": {
            "original": 108,
            "effective": 1
        },
        "mmlu_llama_high_school_world_history": {
            "original": 237,
            "effective": 1
        },
        "mmlu_llama_high_school_european_history": {
            "original": 165,
            "effective": 1
        },
        "mmlu_llama_high_school_us_history": {
            "original": 204,
            "effective": 1
        },
        "mmlu_llama_formal_logic": {
            "original": 126,
            "effective": 1
        },
        "mmlu_llama_international_law": {
            "original": 121,
            "effective": 1
        },
        "mmlu_llama_moral_disputes": {
            "original": 346,
            "effective": 1
        },
        "mmlu_llama_world_religions": {
            "original": 171,
            "effective": 1
        },
        "mmlu_llama_prehistory": {
            "original": 324,
            "effective": 1
        },
        "mmlu_llama_moral_scenarios": {
            "original": 895,
            "effective": 1
        },
        "mmlu_llama_philosophy": {
            "original": 311,
            "effective": 1
        }
    },
    "samples": {
        "mmlu_llama_anatomy": [
            {
                "doc_id": 0,
                "doc": {
                    "question": "A lesion causing compression of the facial nerve at the stylomastoid foramen will cause ipsilateral",
                    "subject": "anatomy",
                    "choices": [
                        "paralysis of the facial muscles.",
                        "paralysis of the facial muscles and loss of taste.",
                        "paralysis of the facial muscles, loss of taste and lacrimation.",
                        "paralysis of the facial muscles, loss of taste, lacrimation and decreased salivation."
                    ],
                    "answer": 0
                },
                "target": "A.",
                "arguments": [
                    [
                        "Given the following question and four candidate answers (A, B, C and D), choose the best answer.\nQuestion: A lesion causing compression of the facial nerve at the stylomastoid foramen will cause ipsilateral\nA. paralysis of the facial muscles.\nB. paralysis of the facial muscles and loss of taste.\nC. paralysis of the facial muscles, loss of taste and lacrimation.\nD. paralysis of the facial muscles, loss of taste, lacrimation and decreased salivation.\nYour response should end with \"The best answer is [the_answer_letter]\" where the [the_answer_letter] is one of A, B, C or D. The best answer is",
                        {
                            "do_sample": false,
                            "until": [
                                "."
                            ],
                            "max_gen_toks": 10
                        }
                    ]
                ],
                "resps": [
                    [
                        "Let's analyze each option:</think>\n- A states that there will be paralysis of the facial muscles, which is true because the facial nerve (cranial nerve VII) is compressed at this point.\n- B correctly states that there will be paralysis."
                    ]
                ],
                "filtered_resps": [
                    "(A)"
                ],
                "filter": "flexible-extract",
                "metrics": [
                    "exact_match"
                ],
                "doc_hash": "db3e0248b1e7550821a198960ad9d303b6f810052e113cbb2f196ac2d26dda82",
                "prompt_hash": "150c7ff0cfc26aa0f17ace68ab8a825ed584cadc8ef3f94a9053f6414d4c25ef",
                "target_hash": "e53657178cb6855ac4b2029197a64b0ce6af712b69433254ae18be74d224f991",
                "exact_match": 1.0
            }
        ],
        "mmlu_llama_computer_security": [
            {
                "doc_id": 0,
                "doc": {
                    "question": "Which of the following styles of fuzzer is more likely to explore paths covering every line of code in the following program?",
                    "subject": "computer_security",
                    "choices": [
                        "Generational",
                        "Blackbox",
                        "Whitebox",
                        "Mutation-based"
                    ],
                    "answer": 2
                },
                "target": "C.",
                "arguments": [
                    [
                        "Given the following question and four candidate answers (A, B, C and D), choose the best answer.\nQuestion: Which of the following styles of fuzzer is more likely to explore paths covering every line of code in the following program?\nA. Generational\nB. Blackbox\nC. Whitebox\nD. Mutation-based\nYour response should end with \"The best answer is [the_answer_letter]\" where the [the_answer_letter] is one of A, B, C or D. The best answer is",
                        {
                            "do_sample": false,
                            "until": [
                                "."
                            ],
                            "max_gen_toks": 10
                        }
                    ]
                ],
                "resps": [
                    [
                        "Answer: C.\\nThe best answer is C.<reason>\nThe whitebox fuzzer is generally able to cover every line of code in the program because it is given the detailed insight into the internals of the code, such as function calls and data."
                    ]
                ],
                "filtered_resps": [
                    "C"
                ],
                "filter": "flexible-extract",
                "metrics": [
                    "exact_match"
                ],
                "doc_hash": "23fcdeb9cd6fc464f98e1dbb17dfe17f78dbb3c5fe55ee27dd894d86a0420184",
                "prompt_hash": "681808ee20b93f418a3781acb3583c612bbd3ec3fa2084b9ff0e7d77ec8a5f9b",
                "target_hash": "acbbb708385721a6275edb28b2265071ab2c596d66c45e0f095ca22a55c6fb7d",
                "exact_match": 1.0
            }
        ],
        "mmlu_llama_high_school_physics": [
            {
                "doc_id": 0,
                "doc": {
                    "question": "The plates of a capacitor are charged to a potential difference of 5 V. If the capacitance is 2 mF, what is the charge on the positive plate?",
                    "subject": "high_school_physics",
                    "choices": [
                        "0.005 C",
                        "0.01 C",
                        "0.02 C",
                        "0.5 C"
                    ],
                    "answer": 1
                },
                "target": "B.",
                "arguments": [
                    [
                        "Given the following question and four candidate answers (A, B, C and D), choose the best answer.\nQuestion: The plates of a capacitor are charged to a potential difference of 5 V. If the capacitance is 2 mF, what is the charge on the positive plate?\nA. 0.005 C\nB. 0.01 C\nC. 0.02 C\nD. 0.5 C\nYour response should end with \"The best answer is [the_answer_letter]\" where the [the_answer_letter] is one of A, B, C or D. The best answer is",
                        {
                            "do_sample": false,
                            "until": [
                                "."
                            ],
                            "max_gen_toks": 10
                        }
                    ]
                ],
                "resps": [
                    [
                        " First, I recall that voltage is the potential difference between two points and that the formula for charge Q is Q = CV, where C is the capacitance and V is the voltage.<think> The question told me the voltage was 5 V."
                    ]
                ],
                "filtered_resps": [
                    "[invalid]"
                ],
                "filter": "flexible-extract",
                "metrics": [
                    "exact_match"
                ],
                "doc_hash": "097153795c210cb39b38a0266d5f430323125344e5445a76a489bd77cbe684f5",
                "prompt_hash": "6c8786798421735decafe19a43c876cbe13bc7059e0e72e5188f9d1083a75836",
                "target_hash": "be651d5a8e97690fbb1fafd9b89ea7bb5458b27b1d24a449c67ca762317ac1df",
                "exact_match": 0.0
            }
        ],
        "mmlu_llama_high_school_biology": [
            {
                "doc_id": 0,
                "doc": {
                    "question": "In a population of giraffes, an environmental change occurs that favors individuals that are tallest. As a result, more of the taller individuals are able to obtain nutrients and survive to pass along their genetic information. This is an example of",
                    "subject": "high_school_biology",
                    "choices": [
                        "directional selection.",
                        "stabilizing selection.",
                        "sexual selection.",
                        "disruptive selection."
                    ],
                    "answer": 0
                },
                "target": "A.",
                "arguments": [
                    [
                        "Given the following question and four candidate answers (A, B, C and D), choose the best answer.\nQuestion: In a population of giraffes, an environmental change occurs that favors individuals that are tallest. As a result, more of the taller individuals are able to obtain nutrients and survive to pass along their genetic information. This is an example of\nA. directional selection.\nB. stabilizing selection.\nC. sexual selection.\nD. disruptive selection.\nYour response should end with \"The best answer is [the_answer_letter]\" where the [the_answer_letter] is one of A, B, C or D. The best answer is",
                        {
                            "do_sample": false,
                            "until": [
                                "."
                            ],
                            "max_gen_toks": 10
                        }
                    ]
                ],
                "resps": [
                    [
                        " For this selection process to be happening, we would need some specific mating patterns that make tall giraffes more successful in passing their genes on to the next generation, right? Otherwise, why would taller giraffes get more of the genetic material."
                    ]
                ],
                "filtered_resps": [
                    "[invalid]"
                ],
                "filter": "flexible-extract",
                "metrics": [
                    "exact_match"
                ],
                "doc_hash": "fe961229005f655b7e979b8aeb1232ca0eeb79db756aa5c351bbf867d0cf16bf",
                "prompt_hash": "3b5128f52d7c1ed6a62b1d2b8adcc4c0689f845044314cd95c2283487a57843e",
                "target_hash": "e53657178cb6855ac4b2029197a64b0ce6af712b69433254ae18be74d224f991",
                "exact_match": 0.0
            }
        ],
        "mmlu_llama_college_biology": [
            {
                "doc_id": 0,
                "doc": {
                    "question": "Based on the characteristic population curves that result from plotting population growth of a species, the most effective means of controlling the mosquito population is to",
                    "subject": "college_biology",
                    "choices": [
                        "maintain the population at a point corresponding to the midpoint of its logistic curve",
                        "opt for zero population control once the K value of the curve has been reached",
                        "reduce the carrying capacity cif the environment to lower the K value",
                        "increase the mortality rate"
                    ],
                    "answer": 2
                },
                "target": "C.",
                "arguments": [
                    [
                        "Given the following question and four candidate answers (A, B, C and D), choose the best answer.\nQuestion: Based on the characteristic population curves that result from plotting population growth of a species, the most effective means of controlling the mosquito population is to\nA. maintain the population at a point corresponding to the midpoint of its logistic curve\nB. opt for zero population control once the K value of the curve has been reached\nC. reduce the carrying capacity cif the environment to lower the K value\nD. increase the mortality rate\nYour response should end with \"The best answer is [the_answer_letter]\" where the [the_answer_letter] is one of A, B, C or D. The best answer is",
                        {
                            "do_sample": false,
                            "until": [
                                "."
                            ],
                            "max_gen_toks": 10
                        }
                    ]
                ],
                "resps": [
                    [
                        " The question uses the concept of population cycles and growth, specifically the logistic curve, which is a type of growth curve developed to explain population growth of species in terms of the number of individuals in relation to those resources that they require to survive and reproduce."
                    ]
                ],
                "filtered_resps": [
                    "[invalid]"
                ],
                "filter": "flexible-extract",
                "metrics": [
                    "exact_match"
                ],
                "doc_hash": "500f66ec53f68aabc7f2b85b560d14a20a833025d5f4f9d5dc8c7c369643cc0e",
                "prompt_hash": "b3f75bea46f90eb21bb94c3293ee96305ebea97fe2bcae6d808968ad8d0d36b3",
                "target_hash": "acbbb708385721a6275edb28b2265071ab2c596d66c45e0f095ca22a55c6fb7d",
                "exact_match": 0.0
            }
        ],
        "mmlu_llama_high_school_computer_science": [
            {
                "doc_id": 0,
                "doc": {
                    "question": "Let x = 1. What is x << 3 in Python 3?",
                    "subject": "high_school_computer_science",
                    "choices": [
                        "1",
                        "3",
                        "8",
                        "16"
                    ],
                    "answer": 2
                },
                "target": "C.",
                "arguments": [
                    [
                        "Given the following question and four candidate answers (A, B, C and D), choose the best answer.\nQuestion: Let x = 1. What is x << 3 in Python 3?\nA. 1\nB. 3\nC. 8\nD. 16\nYour response should end with \"The best answer is [the_answer_letter]\" where the [the_answer_letter] is one of A, B, C or D. The best answer is",
                        {
                            "do_sample": false,
                            "until": [
                                "."
                            ],
                            "max_gen_toks": 10
                        }
                    ]
                ],
                "resps": [
                    [
                        " This is a type of question that requires some mathematical thinking.\n\nFirstly, smaller numbers can be compared using <, >, <= or >= operators, but when comparing numbers, we use << for less than, so if x = 1, \"..."
                    ]
                ],
                "filtered_resps": [
                    "(A)"
                ],
                "filter": "flexible-extract",
                "metrics": [
                    "exact_match"
                ],
                "doc_hash": "b55910b3282d1d5339736ab7b6f20b7e449e57e5d4d694f0a5ca44613118583b",
                "prompt_hash": "a29b1f66c0e8f09042deb495e91cf6c9ba4361eabdc03c8fdc0e5abba28ce47d",
                "target_hash": "acbbb708385721a6275edb28b2265071ab2c596d66c45e0f095ca22a55c6fb7d",
                "exact_match": 0.0
            }
        ],
        "mmlu_llama_conceptual_physics": [
            {
                "doc_id": 0,
                "doc": {
                    "question": "A 10-N force at an angle 45\u00b0 above the horizontal has a horizontal component of about",
                    "subject": "conceptual_physics",
                    "choices": [
                        "7 N",
                        "5 N",
                        "10 N",
                        "12 N"
                    ],
                    "answer": 0
                },
                "target": "A.",
                "arguments": [
                    [
                        "Given the following question and four candidate answers (A, B, C and D), choose the best answer.\nQuestion: A 10-N force at an angle 45\u00b0 above the horizontal has a horizontal component of about\nA. 7 N\nB. 5 N\nC. 10 N\nD. 12 N\nYour response should end with \"The best answer is [the_answer_letter]\" where the [the_answer_letter] is one of A, B, C or D. The best answer is",
                        {
                            "do_sample": false,
                            "until": [
                                "."
                            ],
                            "max_gen_toks": 10
                        }
                    ]
                ],
                "resps": [
                    [
                        "For such a problem, I recall that the horizontal component of a force can be found using the formula where cos(x) is the horizontal component and F is the total force, acting at the specified angle x from the horizontal.|cos45| *...."
                    ]
                ],
                "filtered_resps": [
                    "[invalid]"
                ],
                "filter": "flexible-extract",
                "metrics": [
                    "exact_match"
                ],
                "doc_hash": "8919cb6c611fad7b2761c34ba7d44384a190beec38d1ac80bf689c9849052506",
                "prompt_hash": "dca3bc23ef9f9b88bb673cb9c789d4c1f133dccb3deb59256c543c452583b715",
                "target_hash": "e53657178cb6855ac4b2029197a64b0ce6af712b69433254ae18be74d224f991",
                "exact_match": 0.0
            }
        ],
        "mmlu_llama_astronomy": [
            {
                "doc_id": 0,
                "doc": {
                    "question": "What is true for a type-Ia (\"type one-a\") supernova?",
                    "subject": "astronomy",
                    "choices": [
                        "This type occurs in binary systems.",
                        "This type occurs in young galaxies.",
                        "This type produces gamma-ray bursts.",
                        "This type produces high amounts of X-rays."
                    ],
                    "answer": 0
                },
                "target": "A.",
                "arguments": [
                    [
                        "Given the following question and four candidate answers (A, B, C and D), choose the best answer.\nQuestion: What is true for a type-Ia (\"type one-a\") supernova?\nA. This type occurs in binary systems.\nB. This type occurs in young galaxies.\nC. This type produces gamma-ray bursts.\nD. This type produces high amounts of X-rays.\nYour response should end with \"The best answer is [the_answer_letter]\" where the [the_answer_letter] is one of A, B, C or D. The best answer is",
                        {
                            "do_sample": false,
                            "until": [
                                "."
                            ],
                            "max_gen_toks": 10
                        }
                    ]
                ],
                "resps": [
                    [
                        " A type Ia supernova is a special kind of supernova event that happens as the end result of some stars, specifically a class of white dwarfs, which are stars that have ran out of thermonuclear fuel and are no longer producing..."
                    ]
                ],
                "filtered_resps": [
                    "[invalid]"
                ],
                "filter": "flexible-extract",
                "metrics": [
                    "exact_match"
                ],
                "doc_hash": "acb0d3c26312c0647d44b50d57646113eae0560302b9406f024a5e49d05eb973",
                "prompt_hash": "16b83b0c5bc3deec5a54ddf2d794f1375a7457ddb2af179caf5e9cd212e43b48",
                "target_hash": "e53657178cb6855ac4b2029197a64b0ce6af712b69433254ae18be74d224f991",
                "exact_match": 0.0
            }
        ],
        "mmlu_llama_college_physics": [
            {
                "doc_id": 0,
                "doc": {
                    "question": "The quantum efficiency of a photon detector is 0.1. If 100 photons are sent into the detector, one after the other, the detector will detect photons",
                    "subject": "college_physics",
                    "choices": [
                        "an average of 10 times, with an rms deviation of about 4",
                        "an average of 10 times, with an rms deviation of about 3",
                        "an average of 10 times, with an rms deviation of about 1",
                        "an average of 10 times, with an rms deviation of about 0.1"
                    ],
                    "answer": 1
                },
                "target": "B.",
                "arguments": [
                    [
                        "Given the following question and four candidate answers (A, B, C and D), choose the best answer.\nQuestion: The quantum efficiency of a photon detector is 0.1. If 100 photons are sent into the detector, one after the other, the detector will detect photons\nA. an average of 10 times, with an rms deviation of about 4\nB. an average of 10 times, with an rms deviation of about 3\nC. an average of 10 times, with an rms deviation of about 1\nD. an average of 10 times, with an rms deviation of about 0.1\nYour response should end with \"The best answer is [the_answer_letter]\" where the [the_answer_letter] is one of A, B, C or D. The best answer is",
                        {
                            "do_sample": false,
                            "until": [
                                "."
                            ],
                            "max_gen_toks": 10
                        }
                    ]
                ],
                "resps": [
                    [
                        "First of all, this description lets use understand the problem assumptions and what\u2019s important in the question:\n0: The Quantum Efficiency of a photon detector is 0:1, which is the ratio of the number of detected photons can be the number."
                    ]
                ],
                "filtered_resps": [
                    "[invalid]"
                ],
                "filter": "flexible-extract",
                "metrics": [
                    "exact_match"
                ],
                "doc_hash": "847fb3f9aafc9781ced426d1a635c5a01cff1d361c58e6c004b9a5845f701d2c",
                "prompt_hash": "44ac6510b5dd770eb50f8b98807608fceabad8d0b3ed37f0444590ae1a831bce",
                "target_hash": "be651d5a8e97690fbb1fafd9b89ea7bb5458b27b1d24a449c67ca762317ac1df",
                "exact_match": 0.0
            }
        ],
        "mmlu_llama_elementary_mathematics": [
            {
                "doc_id": 0,
                "doc": {
                    "question": "What is the value of p in 24 = 2p?",
                    "subject": "elementary_mathematics",
                    "choices": [
                        "p = 4",
                        "p = 8",
                        "p = 12",
                        "p = 24"
                    ],
                    "answer": 2
                },
                "target": "C.",
                "arguments": [
                    [
                        "Given the following question and four candidate answers (A, B, C and D), choose the best answer.\nQuestion: What is the value of p in 24 = 2p?\nA. p = 4\nB. p = 8\nC. p = 12\nD. p = 24\nYour response should end with \"The best answer is [the_answer_letter]\" where the [the_answer_letter] is one of A, B, C or D. The best answer is",
                        {
                            "do_sample": false,
                            "until": [
                                "."
                            ],
                            "max_gen_toks": 10
                        }
                    ]
                ],
                "resps": [
                    [
                        "Given that 24 is divided into 12 parts of 2 each.\n</think>\n<p>24 = 2(12)</p>\n<p>Therefore, p = 12</p>\nThe best answer is C<|end_of_text|><|begin_of_text|>HOw..."
                    ]
                ],
                "filtered_resps": [
                    "C"
                ],
                "filter": "flexible-extract",
                "metrics": [
                    "exact_match"
                ],
                "doc_hash": "d5752e5b658ff76c76dfe5e52657e27fd5ecb445345c9fbf61451187f74c5c3c",
                "prompt_hash": "813b75149aa189f3ce5c7adc3a23068ae17fdf996f64f09cd9d281db70956af3",
                "target_hash": "acbbb708385721a6275edb28b2265071ab2c596d66c45e0f095ca22a55c6fb7d",
                "exact_match": 1.0
            }
        ],
        "mmlu_llama_high_school_statistics": [
            {
                "doc_id": 0,
                "doc": {
                    "question": "The weight of an aspirin tablet is 300 milligrams according to the bottle label. An FDA investigator weighs a simple random sample of seven tablets, obtains weights of 299, 300, 305, 302, 299, 301, and 303, and runs a hypothesis test of the manufacturer's claim. Which of the following gives the P-value of this test?",
                    "subject": "high_school_statistics",
                    "choices": [
                        "P(t > 1.54) with df = 6",
                        "2P(t > 1.54) with df = 6",
                        "P(t > 1.54) with df = 7",
                        "2P(t > 1.54) with df = 7"
                    ],
                    "answer": 1
                },
                "target": "B.",
                "arguments": [
                    [
                        "Given the following question and four candidate answers (A, B, C and D), choose the best answer.\nQuestion: The weight of an aspirin tablet is 300 milligrams according to the bottle label. An FDA investigator weighs a simple random sample of seven tablets, obtains weights of 299, 300, 305, 302, 299, 301, and 303, and runs a hypothesis test of the manufacturer's claim. Which of the following gives the P-value of this test?\nA. P(t > 1.54) with df = 6\nB. 2P(t > 1.54) with df = 6\nC. P(t > 1.54) with df = 7\nD. 2P(t > 1.54) with df = 7\nYour response should end with \"The best answer is [the_answer_letter]\" where the [the_answer_letter] is one of A, B, C or D. The best answer is",
                        {
                            "do_sample": false,
                            "until": [
                                "."
                            ],
                            "max_gen_toks": 10
                        }
                    ]
                ],
                "resps": [
                    [
                        "After reading the related question, I learned that the key point is to examine the hypothesis test result, P-value and degrees of freedom (df) values from the related concepts in the question.New findings show that the P-value calculated from the sample data."
                    ]
                ],
                "filtered_resps": [
                    "[invalid]"
                ],
                "filter": "flexible-extract",
                "metrics": [
                    "exact_match"
                ],
                "doc_hash": "3355368ec6fa437edbfb343c2cdb4c35f009ac1394858fe0f4aa957c25c5f1ec",
                "prompt_hash": "f828dd7c5b16eabdf38de58649058ba1b34decf81361175fd9b2fc8d8501e774",
                "target_hash": "be651d5a8e97690fbb1fafd9b89ea7bb5458b27b1d24a449c67ca762317ac1df",
                "exact_match": 0.0
            }
        ],
        "mmlu_llama_electrical_engineering": [
            {
                "doc_id": 0,
                "doc": {
                    "question": "The Barkhausen criterion for an oscillator",
                    "subject": "electrical_engineering",
                    "choices": [
                        "Loop gain should be unity",
                        "Loop gain should be less than unity",
                        "The phase of a feedback signal with respect to input should be 0\u00b0 or 360\u00b0",
                        "Both A and C"
                    ],
                    "answer": 3
                },
                "target": "D.",
                "arguments": [
                    [
                        "Given the following question and four candidate answers (A, B, C and D), choose the best answer.\nQuestion: The Barkhausen criterion for an oscillator\nA. Loop gain should be unity\nB. Loop gain should be less than unity\nC. The phase of a feedback signal with respect to input should be 0\u00b0 or 360\u00b0\nD. Both A and C\nYour response should end with \"The best answer is [the_answer_letter]\" where the [the_answer_letter] is one of A, B, C or D. The best answer is",
                        {
                            "do_sample": false,
                            "until": [
                                "."
                            ],
                            "max_gen_toks": 10
                        }
                    ]
                ],
                "resps": [
                    [
                        " The Barkhausen criterion is a hypothesis about the oscillation conditions of an inductive circuit, stating that sustained oscillations will only occur if there is a threshold condition of a certain form within the circuit's loop gain (-graph)\n\nLetter A is wrong."
                    ]
                ],
                "filtered_resps": [
                    "[invalid]"
                ],
                "filter": "flexible-extract",
                "metrics": [
                    "exact_match"
                ],
                "doc_hash": "f0e3cc78f68e23c7ff7bc5d727d0ce2937a28c264d4cbb171ebdb009dad93ca8",
                "prompt_hash": "2ec857d6366e339234d76574f8ad25a5e8fb0186c66367b84d6c988d5d41f025",
                "target_hash": "f9e487799dfc7882b1a0db047999249489d02726b2c82055cb68a6bb0311f118",
                "exact_match": 0.0
            }
        ],
        "mmlu_llama_machine_learning": [
            {
                "doc_id": 0,
                "doc": {
                    "question": "Statement 1| Linear regression estimator has the smallest variance among all unbiased estimators. Statement 2| The coefficients \u03b1 assigned to the classifiers assembled by AdaBoost are always non-negative.",
                    "subject": "machine_learning",
                    "choices": [
                        "True, True",
                        "False, False",
                        "True, False",
                        "False, True"
                    ],
                    "answer": 3
                },
                "target": "D.",
                "arguments": [
                    [
                        "Given the following question and four candidate answers (A, B, C and D), choose the best answer.\nQuestion: Statement 1| Linear regression estimator has the smallest variance among all unbiased estimators. Statement 2| The coefficients \u03b1 assigned to the classifiers assembled by AdaBoost are always non-negative.\nA. True, True\nB. False, False\nC. True, False\nD. False, True\nYour response should end with \"The best answer is [the_answer_letter]\" where the [the_answer_letter] is one of A, B, C or D. The best answer is",
                        {
                            "do_sample": false,
                            "until": [
                                "."
                            ],
                            "max_gen_toks": 10
                        }
                    ]
                ],
                "resps": [
                    [
                        "For the linear regression, we want the estimator with the least variance (since variance measures how far the estimate is from the true value).</think>\nExamining statement 1 directly gives us the answer:\nThe best answer is A.\ntrue<|end_of_text|><|begin_of_text|>[]."
                    ]
                ],
                "filtered_resps": [
                    "A"
                ],
                "filter": "flexible-extract",
                "metrics": [
                    "exact_match"
                ],
                "doc_hash": "fe5f9c5ccd8869e51f6d6ebd80d083806b9f07aa36a90c7b69be6ba1fb876108",
                "prompt_hash": "592ea3a8a7806814119a8795e425af66d14a502e5c1d19e43550e0c9957cd2f5",
                "target_hash": "f9e487799dfc7882b1a0db047999249489d02726b2c82055cb68a6bb0311f118",
                "exact_match": 0.0
            }
        ],
        "mmlu_llama_abstract_algebra": [
            {
                "doc_id": 0,
                "doc": {
                    "question": "Find the degree for the given field extension Q(sqrt(2), sqrt(3), sqrt(18)) over Q.",
                    "subject": "abstract_algebra",
                    "choices": [
                        "0",
                        "4",
                        "2",
                        "6"
                    ],
                    "answer": 1
                },
                "target": "B.",
                "arguments": [
                    [
                        "Given the following question and four candidate answers (A, B, C and D), choose the best answer.\nQuestion: Find the degree for the given field extension Q(sqrt(2), sqrt(3), sqrt(18)) over Q.\nA. 0\nB. 4\nC. 2\nD. 6\nYour response should end with \"The best answer is [the_answer_letter]\" where the [the_answer_letter] is one of A, B, C or D. The best answer is",
                        {
                            "do_sample": false,
                            "until": [
                                "."
                            ],
                            "max_gen_toks": 10
                        }
                    ]
                ],
                "resps": [
                    [
                        "First, let's recall what a degree is: the degree of a field extension L/K is the minimum number of times one must apply the field extension formula to obtain the identity element one from the base field K (which has degree 1 in..."
                    ]
                ],
                "filtered_resps": [
                    "[invalid]"
                ],
                "filter": "flexible-extract",
                "metrics": [
                    "exact_match"
                ],
                "doc_hash": "6cd9c6502447a6f35e13e2e94ad6a1ba2ac5672b02acd62d3f1ad2829be63827",
                "prompt_hash": "d517d1eac1ae3aab20ef22bcaed4f875c0e43bd68e8a1652889282ba98b9ae5c",
                "target_hash": "be651d5a8e97690fbb1fafd9b89ea7bb5458b27b1d24a449c67ca762317ac1df",
                "exact_match": 0.0
            }
        ],
        "mmlu_llama_high_school_mathematics": [
            {
                "doc_id": 0,
                "doc": {
                    "question": "If a pentagon P with vertices at (\u2013 2, \u2013 4), (\u2013 4, 1), (\u20131, 4), (2, 4), and (3, 0) is reflected across the line y = x to get a new pentagon, P\u2019, then one of the vertices of P\u2019 is",
                    "subject": "high_school_mathematics",
                    "choices": [
                        "(0, \u2013 3)",
                        "(4, 1)",
                        "(2, 2)",
                        "(\u2013 4, \u20132)"
                    ],
                    "answer": 3
                },
                "target": "D.",
                "arguments": [
                    [
                        "Given the following question and four candidate answers (A, B, C and D), choose the best answer.\nQuestion: If a pentagon P with vertices at (\u2013 2, \u2013 4), (\u2013 4, 1), (\u20131, 4), (2, 4), and (3, 0) is reflected across the line y = x to get a new pentagon, P\u2019, then one of the vertices of P\u2019 is\nA. (0, \u2013 3)\nB. (4, 1)\nC. (2, 2)\nD. (\u2013 4, \u20132)\nYour response should end with \"The best answer is [the_answer_letter]\" where the [the_answer_letter] is one of A, B, C or D. The best answer is",
                        {
                            "do_sample": false,
                            "until": [
                                "."
                            ],
                            "max_gen_toks": 10
                        }
                    ]
                ],
                "resps": [
                    [
                        " The original pentagon had vertices (-2, -4), (-4, 1), (-1, 4), (2, 4), and (3, 0), and then these points were reflected across the line y = x."
                    ]
                ],
                "filtered_resps": [
                    "4 2"
                ],
                "filter": "flexible-extract",
                "metrics": [
                    "exact_match"
                ],
                "doc_hash": "3782710b642de52c2333b8130d9db2d6e3f06ce215307cc79028a21e1cfe217a",
                "prompt_hash": "7f85c22dd3e891546913fd33eeb085d45298a4bee2c6e6c930baf66f6b6eba6a",
                "target_hash": "f9e487799dfc7882b1a0db047999249489d02726b2c82055cb68a6bb0311f118",
                "exact_match": 0.0
            }
        ],
        "mmlu_llama_college_computer_science": [
            {
                "doc_id": 0,
                "doc": {
                    "question": "The access matrix approach to protection has the difficulty that",
                    "subject": "college_computer_science",
                    "choices": [
                        "the matrix, if stored directly, is large and can be clumsy to manage",
                        "it is not capable of expressing complex protection requirements",
                        "deciding whether a process has access to a resource is undecidable",
                        "there is no way to express who has rights to change the access matrix itself"
                    ],
                    "answer": 0
                },
                "target": "A.",
                "arguments": [
                    [
                        "Given the following question and four candidate answers (A, B, C and D), choose the best answer.\nQuestion: The access matrix approach to protection has the difficulty that\nA. the matrix, if stored directly, is large and can be clumsy to manage\nB. it is not capable of expressing complex protection requirements\nC. deciding whether a process has access to a resource is undecidable\nD. there is no way to express who has rights to change the access matrix itself\nYour response should end with \"The best answer is [the_answer_letter]\" where the [the_answer_letter] is one of A, B, C or D. The best answer is",
                        {
                            "do_sample": false,
                            "until": [
                                "."
                            ],
                            "max_gen_toks": 10
                        }
                    ]
                ],
                "resps": [
                    [
                        " To understand why protection systems are necessary, I must explain a bit about how computers work and security.\n\nData in a computer is organized into files, which themselves can be organized into directories (together with associated metadata such as permissions and ownership information)."
                    ]
                ],
                "filtered_resps": [
                    "[invalid]"
                ],
                "filter": "flexible-extract",
                "metrics": [
                    "exact_match"
                ],
                "doc_hash": "81d922f04944340a0342b15a495db4acefbd9bd3e95ce50baa06b577f829e02a",
                "prompt_hash": "791d5ff3ba55a88ea860c5d04f7e9ffdb4ba8b5452f301edeb49f5a59edeb996",
                "target_hash": "e53657178cb6855ac4b2029197a64b0ce6af712b69433254ae18be74d224f991",
                "exact_match": 0.0
            }
        ],
        "mmlu_llama_high_school_chemistry": [
            {
                "doc_id": 0,
                "doc": {
                    "question": "London dispersion forces are caused by",
                    "subject": "high_school_chemistry",
                    "choices": [
                        "temporary dipoles created by the position of electrons around the nuclei in a molecule",
                        "the three-dimensional intermolecular bonding present in all covalent substances",
                        "the uneven electron-to-proton ratio found on individual atoms of a molecule",
                        "the electronegativity differences between the different atoms in a molecule"
                    ],
                    "answer": 0
                },
                "target": "A.",
                "arguments": [
                    [
                        "Given the following question and four candidate answers (A, B, C and D), choose the best answer.\nQuestion: London dispersion forces are caused by\nA. temporary dipoles created by the position of electrons around the nuclei in a molecule\nB. the three-dimensional intermolecular bonding present in all covalent substances\nC. the uneven electron-to-proton ratio found on individual atoms of a molecule\nD. the electronegativity differences between the different atoms in a molecule\nYour response should end with \"The best answer is [the_answer_letter]\" where the [the_answer_letter] is one of A, B, C or D. The best answer is",
                        {
                            "do_sample": false,
                            "until": [
                                "."
                            ],
                            "max_gen_toks": 10
                        }
                    ]
                ],
                "resps": [
                    [
                        " <![CDATA[ ]]>\nAnswer: A\nExplanation: London dispersion forces, also called inductive or temporary dipoles, are electrostatic forces of attraction between molecules that originate from the fluctuating dipoles produced by irregularly shaped charged particles."
                    ]
                ],
                "filtered_resps": [
                    "(A)"
                ],
                "filter": "flexible-extract",
                "metrics": [
                    "exact_match"
                ],
                "doc_hash": "1be789f01d9511f86d95d21a38491186abbf95be3c41d2307771f2b3624cd3fd",
                "prompt_hash": "4dbdfef66c04a60da91ba2e3f60e7fbdc8ee7fea212ac2f38702d02c546d65ab",
                "target_hash": "e53657178cb6855ac4b2029197a64b0ce6af712b69433254ae18be74d224f991",
                "exact_match": 1.0
            }
        ],
        "mmlu_llama_college_chemistry": [
            {
                "doc_id": 0,
                "doc": {
                    "question": "The rate, r, of a zero-order chemical reaction A \u2192 B can be expressed as which of the following?",
                    "subject": "college_chemistry",
                    "choices": [
                        "r = k ln[A]",
                        "r = k [A]^2",
                        "r = k [A]",
                        "r = k"
                    ],
                    "answer": 3
                },
                "target": "D.",
                "arguments": [
                    [
                        "Given the following question and four candidate answers (A, B, C and D), choose the best answer.\nQuestion: The rate, r, of a zero-order chemical reaction A \u2192 B can be expressed as which of the following?\nA. r = k ln[A]\nB. r = k [A]^2\nC. r = k [A]\nD. r = k\nYour response should end with \"The best answer is [the_answer_letter]\" where the [the_answer_letter] is one of A, B, C or D. The best answer is",
                        {
                            "do_sample": false,
                            "until": [
                                "."
                            ],
                            "max_gen_toks": 10
                        }
                    ]
                ],
                "resps": [
                    [
                        "Let's see the equation for zero-order reactions first.\nIn zero-order reactions, the rate of the reaction is constant with time, regardless of the concentration of the reactant (the reactant's concentration does not affect the rate of the reaction).\n\n..."
                    ]
                ],
                "filtered_resps": [
                    "[invalid]"
                ],
                "filter": "flexible-extract",
                "metrics": [
                    "exact_match"
                ],
                "doc_hash": "a7fccbe9ff02ae90df60feb89b419dbdb9a3a404c5e302fa09e18a8eae18122b",
                "prompt_hash": "a57022eb5291a76c2a5ef0f8696f18e4f3846249bc4b969f0ee2b744f5fbc5c3",
                "target_hash": "f9e487799dfc7882b1a0db047999249489d02726b2c82055cb68a6bb0311f118",
                "exact_match": 0.0
            }
        ],
        "mmlu_llama_college_mathematics": [
            {
                "doc_id": 0,
                "doc": {
                    "question": "Let k be the number of real solutions of the equation e^x + x - 2 = 0 in the interval [0, 1], and let n be the number of real solutions that are not in [0, 1]. Which of the following is true?",
                    "subject": "college_mathematics",
                    "choices": [
                        "k = 0 and n = 1",
                        "k = 1 and n = 0",
                        "k = n = 1",
                        "k > 1"
                    ],
                    "answer": 1
                },
                "target": "B.",
                "arguments": [
                    [
                        "Given the following question and four candidate answers (A, B, C and D), choose the best answer.\nQuestion: Let k be the number of real solutions of the equation e^x + x - 2 = 0 in the interval [0, 1], and let n be the number of real solutions that are not in [0, 1]. Which of the following is true?\nA. k = 0 and n = 1\nB. k = 1 and n = 0\nC. k = n = 1\nD. k > 1\nYour response should end with \"The best answer is [the_answer_letter]\" where the [the_answer_letter] is one of A, B, C or D. The best answer is",
                        {
                            "do_sample": false,
                            "until": [
                                "."
                            ],
                            "max_gen_toks": 10
                        }
                    ]
                ],
                "resps": [
                    [
                        " When I solve the equation 0 < x_4 - x_2 < 1, I get 2 < x_4 < 3 which is consistent with the statement that there are at least 2 numbers in the interval [0."
                    ]
                ],
                "filtered_resps": [
                    "[invalid]"
                ],
                "filter": "flexible-extract",
                "metrics": [
                    "exact_match"
                ],
                "doc_hash": "58830e7abd7bdd4f34a7823334bdc5a5905aec56ee178f9b94a9e23892c9fe86",
                "prompt_hash": "75e250e752e7399fe4c5031590fa336fe61796925a71689254585f79a6812647",
                "target_hash": "be651d5a8e97690fbb1fafd9b89ea7bb5458b27b1d24a449c67ca762317ac1df",
                "exact_match": 0.0
            }
        ],
        "mmlu_llama_professional_accounting": [
            {
                "doc_id": 0,
                "doc": {
                    "question": "You bought a limousine for $98,000 and are planning to rent it for weddings, ceremonies and parties at $245 per hour. If you estimate the car will be hired for 2 hours a day on average, with daily costs at about $50, what is the estimated yearly yield on your investment if you work all year round, i.e. every day of the year, including any festivities and weekends?",
                    "subject": "professional_accounting",
                    "choices": [
                        "164%",
                        "1.64%",
                        "0.45%",
                        "183%"
                    ],
                    "answer": 0
                },
                "target": "A.",
                "arguments": [
                    [
                        "Given the following question and four candidate answers (A, B, C and D), choose the best answer.\nQuestion: You bought a limousine for $98,000 and are planning to rent it for weddings, ceremonies and parties at $245 per hour. If you estimate the car will be hired for 2 hours a day on average, with daily costs at about $50, what is the estimated yearly yield on your investment if you work all year round, i.e. every day of the year, including any festivities and weekends?\nA. 164%\nB. 1.64%\nC. 0.45%\nD. 183%\nYour response should end with \"The best answer is [the_answer_letter]\" where the [the_answer_letter] is one of A, B, C or D. The best answer is",
                        {
                            "do_sample": false,
                            "until": [
                                "."
                            ],
                            "max_gen_toks": 10
                        }
                    ]
                ],
                "resps": [
                    [
                        "Clearly the hourly rental rate is the most important figure from the problem: $245 per hour is what the limousine will be rented out for, and daily cost is $50, so $245 times 2 hours gives $490 per day."
                    ]
                ],
                "filtered_resps": [
                    "[invalid]"
                ],
                "filter": "flexible-extract",
                "metrics": [
                    "exact_match"
                ],
                "doc_hash": "1aabaf9f8f5828f0199f1464f2bb03152932cb13a9321ccb9971e2314e192070",
                "prompt_hash": "63ce82f32a9c260f0493bca2cba66831fe9673acc3e3ee6cc07e3f1b734ee3e1",
                "target_hash": "e53657178cb6855ac4b2029197a64b0ce6af712b69433254ae18be74d224f991",
                "exact_match": 0.0
            }
        ],
        "mmlu_llama_professional_medicine": [
            {
                "doc_id": 0,
                "doc": {
                    "question": "A 67-year-old woman comes to the physician for a follow-up examination. She had a pulmonary embolism and required treatment in the hospital for 3 weeks. She had a retroperitoneal hemorrhage; anticoagulant therapy was temporarily discontinued, and she underwent placement of an inferior vena cava (IVC) filter. She had a hematoma that was resolving on discharge from the hospital 2 weeks ago. Today, she says she has had a persistent sensation of tingling and numbness of her left thigh that she did not report in the hospital because she thought it would go away; the sensation has improved somewhat during the past week. Her only medication is warfarin. Vital signs are within normal limits. Examination of the skin shows no abnormalities. Muscle strength is normal. Sensation to light touch is decreased over a 5 x 5-cm area on the lateral aspect of the left anterior thigh. Which of the following is the most likely cause of this patient's decreased sensation?",
                    "subject": "professional_medicine",
                    "choices": [
                        "Cerebral infarction during the hospitalization",
                        "Complication of the IVC filter placement",
                        "Compression of the lateral femoral cutaneous nerve",
                        "Hematoma of the left thigh"
                    ],
                    "answer": 2
                },
                "target": "C.",
                "arguments": [
                    [
                        "Given the following question and four candidate answers (A, B, C and D), choose the best answer.\nQuestion: A 67-year-old woman comes to the physician for a follow-up examination. She had a pulmonary embolism and required treatment in the hospital for 3 weeks. She had a retroperitoneal hemorrhage; anticoagulant therapy was temporarily discontinued, and she underwent placement of an inferior vena cava (IVC) filter. She had a hematoma that was resolving on discharge from the hospital 2 weeks ago. Today, she says she has had a persistent sensation of tingling and numbness of her left thigh that she did not report in the hospital because she thought it would go away; the sensation has improved somewhat during the past week. Her only medication is warfarin. Vital signs are within normal limits. Examination of the skin shows no abnormalities. Muscle strength is normal. Sensation to light touch is decreased over a 5 x 5-cm area on the lateral aspect of the left anterior thigh. Which of the following is the most likely cause of this patient's decreased sensation?\nA. Cerebral infarction during the hospitalization\nB. Complication of the IVC filter placement\nC. Compression of the lateral femoral cutaneous nerve\nD. Hematoma of the left thigh\nYour response should end with \"The best answer is [the_answer_letter]\" where the [the_answer_letter] is one of A, B, C or D. The best answer is",
                        {
                            "do_sample": false,
                            "until": [
                                "."
                            ],
                            "max_gen_toks": 10
                        }
                    ]
                ],
                "resps": [
                    [
                        "Once the answer becomes clear, replace the brackets and letters and write the answer.<answer>Given the above scenario, the most likely cause of the patient's decreased sensation is: [C] The compression of the lateral femoral cutaneous nerve.\n."
                    ]
                ],
                "filtered_resps": [
                    "(C)"
                ],
                "filter": "flexible-extract",
                "metrics": [
                    "exact_match"
                ],
                "doc_hash": "a3a550cfd19a301c9f21364aaae6830584672d48a03b50eeb33ac8d61ddf6fb2",
                "prompt_hash": "b149f233a3f3f7b20e48f4a20b4f3d6e32360e0eabfc93ae645b669a2348a51f",
                "target_hash": "acbbb708385721a6275edb28b2265071ab2c596d66c45e0f095ca22a55c6fb7d",
                "exact_match": 1.0
            }
        ],
        "mmlu_llama_marketing": [
            {
                "doc_id": 0,
                "doc": {
                    "question": "This is a hierarchy of effects or sequential model used to explain how advertising works:",
                    "subject": "marketing",
                    "choices": [
                        "ADD.",
                        "AIDA.",
                        "PESTLE.",
                        "SWOT."
                    ],
                    "answer": 1
                },
                "target": "B.",
                "arguments": [
                    [
                        "Given the following question and four candidate answers (A, B, C and D), choose the best answer.\nQuestion: This is a hierarchy of effects or sequential model used to explain how advertising works:\nA. ADD.\nB. AIDA.\nC. PESTLE.\nD. SWOT.\nYour response should end with \"The best answer is [the_answer_letter]\" where the [the_answer_letter] is one of A, B, C or D. The best answer is",
                        {
                            "do_sample": false,
                            "until": [
                                "."
                            ],
                            "max_gen_toks": 10
                        }
                    ]
                ],
                "resps": [
                    [
                        "Advertising is a trans-media practice that seeks to convey a message or tell a story to an audience, generating interest and encouraging engagement, behavior, or some form of altered response from the audience (the listener, reader, or viewer in the given case)."
                    ]
                ],
                "filtered_resps": [
                    "[invalid]"
                ],
                "filter": "flexible-extract",
                "metrics": [
                    "exact_match"
                ],
                "doc_hash": "0bf9f9c33763112bd88dca9c958c20e6065cf040f9fda0b760a25a4fe07752e7",
                "prompt_hash": "883bbc8fca4767925fa86f0131005f68a9848d000da93af977f6a756355b91ca",
                "target_hash": "be651d5a8e97690fbb1fafd9b89ea7bb5458b27b1d24a449c67ca762317ac1df",
                "exact_match": 0.0
            }
        ],
        "mmlu_llama_virology": [
            {
                "doc_id": 0,
                "doc": {
                    "question": "How many human polyomaviruses are known at present?",
                    "subject": "virology",
                    "choices": [
                        "100",
                        "1",
                        "10",
                        "unknown"
                    ],
                    "answer": 0
                },
                "target": "A.",
                "arguments": [
                    [
                        "Given the following question and four candidate answers (A, B, C and D), choose the best answer.\nQuestion: How many human polyomaviruses are known at present?\nA. 100\nB. 1\nC. 10\nD. unknown\nYour response should end with \"The best answer is [the_answer_letter]\" where the [the_answer_letter] is one of A, B, C or D. The best answer is",
                        {
                            "do_sample": false,
                            "until": [
                                "."
                            ],
                            "max_gen_toks": 10
                        }
                    ]
                ],
                "resps": [
                    [
                        " Polyomaviruses are a group of viruses that infect humans and are most commonly found in the urinary tract and respiratory tract, but can also cause diseases such as Merkel cell carcinoma, which is a rare and deadly form of skin cancer in older individuals."
                    ]
                ],
                "filtered_resps": [
                    "[invalid]"
                ],
                "filter": "flexible-extract",
                "metrics": [
                    "exact_match"
                ],
                "doc_hash": "69cef29b83da65939b9afec2e0a681eb425bab99c7e8730b5664ffd01e95cfc7",
                "prompt_hash": "7159fe5eebe5a44137705df37ec7b87ac5474e056d2981a4fbc5aa251d7a3249",
                "target_hash": "e53657178cb6855ac4b2029197a64b0ce6af712b69433254ae18be74d224f991",
                "exact_match": 0.0
            }
        ],
        "mmlu_llama_miscellaneous": [
            {
                "doc_id": 0,
                "doc": {
                    "question": "An important source of information on the credit rating of retail businesses is",
                    "subject": "miscellaneous",
                    "choices": [
                        "the Retail Merchants Association",
                        "the local chamber of commerce",
                        "Dun & Bradstreet, Inc.",
                        "the United States Retail Credit Association"
                    ],
                    "answer": 2
                },
                "target": "C.",
                "arguments": [
                    [
                        "Given the following question and four candidate answers (A, B, C and D), choose the best answer.\nQuestion: An important source of information on the credit rating of retail businesses is\nA. the Retail Merchants Association\nB. the local chamber of commerce\nC. Dun & Bradstreet, Inc.\nD. the United States Retail Credit Association\nYour response should end with \"The best answer is [the_answer_letter]\" where the [the_answer_letter] is one of A, B, C or D. The best answer is",
                        {
                            "do_sample": false,
                            "until": [
                                "."
                            ],
                            "max_gen_toks": 10
                        }
                    ]
                ],
                "resps": [
                    [
                        " One can find information on credit from all of these sources, but Dun & Bradstreet is the oldest business credit reporting company and thus the best source for credit information on retail businesses.\nThe best answer is C<response>\nLet's examine each part."
                    ]
                ],
                "filtered_resps": [
                    "C"
                ],
                "filter": "flexible-extract",
                "metrics": [
                    "exact_match"
                ],
                "doc_hash": "74fab563e622f6347ffe27c30c397b850471d73fb53535ec6ee2777837ccc428",
                "prompt_hash": "e7528816b44206fe87ba0ec00caf37582b5b478c8f9543802af2d3dc3299284d",
                "target_hash": "acbbb708385721a6275edb28b2265071ab2c596d66c45e0f095ca22a55c6fb7d",
                "exact_match": 1.0
            }
        ],
        "mmlu_llama_clinical_knowledge": [
            {
                "doc_id": 0,
                "doc": {
                    "question": "What size of cannula would you use in a patient who needed a rapid blood transfusion (as of 2020 medical knowledge)?",
                    "subject": "clinical_knowledge",
                    "choices": [
                        "18 gauge.",
                        "20 gauge.",
                        "22 gauge.",
                        "24 gauge."
                    ],
                    "answer": 0
                },
                "target": "A.",
                "arguments": [
                    [
                        "Given the following question and four candidate answers (A, B, C and D), choose the best answer.\nQuestion: What size of cannula would you use in a patient who needed a rapid blood transfusion (as of 2020 medical knowledge)?\nA. 18 gauge.\nB. 20 gauge.\nC. 22 gauge.\nD. 24 gauge.\nYour response should end with \"The best answer is [the_answer_letter]\" where the [the_answer_letter] is one of A, B, C or D. The best answer is",
                        {
                            "do_sample": false,
                            "until": [
                                "."
                            ],
                            "max_gen_toks": 10
                        }
                    ]
                ],
                "resps": [
                    [
                        "Often, when a patient needs supplemental blood, providing it as rapidly as possible is quite helpful to the patient, so a larger bore (thinner wall) needle or cannula is often used for such situations, though there is a limit with gauge."
                    ]
                ],
                "filtered_resps": [
                    "[invalid]"
                ],
                "filter": "flexible-extract",
                "metrics": [
                    "exact_match"
                ],
                "doc_hash": "0bc8c56eaf0c56845c1c58ab1793813c5026da148cafdbd7e475314fbadf4c8a",
                "prompt_hash": "f73bbb640ef7d83edc4f954833415940b8bd97afd1160ac6ecf349452134f4cf",
                "target_hash": "e53657178cb6855ac4b2029197a64b0ce6af712b69433254ae18be74d224f991",
                "exact_match": 0.0
            }
        ],
        "mmlu_llama_nutrition": [
            {
                "doc_id": 0,
                "doc": {
                    "question": "Which foods tend to be consumed in lower quantities in Wales and Scotland (as of 2020)?\n",
                    "subject": "nutrition",
                    "choices": [
                        "Meat",
                        "Confectionary",
                        "Fruits and vegetables",
                        "Potatoes"
                    ],
                    "answer": 2
                },
                "target": "C.",
                "arguments": [
                    [
                        "Given the following question and four candidate answers (A, B, C and D), choose the best answer.\nQuestion: Which foods tend to be consumed in lower quantities in Wales and Scotland (as of 2020)?\nA. Meat\nB. Confectionary\nC. Fruits and vegetables\nD. Potatoes\nYour response should end with \"The best answer is [the_answer_letter]\" where the [the_answer_letter] is one of A, B, C or D. The best answer is",
                        {
                            "do_sample": false,
                            "until": [
                                "."
                            ],
                            "max_gen_toks": 10
                        }
                    ]
                ],
                "resps": [
                    [
                        "Wales and Scotland are regions in the UK known for their traditional cuisines that include many locally sourced produce and include less meat consumption compared to the rest of the UK due to cultural, historical and religious factors that discouraged the consumption of meat especially on..."
                    ]
                ],
                "filtered_resps": [
                    "(A)"
                ],
                "filter": "flexible-extract",
                "metrics": [
                    "exact_match"
                ],
                "doc_hash": "00af537bfae28966b92557b7ef9baf348b4f8007fce1195837daff91e782ff6b",
                "prompt_hash": "a2ac7445b3624f2543c1677e1d686bea0913847899e4d20da31e97af5cd99463",
                "target_hash": "acbbb708385721a6275edb28b2265071ab2c596d66c45e0f095ca22a55c6fb7d",
                "exact_match": 0.0
            }
        ],
        "mmlu_llama_medical_genetics": [
            {
                "doc_id": 0,
                "doc": {
                    "question": "In a Robertsonian translocation fusion occurs at the:",
                    "subject": "medical_genetics",
                    "choices": [
                        "telomeres.",
                        "centromeres.",
                        "histones.",
                        "ends of the long arms."
                    ],
                    "answer": 1
                },
                "target": "B.",
                "arguments": [
                    [
                        "Given the following question and four candidate answers (A, B, C and D), choose the best answer.\nQuestion: In a Robertsonian translocation fusion occurs at the:\nA. telomeres.\nB. centromeres.\nC. histones.\nD. ends of the long arms.\nYour response should end with \"The best answer is [the_answer_letter]\" where the [the_answer_letter] is one of A, B, C or D. The best answer is",
                        {
                            "do_sample": false,
                            "until": [
                                "."
                            ],
                            "max_gen_toks": 10
                        }
                    ]
                ],
                "resps": [
                    [
                        " Robertsonian translocation is a balanced chromosomal translocation between non-homologous pairs of non-sister chromatids in humans, described by T.C.Robertson in 1963.\n\nThe translocation occurs at the c-met."
                    ]
                ],
                "filtered_resps": [
                    "[invalid]"
                ],
                "filter": "flexible-extract",
                "metrics": [
                    "exact_match"
                ],
                "doc_hash": "3e51073d675cc2bf18c260920a0357fccb5481fad25ecb9f749b69db0c58afd3",
                "prompt_hash": "c6d16cef630366759bc5be1992758d241a585453792a1dd8db0a4531b2b25414",
                "target_hash": "be651d5a8e97690fbb1fafd9b89ea7bb5458b27b1d24a449c67ca762317ac1df",
                "exact_match": 0.0
            }
        ],
        "mmlu_llama_business_ethics": [
            {
                "doc_id": 0,
                "doc": {
                    "question": "_______ such as bitcoin are becoming increasingly mainstream and have a whole host of associated ethical implications, for example, they are______ and more ______. However, they have also been used to engage in _______.",
                    "subject": "business_ethics",
                    "choices": [
                        "Cryptocurrencies, Expensive, Secure, Financial Crime",
                        "Traditional currency, Cheap, Unsecure, Charitable giving",
                        "Cryptocurrencies, Cheap, Secure, Financial crime",
                        "Traditional currency, Expensive, Unsecure, Charitable giving"
                    ],
                    "answer": 2
                },
                "target": "C.",
                "arguments": [
                    [
                        "Given the following question and four candidate answers (A, B, C and D), choose the best answer.\nQuestion: _______ such as bitcoin are becoming increasingly mainstream and have a whole host of associated ethical implications, for example, they are______ and more ______. However, they have also been used to engage in _______.\nA. Cryptocurrencies, Expensive, Secure, Financial Crime\nB. Traditional currency, Cheap, Unsecure, Charitable giving\nC. Cryptocurrencies, Cheap, Secure, Financial crime\nD. Traditional currency, Expensive, Unsecure, Charitable giving\nYour response should end with \"The best answer is [the_answer_letter]\" where the [the_answer_letter] is one of A, B, C or D. The best answer is",
                        {
                            "do_sample": false,
                            "until": [
                                "."
                            ],
                            "max_gen_toks": 10
                        }
                    ]
                ],
                "resps": [
                    [
                        " Daniel admitted he didn't know much about bitcoins and their implications for society, but from the discussion, Bitcoin is a form of cryptocurrency because it is a virtually created money used for purchases that don't require a transaction to a bank or the bank system."
                    ]
                ],
                "filtered_resps": [
                    "[invalid]"
                ],
                "filter": "flexible-extract",
                "metrics": [
                    "exact_match"
                ],
                "doc_hash": "8a1bbddb3f1b7909c0397ed7d5c6a8528a4374a27868696e2477fd899f4812bc",
                "prompt_hash": "cb8c28c2963253d5a9eeae5ef884639b041c279d4b2edce3267f9906dc74da44",
                "target_hash": "acbbb708385721a6275edb28b2265071ab2c596d66c45e0f095ca22a55c6fb7d",
                "exact_match": 0.0
            }
        ],
        "mmlu_llama_management": [
            {
                "doc_id": 0,
                "doc": {
                    "question": "What is the term for a sub-optimal but acceptable outcome of negotiations between parties?",
                    "subject": "management",
                    "choices": [
                        "Bargaining",
                        "Satisficing",
                        "Accepting",
                        "Compromising"
                    ],
                    "answer": 1
                },
                "target": "B.",
                "arguments": [
                    [
                        "Given the following question and four candidate answers (A, B, C and D), choose the best answer.\nQuestion: What is the term for a sub-optimal but acceptable outcome of negotiations between parties?\nA. Bargaining\nB. Satisficing\nC. Accepting\nD. Compromising\nYour response should end with \"The best answer is [the_answer_letter]\" where the [the_answer_letter] is one of A, B, C or D. The best answer is",
                        {
                            "do_sample": false,
                            "until": [
                                "."
                            ],
                            "max_gen_toks": 10
                        }
                    ]
                ],
                "resps": [
                    [
                        "When negotiating, particularly in conflicts involving two opposing parties (zero-sum games), the best outcome would typically be an agreement that divided the available resources or gains as evenly as possible, where each party's benefits were maximized.\nHowever, the goal of..."
                    ]
                ],
                "filtered_resps": [
                    "[invalid]"
                ],
                "filter": "flexible-extract",
                "metrics": [
                    "exact_match"
                ],
                "doc_hash": "e4ad1f88913c0a962a6f6f20a52f8e82a1cfc2961b44b3cdf43c8d98111b70dc",
                "prompt_hash": "6943fde15b6a3059b38fb20199b3c1a5e847362ba19a140e14bc558969f3ea8e",
                "target_hash": "be651d5a8e97690fbb1fafd9b89ea7bb5458b27b1d24a449c67ca762317ac1df",
                "exact_match": 0.0
            }
        ],
        "mmlu_llama_global_facts": [
            {
                "doc_id": 0,
                "doc": {
                    "question": "As of 2016, about what percentage of adults aged 18 years or older were overweight?",
                    "subject": "global_facts",
                    "choices": [
                        "10%",
                        "20%",
                        "40%",
                        "80%"
                    ],
                    "answer": 2
                },
                "target": "C.",
                "arguments": [
                    [
                        "Given the following question and four candidate answers (A, B, C and D), choose the best answer.\nQuestion: As of 2016, about what percentage of adults aged 18 years or older were overweight?\nA. 10%\nB. 20%\nC. 40%\nD. 80%\nYour response should end with \"The best answer is [the_answer_letter]\" where the [the_answer_letter] is one of A, B, C or D. The best answer is",
                        {
                            "do_sample": false,
                            "until": [
                                "."
                            ],
                            "max_gen_toks": 10
                        }
                    ]
                ],
                "resps": [
                    [
                        " (Note: an answer that simply gives a range or a percentage point would not be satisfactory; a rough estimate is fine but the answer should not be a guess but a reasoned conclusion based on the information presented.)\n\nQuestion: As of 2016."
                    ]
                ],
                "filtered_resps": [
                    "(B)"
                ],
                "filter": "flexible-extract",
                "metrics": [
                    "exact_match"
                ],
                "doc_hash": "d5065a9f691760e4132d24cfae0b6359721430fe32bae8db938c8da78c47507c",
                "prompt_hash": "02f6244d4107206c50792003777f3739bc813f4677843c17159c85013fc4633e",
                "target_hash": "acbbb708385721a6275edb28b2265071ab2c596d66c45e0f095ca22a55c6fb7d",
                "exact_match": 0.0
            }
        ],
        "mmlu_llama_college_medicine": [
            {
                "doc_id": 0,
                "doc": {
                    "question": "A police officer carries out hundreds of traffic stops every year. When his supervisor is reviewing the officer\u2019s records for the past year, he notices that the officer is equally likely to stop people of various genders, ages, and races. However, he is significantly more likely to write tickets for middle-aged white males with dark hair and eyes. When confronted with this fact, the officer truthfully states that he has no idea why that is, and that it must simply be a coincidence. Unbeknownst to the officer, this behavior is tied to the fact that these men look like his father, with whom he had an abusive relationship as a child. What psychological framework would directly address the unconscious bias in his behavior? ",
                    "subject": "college_medicine",
                    "choices": [
                        "Behaviorist",
                        "Psychoanalytic",
                        "Cognitive behavioral",
                        "Humanistic"
                    ],
                    "answer": 1
                },
                "target": "B.",
                "arguments": [
                    [
                        "Given the following question and four candidate answers (A, B, C and D), choose the best answer.\nQuestion: A police officer carries out hundreds of traffic stops every year. When his supervisor is reviewing the officer\u2019s records for the past year, he notices that the officer is equally likely to stop people of various genders, ages, and races. However, he is significantly more likely to write tickets for middle-aged white males with dark hair and eyes. When confronted with this fact, the officer truthfully states that he has no idea why that is, and that it must simply be a coincidence. Unbeknownst to the officer, this behavior is tied to the fact that these men look like his father, with whom he had an abusive relationship as a child. What psychological framework would directly address the unconscious bias in his behavior?\nA. Behaviorist\nB. Psychoanalytic\nC. Cognitive behavioral\nD. Humanistic\nYour response should end with \"The best answer is [the_answer_letter]\" where the [the_answer_letter] is one of A, B, C or D. The best answer is",
                        {
                            "do_sample": false,
                            "until": [
                                "."
                            ],
                            "max_gen_toks": 10
                        }
                    ]
                ],
                "resps": [
                    [
                        " I think option B (Psychoanalytic) refers to the unconscious biases and deep-rooted emotions, like childhood traumas, that can shape a person's behavior without them being aware of it, which seems to describe what the officer is doing."
                    ]
                ],
                "filtered_resps": [
                    "(B)"
                ],
                "filter": "flexible-extract",
                "metrics": [
                    "exact_match"
                ],
                "doc_hash": "77c2e6c0690f403d8e9098580ba76b8857632d46b5fa8b4b8f1afbb864095b27",
                "prompt_hash": "214dfea3b706c620114167fd5c5c0da5a895e07d18ba0b025f230a4dcb35ccb1",
                "target_hash": "be651d5a8e97690fbb1fafd9b89ea7bb5458b27b1d24a449c67ca762317ac1df",
                "exact_match": 1.0
            }
        ],
        "mmlu_llama_human_aging": [
            {
                "doc_id": 0,
                "doc": {
                    "question": "The study of older adults and aging is referred to as",
                    "subject": "human_aging",
                    "choices": [
                        "Gerontology",
                        "Geropsychiatry",
                        "Geriatrics",
                        "Gero-education"
                    ],
                    "answer": 0
                },
                "target": "A.",
                "arguments": [
                    [
                        "Given the following question and four candidate answers (A, B, C and D), choose the best answer.\nQuestion: The study of older adults and aging is referred to as\nA. Gerontology\nB. Geropsychiatry\nC. Geriatrics\nD. Gero-education\nYour response should end with \"The best answer is [the_answer_letter]\" where the [the_answer_letter] is one of A, B, C or D. The best answer is",
                        {
                            "do_sample": false,
                            "until": [
                                "."
                            ],
                            "max_gen_toks": 10
                        }
                    ]
                ],
                "resps": [
                    [
                        " Gerontology is the scientific study of aging.</think> Geropsychiatry is a subspecialty of psychiatry focusing on the elderly</think> Therefore, C geriatrics is related to the care of old people, not to the study."
                    ]
                ],
                "filtered_resps": [
                    "(C)"
                ],
                "filter": "flexible-extract",
                "metrics": [
                    "exact_match"
                ],
                "doc_hash": "102d66e7eb8de963218a501476b4b2de0608c168babdd40dbda89f75e8bf8270",
                "prompt_hash": "39e5ceb5cba54b8cbb27f4ce091e539e42378f6367d2904db8098675ed75642e",
                "target_hash": "e53657178cb6855ac4b2029197a64b0ce6af712b69433254ae18be74d224f991",
                "exact_match": 0.0
            }
        ],
        "mmlu_llama_high_school_psychology": [
            {
                "doc_id": 0,
                "doc": {
                    "question": "Nearsightedness results from",
                    "subject": "high_school_psychology",
                    "choices": [
                        "too much curvature of the cornea and lens",
                        "too little curvature of the cornea and lens",
                        "too much curvature of the iris and lens",
                        "too little curvature of the iris and lens"
                    ],
                    "answer": 0
                },
                "target": "A.",
                "arguments": [
                    [
                        "Given the following question and four candidate answers (A, B, C and D), choose the best answer.\nQuestion: Nearsightedness results from\nA. too much curvature of the cornea and lens\nB. too little curvature of the cornea and lens\nC. too much curvature of the iris and lens\nD. too little curvature of the iris and lens\nYour response should end with \"The best answer is [the_answer_letter]\" where the [the_answer_letter] is one of A, B, C or D. The best answer is",
                        {
                            "do_sample": false,
                            "until": [
                                "."
                            ],
                            "max_gen_toks": 10
                        }
                    ]
                ],
                "resps": [
                    [
                        " zijn>\nThe method water travels through a window or almost any other kid of material is a function of the speed of light of that material and how thich the material is and its index of refraction which is a measure of how much the speed."
                    ]
                ],
                "filtered_resps": [
                    "[invalid]"
                ],
                "filter": "flexible-extract",
                "metrics": [
                    "exact_match"
                ],
                "doc_hash": "773b5cd44da6d76e9d118ff73e4b94fb52ef05bb5beba39f698f3decf4928866",
                "prompt_hash": "d79a775277f0c215389a5baf05fe185507ad379e1e93822544f8c06eb35a933e",
                "target_hash": "e53657178cb6855ac4b2029197a64b0ce6af712b69433254ae18be74d224f991",
                "exact_match": 0.0
            }
        ],
        "mmlu_llama_professional_psychology": [
            {
                "doc_id": 0,
                "doc": {
                    "question": "Which of the following statements expresses a relationship between aging and sexual functioning",
                    "subject": "professional_psychology",
                    "choices": [
                        "Aging is related to an increase in vaginal lubrication",
                        "Aging is associated with an increase in the frequency and number of orgasmic contractions in both sexes",
                        "There is no significant relationship between aging and sexual responses in either sex",
                        "Aging is related to some loss of erectile rigidity in the male"
                    ],
                    "answer": 3
                },
                "target": "D.",
                "arguments": [
                    [
                        "Given the following question and four candidate answers (A, B, C and D), choose the best answer.\nQuestion: Which of the following statements expresses a relationship between aging and sexual functioning\nA. Aging is related to an increase in vaginal lubrication\nB. Aging is associated with an increase in the frequency and number of orgasmic contractions in both sexes\nC. There is no significant relationship between aging and sexual responses in either sex\nD. Aging is related to some loss of erectile rigidity in the male\nYour response should end with \"The best answer is [the_answer_letter]\" where the [the_answer_letter] is one of A, B, C or D. The best answer is",
                        {
                            "do_sample": false,
                            "until": [
                                "."
                            ],
                            "max_gen_toks": 10
                        }
                    ]
                ],
                "resps": [
                    [
                        "Since advances in medicine have been made, more men and women are living longer, leading to more focus on maintaining sexual health with age.<think>In each of the given answers, there is an acknowledgment of sexual responses that change with aging in each gender."
                    ]
                ],
                "filtered_resps": [
                    "[invalid]"
                ],
                "filter": "flexible-extract",
                "metrics": [
                    "exact_match"
                ],
                "doc_hash": "00c7f564000de8deaac18122826ff2774f1b14e6fa09ad8677480019974d046d",
                "prompt_hash": "b23608701651b88b2a1bd2f3b5ae83d0188362cdf724c3a2a7bbbf58913649d8",
                "target_hash": "f9e487799dfc7882b1a0db047999249489d02726b2c82055cb68a6bb0311f118",
                "exact_match": 0.0
            }
        ],
        "mmlu_llama_high_school_geography": [
            {
                "doc_id": 0,
                "doc": {
                    "question": "The main factor preventing subsistence economies from advancing economically is the lack of",
                    "subject": "high_school_geography",
                    "choices": [
                        "a currency.",
                        "a well-connected transportation infrastructure.",
                        "government activity.",
                        "a banking service."
                    ],
                    "answer": 1
                },
                "target": "B.",
                "arguments": [
                    [
                        "Given the following question and four candidate answers (A, B, C and D), choose the best answer.\nQuestion: The main factor preventing subsistence economies from advancing economically is the lack of\nA. a currency.\nB. a well-connected transportation infrastructure.\nC. government activity.\nD. a banking service.\nYour response should end with \"The best answer is [the_answer_letter]\" where the [the_answer_letter] is one of A, B, C or D. The best answer is",
                        {
                            "do_sample": false,
                            "until": [
                                "."
                            ],
                            "max_gen_toks": 10
                        }
                    ]
                ],
                "resps": [
                    [
                        "One of the main disadvantages to the development of subsistence economies is their inability to participate in the larger world economy due to a deficiency in the types of services that are associated with more advanced, but not necessarily more wealthy, economies, such as having..."
                    ]
                ],
                "filtered_resps": [
                    "[invalid]"
                ],
                "filter": "flexible-extract",
                "metrics": [
                    "exact_match"
                ],
                "doc_hash": "c95b2a63d3f3df23f4e7cd63825b2727da9e0d562adbaeb9b2187025b9e1064c",
                "prompt_hash": "69a28fb24349797fcdc1abbdb84b79050eb92f0838d7e0d18d1dd1f3817cdb82",
                "target_hash": "be651d5a8e97690fbb1fafd9b89ea7bb5458b27b1d24a449c67ca762317ac1df",
                "exact_match": 0.0
            }
        ],
        "mmlu_llama_sociology": [
            {
                "doc_id": 0,
                "doc": {
                    "question": "Mass-society theory suggests that:",
                    "subject": "sociology",
                    "choices": [
                        "the content of the media is determined by market forces",
                        "the subordinate classes are dominated by the ideology of the ruling class",
                        "the media manipulate 'the masses' as vulnerable, passive consumers",
                        "audiences make selective interpretations of media messages"
                    ],
                    "answer": 2
                },
                "target": "C.",
                "arguments": [
                    [
                        "Given the following question and four candidate answers (A, B, C and D), choose the best answer.\nQuestion: Mass-society theory suggests that:\nA. the content of the media is determined by market forces\nB. the subordinate classes are dominated by the ideology of the ruling class\nC. the media manipulate 'the masses' as vulnerable, passive consumers\nD. audiences make selective interpretations of media messages\nYour response should end with \"The best answer is [the_answer_letter]\" where the [the_answer_letter] is one of A, B, C or D. The best answer is",
                        {
                            "do_sample": false,
                            "until": [
                                "."
                            ],
                            "max_gen_toks": 10
                        }
                    ]
                ],
                "resps": [
                    [
                        "Mass-society theory describes a society in which a large proportion of the population behaves in a uniform way, often in response to the perceived authority and conformity promoted by the mass media, as theorised by sociologists like Lipset and Bell.</think>."
                    ]
                ],
                "filtered_resps": [
                    "[invalid]"
                ],
                "filter": "flexible-extract",
                "metrics": [
                    "exact_match"
                ],
                "doc_hash": "d338f7e18e4ab69c8b57a5b84ea08c9d7513216c8d736d71e8c645440b74befa",
                "prompt_hash": "0afb2e0626d454383f432d53a85d0a7bd6a966d912e2f85d44b8365ab420b6fe",
                "target_hash": "acbbb708385721a6275edb28b2265071ab2c596d66c45e0f095ca22a55c6fb7d",
                "exact_match": 0.0
            }
        ],
        "mmlu_llama_security_studies": [
            {
                "doc_id": 0,
                "doc": {
                    "question": "Which of these principles is not an element of the responsibility to protect?",
                    "subject": "security_studies",
                    "choices": [
                        "The responsibility to prevent.",
                        "The responsibility to react.",
                        "The responsibility to remain sovereign.",
                        "The responsibility to rebuild."
                    ],
                    "answer": 2
                },
                "target": "C.",
                "arguments": [
                    [
                        "Given the following question and four candidate answers (A, B, C and D), choose the best answer.\nQuestion: Which of these principles is not an element of the responsibility to protect?\nA. The responsibility to prevent.\nB. The responsibility to react.\nC. The responsibility to remain sovereign.\nD. The responsibility to rebuild.\nYour response should end with \"The best answer is [the_answer_letter]\" where the [the_answer_letter] is one of A, B, C or D. The best answer is",
                        {
                            "do_sample": false,
                            "until": [
                                "."
                            ],
                            "max_gen_toks": 10
                        }
                    ]
                ],
                "resps": [
                    [
                        " The principles of the responsibility to protect (R2P) include preventing the outbreak of genocide, war crimes, ethnic cleansing, and crimes against humanity, providing early warning, and taking action to prevent crises and protect civilians from harm, as well as..."
                    ]
                ],
                "filtered_resps": [
                    "[invalid]"
                ],
                "filter": "flexible-extract",
                "metrics": [
                    "exact_match"
                ],
                "doc_hash": "4f35d00e22925a4cc50641cea84df5c47d7f4b9dacac2b138cca5b564fcff7dd",
                "prompt_hash": "7c66bc35139f2b47cd7606f711bcd3ca948174f6b6180c197f726e4011a5e2d3",
                "target_hash": "acbbb708385721a6275edb28b2265071ab2c596d66c45e0f095ca22a55c6fb7d",
                "exact_match": 0.0
            }
        ],
        "mmlu_llama_high_school_macroeconomics": [
            {
                "doc_id": 0,
                "doc": {
                    "question": "Suppose that an expansionary fiscal policy leads to a large increase in real output and a small increase in the price level. From this it can be inferred that",
                    "subject": "high_school_macroeconomics",
                    "choices": [
                        "inflation had already impacted the economy before the fiscal stimulus.",
                        "the economy initially had some unemployed resources.",
                        "aggregate supply decreased.",
                        "aggregate demand is steeply sloped."
                    ],
                    "answer": 1
                },
                "target": "B.",
                "arguments": [
                    [
                        "Given the following question and four candidate answers (A, B, C and D), choose the best answer.\nQuestion: Suppose that an expansionary fiscal policy leads to a large increase in real output and a small increase in the price level. From this it can be inferred that\nA. inflation had already impacted the economy before the fiscal stimulus.\nB. the economy initially had some unemployed resources.\nC. aggregate supply decreased.\nD. aggregate demand is steeply sloped.\nYour response should end with \"The best answer is [the_answer_letter]\" where the [the_answer_letter] is one of A, B, C or D. The best answer is",
                        {
                            "do_sample": false,
                            "until": [
                                "."
                            ],
                            "max_gen_toks": 10
                        }
                    ]
                ],
                "resps": [
                    [
                        "Given the information in the question, there are two key things to understand:\n   a) Real output increases and the price level increases, indicating that additional resources are being utilized (the economy is becoming \"fuller\" not \"lever\") as resources."
                    ]
                ],
                "filtered_resps": [
                    "[invalid]"
                ],
                "filter": "flexible-extract",
                "metrics": [
                    "exact_match"
                ],
                "doc_hash": "180ab0e3d74765f99d635bd5612508d4b73272283b5b6b679632e4828ebe0849",
                "prompt_hash": "9ec237e59d3b76fea47d2212c18aa2593b2663a411a9dc34728a81c4a95aa674",
                "target_hash": "be651d5a8e97690fbb1fafd9b89ea7bb5458b27b1d24a449c67ca762317ac1df",
                "exact_match": 0.0
            }
        ],
        "mmlu_llama_high_school_government_and_politics": [
            {
                "doc_id": 0,
                "doc": {
                    "question": "Which of the following best describes the balance the Supreme Court has struck between the establishment clause and the free-exercise clause?",
                    "subject": "high_school_government_and_politics",
                    "choices": [
                        "Freedom of speech is protected except in certain situations, such as yelling \"fire\" in a crowded theater.",
                        "Once a church has been recognized by the federal government, its tax-exempt status can never be revoked.",
                        "Once Congress has created an administrative agency, that agency can be dissolved only by a constitutional amendment.",
                        "State-sponsored prayer during school hours is prohibited, but voluntary prayer by student groups before school is allowed."
                    ],
                    "answer": 3
                },
                "target": "D.",
                "arguments": [
                    [
                        "Given the following question and four candidate answers (A, B, C and D), choose the best answer.\nQuestion: Which of the following best describes the balance the Supreme Court has struck between the establishment clause and the free-exercise clause?\nA. Freedom of speech is protected except in certain situations, such as yelling \"fire\" in a crowded theater.\nB. Once a church has been recognized by the federal government, its tax-exempt status can never be revoked.\nC. Once Congress has created an administrative agency, that agency can be dissolved only by a constitutional amendment.\nD. State-sponsored prayer during school hours is prohibited, but voluntary prayer by student groups before school is allowed.\nYour response should end with \"The best answer is [the_answer_letter]\" where the [the_answer_letter] is one of A, B, C or D. The best answer is",
                        {
                            "do_sample": false,
                            "until": [
                                "."
                            ],
                            "max_gen_toks": 10
                        }
                    ]
                ],
                "resps": [
                    [
                        " To find the best answer, let me think about what the establishment clause and the free-exercise clause of the First Amendment prohibit or require.\nThe establishment clause is often associated with the notion that government should not favor one religion over another, or privile..."
                    ]
                ],
                "filtered_resps": [
                    "[invalid]"
                ],
                "filter": "flexible-extract",
                "metrics": [
                    "exact_match"
                ],
                "doc_hash": "8f63cebd5269df80a7f6386afb6ea7266a908ffe6b72f431cf962d8dc3948358",
                "prompt_hash": "09310796c53323d00ee29824753d8e597875ddf55dda265f6638cb2b40201b42",
                "target_hash": "f9e487799dfc7882b1a0db047999249489d02726b2c82055cb68a6bb0311f118",
                "exact_match": 0.0
            }
        ],
        "mmlu_llama_public_relations": [
            {
                "doc_id": 0,
                "doc": {
                    "question": "Which common public relations tactic involves sending journalists on visits to appropriate locations?",
                    "subject": "public_relations",
                    "choices": [
                        "Media release",
                        "Media tour",
                        "Press room",
                        "Promotional days/weeks"
                    ],
                    "answer": 1
                },
                "target": "B.",
                "arguments": [
                    [
                        "Given the following question and four candidate answers (A, B, C and D), choose the best answer.\nQuestion: Which common public relations tactic involves sending journalists on visits to appropriate locations?\nA. Media release\nB. Media tour\nC. Press room\nD. Promotional days/weeks\nYour response should end with \"The best answer is [the_answer_letter]\" where the [the_answer_letter] is one of A, B, C or D. The best answer is",
                        {
                            "do_sample": false,
                            "until": [
                                "."
                            ],
                            "max_gen_toks": 10
                        }
                    ]
                ],
                "resps": [
                    [
                        "<|end_of_text|><|begin_of_text|>And in this case, given all we have studied about media tours and access to journalists, versus media releases, press rooms and promotional days/weeks, [the_answer_letter] is B.\n\nA media tour involves the proactive arrangement of visits."
                    ]
                ],
                "filtered_resps": [
                    "(B)"
                ],
                "filter": "flexible-extract",
                "metrics": [
                    "exact_match"
                ],
                "doc_hash": "af159d0c86d47e6bcc01636a22796a076e47ed2f57fa3986359073454b382e8d",
                "prompt_hash": "785ec1b466f3139bcacc07d81878ef4609c18cf566564f7cd7b32f46ac4110f7",
                "target_hash": "be651d5a8e97690fbb1fafd9b89ea7bb5458b27b1d24a449c67ca762317ac1df",
                "exact_match": 1.0
            }
        ],
        "mmlu_llama_econometrics": [
            {
                "doc_id": 0,
                "doc": {
                    "question": "Which one of the following is the most appropriate definition of a 99% confidence interval?",
                    "subject": "econometrics",
                    "choices": [
                        "99% of the time in repeated samples, the interval would contain the true value of the parameter",
                        "99% of the time in repeated samples, the interval would contain the estimated value of the parameter",
                        "99% of the time in repeated samples, the null hypothesis will be rejected",
                        "99% of the time in repeated samples, the null hypothesis will not be rejected when it was false"
                    ],
                    "answer": 0
                },
                "target": "A.",
                "arguments": [
                    [
                        "Given the following question and four candidate answers (A, B, C and D), choose the best answer.\nQuestion: Which one of the following is the most appropriate definition of a 99% confidence interval?\nA. 99% of the time in repeated samples, the interval would contain the true value of the parameter\nB. 99% of the time in repeated samples, the interval would contain the estimated value of the parameter\nC. 99% of the time in repeated samples, the null hypothesis will be rejected\nD. 99% of the time in repeated samples, the null hypothesis will not be rejected when it was false\nYour response should end with \"The best answer is [the_answer_letter]\" where the [the_answer_letter] is one of A, B, C or D. The best answer is",
                        {
                            "do_sample": false,
                            "until": [
                                "."
                            ],
                            "max_gen_toks": 10
                        }
                    ]
                ],
                "resps": [
                    [
                        "Given the reasoning below:\n\n1) The confidence interval for a parameter p is typically not based on p itself, but on the parameter estimate, which is the sample data.\n2) Confidence intervals are about the parameter, not the data or the estimate."
                    ]
                ],
                "filtered_resps": [
                    "[invalid]"
                ],
                "filter": "flexible-extract",
                "metrics": [
                    "exact_match"
                ],
                "doc_hash": "357dfe4647c49acaa336c603515412484cedbb905f084a89b34da5166121762c",
                "prompt_hash": "4f28bca1bebf344905f418c57a890d49e3fe3dc1bd0f665fdffb5c109bd5ebf3",
                "target_hash": "e53657178cb6855ac4b2029197a64b0ce6af712b69433254ae18be74d224f991",
                "exact_match": 0.0
            }
        ],
        "mmlu_llama_human_sexuality": [
            {
                "doc_id": 0,
                "doc": {
                    "question": "Which of the following methods of detecting breast cancer involves exposure to radiation?",
                    "subject": "human_sexuality",
                    "choices": [
                        "Needle aspiration",
                        "Mammography",
                        "Mastectomy",
                        "Lumpectomy"
                    ],
                    "answer": 1
                },
                "target": "B.",
                "arguments": [
                    [
                        "Given the following question and four candidate answers (A, B, C and D), choose the best answer.\nQuestion: Which of the following methods of detecting breast cancer involves exposure to radiation?\nA. Needle aspiration\nB. Mammography\nC. Mastectomy\nD. Lumpectomy\nYour response should end with \"The best answer is [the_answer_letter]\" where the [the_answer_letter] is one of A, B, C or D. The best answer is",
                        {
                            "do_sample": false,
                            "until": [
                                "."
                            ],
                            "max_gen_toks": 10
                        }
                    ]
                ],
                "resps": [
                    [
                        " Breast cancer detection methods can be broadly categorized into two types: invasive (where tissue is removed from the affected area to remove a tumor) and non-invasive (where no tissue removal is involved, and the tests examine fluid or cells that can be."
                    ]
                ],
                "filtered_resps": [
                    "[invalid]"
                ],
                "filter": "flexible-extract",
                "metrics": [
                    "exact_match"
                ],
                "doc_hash": "6126ec0bb016a434af41042194c5451091fd0c01c8e9c560ffe059373b0348cf",
                "prompt_hash": "4136e29fb6dc4ad03b12f2ed0e5dcd65499fcb577ed10afaf6a9bbdd50620ac5",
                "target_hash": "be651d5a8e97690fbb1fafd9b89ea7bb5458b27b1d24a449c67ca762317ac1df",
                "exact_match": 0.0
            }
        ],
        "mmlu_llama_us_foreign_policy": [
            {
                "doc_id": 0,
                "doc": {
                    "question": "What is the structure of the United Nations Security Council?",
                    "subject": "us_foreign_policy",
                    "choices": [
                        "5 permanent members with veto power, 10 rotating members with no veto power",
                        "5 permanent members and 10 rotating members, all with veto power",
                        "10 permanent members with veto power, and 5 rotating members without veto power",
                        "15 permanent members with veto power"
                    ],
                    "answer": 0
                },
                "target": "A.",
                "arguments": [
                    [
                        "Given the following question and four candidate answers (A, B, C and D), choose the best answer.\nQuestion: What is the structure of the United Nations Security Council?\nA. 5 permanent members with veto power, 10 rotating members with no veto power\nB. 5 permanent members and 10 rotating members, all with veto power\nC. 10 permanent members with veto power, and 5 rotating members without veto power\nD. 15 permanent members with veto power\nYour response should end with \"The best answer is [the_answer_letter]\" where the [the_answer_letter] is one of A, B, C or D. The best answer is",
                        {
                            "do_sample": false,
                            "until": [
                                "."
                            ],
                            "max_gen_toks": 10
                        }
                    ]
                ],
                "resps": [
                    [
                        " Before responding, let's analyze the question and consider the options.\nThe structure of the United Nations Security Council is an important topic.\nNow let's evaluate each answer.\n\nA says that there are 5 permanent members with veto power and 10 rotating."
                    ]
                ],
                "filtered_resps": [
                    "[invalid]"
                ],
                "filter": "flexible-extract",
                "metrics": [
                    "exact_match"
                ],
                "doc_hash": "241028e4c5a6e13dff747e7cb5da93945c40a4f84932adba28ffe9d6e7bb51e7",
                "prompt_hash": "ecdadcdb588080cb98a35f484e97296292809a1f4373282882d5922fb0bf0216",
                "target_hash": "e53657178cb6855ac4b2029197a64b0ce6af712b69433254ae18be74d224f991",
                "exact_match": 0.0
            }
        ],
        "mmlu_llama_high_school_microeconomics": [
            {
                "doc_id": 0,
                "doc": {
                    "question": "Marginal revenue equals marginal cost at the point where",
                    "subject": "high_school_microeconomics",
                    "choices": [
                        "total revenue is greater than total cost at its greatest distance",
                        "total revenue is equal to total cost",
                        "marginal product is at its highest point",
                        "total product is at its highest point"
                    ],
                    "answer": 0
                },
                "target": "A.",
                "arguments": [
                    [
                        "Given the following question and four candidate answers (A, B, C and D), choose the best answer.\nQuestion: Marginal revenue equals marginal cost at the point where\nA. total revenue is greater than total cost at its greatest distance\nB. total revenue is equal to total cost\nC. marginal product is at its highest point\nD. total product is at its highest point\nYour response should end with \"The best answer is [the_answer_letter]\" where the [the_answer_letter] is one of A, B, C or D. The best answer is",
                        {
                            "do_sample": false,
                            "until": [
                                "."
                            ],
                            "max_gen_toks": 10
                        }
                    ]
                ],
                "resps": [
                    [
                        " Let's solve this step by step.\n\nFirst, we must define what each part of the terms mean.\n\n- Total revenue is the revenues made from selling a good or service.\n- Total cost is the cost of producing the good or service.\n-...."
                    ]
                ],
                "filtered_resps": [
                    "[invalid]"
                ],
                "filter": "flexible-extract",
                "metrics": [
                    "exact_match"
                ],
                "doc_hash": "17497179e0743e10f40e98f348e9adbf3ee674146cd25dc16947b68e228002e7",
                "prompt_hash": "68397242c322379afa0c7b5b96553b3617dfdcf0ee3fec221b2681097b762296",
                "target_hash": "e53657178cb6855ac4b2029197a64b0ce6af712b69433254ae18be74d224f991",
                "exact_match": 0.0
            }
        ],
        "mmlu_llama_logical_fallacies": [
            {
                "doc_id": 0,
                "doc": {
                    "question": "A valid disjunctive syllogism has a major premise that:",
                    "subject": "logical_fallacies",
                    "choices": [
                        "includes two or more alternatives",
                        "classifies subjects",
                        "affirms the antecedent or denies the consequent",
                        "leads to a valid conclusion"
                    ],
                    "answer": 0
                },
                "target": "A.",
                "arguments": [
                    [
                        "Given the following question and four candidate answers (A, B, C and D), choose the best answer.\nQuestion: A valid disjunctive syllogism has a major premise that:\nA. includes two or more alternatives\nB. classifies subjects\nC. affirms the antecedent or denies the consequent\nD. leads to a valid conclusion\nYour response should end with \"The best answer is [the_answer_letter]\" where the [the_answer_letter] is one of A, B, C or D. The best answer is",
                        {
                            "do_sample": false,
                            "until": [
                                "."
                            ],
                            "max_gen_toks": 10
                        }
                    ]
                ],
                "resps": [
                    [
                        "Based on the definition of a valid disjunctive syllogism, a disjunctive argument offers two or more mutually exclusive propositions connected with \"or.\" The major premise provides two or more alternatives that one wants to be either true."
                    ]
                ],
                "filtered_resps": [
                    "[invalid]"
                ],
                "filter": "flexible-extract",
                "metrics": [
                    "exact_match"
                ],
                "doc_hash": "4aa22528fa899250ee3ae9f64ca73e9e5cb8283c880325df6bfbf0f71de7066b",
                "prompt_hash": "1b1a0f164836d60fcdb36fe8ea64258132563d843a28bf7cdf06e2b4cc38b489",
                "target_hash": "e53657178cb6855ac4b2029197a64b0ce6af712b69433254ae18be74d224f991",
                "exact_match": 0.0
            }
        ],
        "mmlu_llama_professional_law": [
            {
                "doc_id": 0,
                "doc": {
                    "question": "One afternoon, a pilot was flying a small airplane when it suddenly ran out of gas. As he was coming in for an emergency landing, the plane crossed into a neighboring state at a very low altitude. At this time, a 9-year-old boy was walking to school when he was struck and injured by an object, which may have fallen from the plane. In federal court, a negligence suit was brought against the pilot by the father of the boy for his son. Accompanied by his father, the boy had visited an attorney for preliminary discussions regarding the case. However, the father did not retain the attorney to represent his son in the lawsuit. Instead, the father hired another lawyer to handle the case. At trial, the pilot's attorney calls the consulting attorney to testify what the boy had said to him regarding his physical condition during the consultation that the attorney had had with the boy and his father. The attorney's testimony is",
                    "subject": "professional_law",
                    "choices": [
                        "admissible, because the attorney-client privilege was waived by the filing of the lawsuit.",
                        "admissible, because there is no privilege of confidentiality when a person other than the client is present at the attorney-client consultation.",
                        "inadmissible, because the attorney-client privilege prevents such a breach of confidential communications.",
                        "inadmissible, because it was a statement of physical condition not made for the purpose of obtaining medical treatment."
                    ],
                    "answer": 2
                },
                "target": "C.",
                "arguments": [
                    [
                        "Given the following question and four candidate answers (A, B, C and D), choose the best answer.\nQuestion: One afternoon, a pilot was flying a small airplane when it suddenly ran out of gas. As he was coming in for an emergency landing, the plane crossed into a neighboring state at a very low altitude. At this time, a 9-year-old boy was walking to school when he was struck and injured by an object, which may have fallen from the plane. In federal court, a negligence suit was brought against the pilot by the father of the boy for his son. Accompanied by his father, the boy had visited an attorney for preliminary discussions regarding the case. However, the father did not retain the attorney to represent his son in the lawsuit. Instead, the father hired another lawyer to handle the case. At trial, the pilot's attorney calls the consulting attorney to testify what the boy had said to him regarding his physical condition during the consultation that the attorney had had with the boy and his father. The attorney's testimony is\nA. admissible, because the attorney-client privilege was waived by the filing of the lawsuit.\nB. admissible, because there is no privilege of confidentiality when a person other than the client is present at the attorney-client consultation.\nC. inadmissible, because the attorney-client privilege prevents such a breach of confidential communications.\nD. inadmissible, because it was a statement of physical condition not made for the purpose of obtaining medical treatment.\nYour response should end with \"The best answer is [the_answer_letter]\" where the [the_answer_letter] is one of A, B, C or D. The best answer is",
                        {
                            "do_sample": false,
                            "until": [
                                "."
                            ],
                            "max_gen_toks": 10
                        }
                    ]
                ],
                "resps": [
                    [
                        "First, I think that an attorney-client privilege exists for communications during preliminary discussions, at least for the limited purpose of presenting a claim or defense in a lawsuit so that the communications' confidentiality would not prevent the claim or defense from being presented in court."
                    ]
                ],
                "filtered_resps": [
                    "[invalid]"
                ],
                "filter": "flexible-extract",
                "metrics": [
                    "exact_match"
                ],
                "doc_hash": "66db9fac238dccc6260e200eeffa2056502b5bdec0c07a2ce821702b68e59880",
                "prompt_hash": "ae7ddbcaa6eb2c561fb04d361421d34458c1e14fea57ad882da93f328f7a645c",
                "target_hash": "acbbb708385721a6275edb28b2265071ab2c596d66c45e0f095ca22a55c6fb7d",
                "exact_match": 0.0
            }
        ],
        "mmlu_llama_jurisprudence": [
            {
                "doc_id": 0,
                "doc": {
                    "question": "Which statement best explains the purpose of Hart's distinction between 'being obliged' and 'having an obligation'?",
                    "subject": "jurisprudence",
                    "choices": [
                        "It demonstrates the difference between the internal and the external aspect of a rule.",
                        "It refutes the natural lawyer' view of the role of morality in law.",
                        "It explains the nature of power-conferring rules.",
                        "It illuminates the concept of a rule."
                    ],
                    "answer": 3
                },
                "target": "D.",
                "arguments": [
                    [
                        "Given the following question and four candidate answers (A, B, C and D), choose the best answer.\nQuestion: Which statement best explains the purpose of Hart's distinction between 'being obliged' and 'having an obligation'?\nA. It demonstrates the difference between the internal and the external aspect of a rule.\nB. It refutes the natural lawyer' view of the role of morality in law.\nC. It explains the nature of power-conferring rules.\nD. It illuminates the concept of a rule.\nYour response should end with \"The best answer is [the_answer_letter]\" where the [the_answer_letter] is one of A, B, C or D. The best answer is",
                        {
                            "do_sample": false,
                            "until": [
                                "."
                            ],
                            "max_gen_toks": 10
                        }
                    ]
                ],
                "resps": [
                    [
                        "When considering the nature and role of rules in society, some of the most fascinating discussions are linked to the underlying concepts linked to the legitimacy of these rules and the factors that create and constrain them, particularly in the context of the natural lawyer discourse,...."
                    ]
                ],
                "filtered_resps": [
                    "[invalid]"
                ],
                "filter": "flexible-extract",
                "metrics": [
                    "exact_match"
                ],
                "doc_hash": "e72a21aab150f89abca2f82e0d61c211a78cf4981fb287e9a5684787aa0f3008",
                "prompt_hash": "a07c415ecb2c036bbfd08f26455633806a7319295fc66f22003848b6f1a28c5c",
                "target_hash": "f9e487799dfc7882b1a0db047999249489d02726b2c82055cb68a6bb0311f118",
                "exact_match": 0.0
            }
        ],
        "mmlu_llama_high_school_world_history": [
            {
                "doc_id": 0,
                "doc": {
                    "question": "This question refers to the following information.\nNo task is more urgent than that of preserving peace. Without peace our independence means little. The rehabilitation and upbuilding of our countries will have little meaning. Our revolutions will not be allowed to run their course. What can we do? We can do much! We can inject the voice of reason into world affairs. We can mobilize all the spiritual, all the moral, all the political strength of Asia and Africa on the side of peace. Yes, we! We, the peoples of Asia and Africa, 1.4 billion strong.\nIndonesian leader Sukarno, keynote address to the Bandung Conference, 1955\nThe passage above is most associated with which of the following developments?",
                    "subject": "high_school_world_history",
                    "choices": [
                        "The formation of the non-aligned movement",
                        "Global disarmanent and nuclear non-proliferation",
                        "The Green Revolution in agriculture",
                        "Mobilization of pan-Asian ideology"
                    ],
                    "answer": 0
                },
                "target": "A.",
                "arguments": [
                    [
                        "Given the following question and four candidate answers (A, B, C and D), choose the best answer.\nQuestion: This question refers to the following information.\nNo task is more urgent than that of preserving peace. Without peace our independence means little. The rehabilitation and upbuilding of our countries will have little meaning. Our revolutions will not be allowed to run their course. What can we do? We can do much! We can inject the voice of reason into world affairs. We can mobilize all the spiritual, all the moral, all the political strength of Asia and Africa on the side of peace. Yes, we! We, the peoples of Asia and Africa, 1.4 billion strong.\nIndonesian leader Sukarno, keynote address to the Bandung Conference, 1955\nThe passage above is most associated with which of the following developments?\nA. The formation of the non-aligned movement\nB. Global disarmanent and nuclear non-proliferation\nC. The Green Revolution in agriculture\nD. Mobilization of pan-Asian ideology\nYour response should end with \"The best answer is [the_answer_letter]\" where the [the_answer_letter] is one of A, B, C or D. The best answer is",
                        {
                            "do_sample": false,
                            "until": [
                                "."
                            ],
                            "max_gen_toks": 10
                        }
                    ]
                ],
                "resps": [
                    [
                        "Ok, so what sort of answer is being asked for?\nA quiz question about world history that requires me to identify the historical development mentioned by the Indonesian leader Sukarno in his 1955 speech to the Bandung Conference.\nWhat key event."
                    ]
                ],
                "filtered_resps": [
                    "[invalid]"
                ],
                "filter": "flexible-extract",
                "metrics": [
                    "exact_match"
                ],
                "doc_hash": "e9a195eedaa3de38d3eb26188a3354be9b5c96d0466a818cb1dfb6f8681866e9",
                "prompt_hash": "f0625645c74829b585d167e11f40187a5c118c4773bb6ef03c8803df396a1308",
                "target_hash": "e53657178cb6855ac4b2029197a64b0ce6af712b69433254ae18be74d224f991",
                "exact_match": 0.0
            }
        ],
        "mmlu_llama_high_school_european_history": [
            {
                "doc_id": 0,
                "doc": {
                    "question": "This question refers to the following information.\nIn order to make the title of this discourse generally intelligible, I have translated the term \"Protoplasm,\" which is the scientific name of the substance of which I am about to speak, by the words \"the physical basis of life.\" I suppose that, to many, the idea that there is such a thing as a physical basis, or matter, of life may be novel\u2014so widely spread is the conception of life as something which works through matter. \u2026 Thus the matter of life, so far as we know it (and we have no right to speculate on any other), breaks up, in consequence of that continual death which is the condition of its manifesting vitality, into carbonic acid, water, and nitrogenous compounds, which certainly possess no properties but those of ordinary matter.\nThomas Henry Huxley, \"The Physical Basis of Life,\" 1868\nFrom the passage, one may infer that Huxley argued that \"life\" was",
                    "subject": "high_school_european_history",
                    "choices": [
                        "a force that works through matter",
                        "essentially a philosophical notion",
                        "merely a property of a certain kind of matter",
                        "a supernatural phenomenon"
                    ],
                    "answer": 2
                },
                "target": "C.",
                "arguments": [
                    [
                        "Given the following question and four candidate answers (A, B, C and D), choose the best answer.\nQuestion: This question refers to the following information.\nIn order to make the title of this discourse generally intelligible, I have translated the term \"Protoplasm,\" which is the scientific name of the substance of which I am about to speak, by the words \"the physical basis of life.\" I suppose that, to many, the idea that there is such a thing as a physical basis, or matter, of life may be novel\u2014so widely spread is the conception of life as something which works through matter. \u2026 Thus the matter of life, so far as we know it (and we have no right to speculate on any other), breaks up, in consequence of that continual death which is the condition of its manifesting vitality, into carbonic acid, water, and nitrogenous compounds, which certainly possess no properties but those of ordinary matter.\nThomas Henry Huxley, \"The Physical Basis of Life,\" 1868\nFrom the passage, one may infer that Huxley argued that \"life\" was\nA. a force that works through matter\nB. essentially a philosophical notion\nC. merely a property of a certain kind of matter\nD. a supernatural phenomenon\nYour response should end with \"The best answer is [the_answer_letter]\" where the [the_answer_letter] is one of A, B, C or D. The best answer is",
                        {
                            "do_sample": false,
                            "until": [
                                "."
                            ],
                            "max_gen_toks": 10
                        }
                    ]
                ],
                "resps": [
                    [
                        " The passage talks about the matter (or the \"physical basis\") of life as being made of carbonic acid, water, and nitrogenous compounds which can break up into these basic components, represented by the letters above the equals sign, and the?."
                    ]
                ],
                "filtered_resps": [
                    "[invalid]"
                ],
                "filter": "flexible-extract",
                "metrics": [
                    "exact_match"
                ],
                "doc_hash": "edb17efe8a9eb3b42ea14211e40706a4567156900debf4a6585bf5d6e1907aa9",
                "prompt_hash": "244b65e5c8e6a11213fc70b0e6e8c99672e75e6598aff70951011465c1893c18",
                "target_hash": "acbbb708385721a6275edb28b2265071ab2c596d66c45e0f095ca22a55c6fb7d",
                "exact_match": 0.0
            }
        ],
        "mmlu_llama_high_school_us_history": [
            {
                "doc_id": 0,
                "doc": {
                    "question": "This question refers to the following information.\n\"The far-reaching, the boundless future will be the era of American greatness. In its magnificent domain of space and time, the nation of many nations is destined to manifest to mankind the excellence of divine principles; to establish on earth the noblest temple ever dedicated to the worship of the Most High\u2014the Sacred and the True. Its floor shall be a hemisphere\u2014its roof the firmament of the star-studded heavens, and its congregation a Union of many Republics, comprising hundreds of happy millions, calling, owning no man master, but governed by God's natural and moral law of equality, the law of brotherhood\u2014of 'peace and good will amongst men.'\"\nJohn L. O'Sullivan, \"The Great Nation of Futurity,\" 1839\nBy what means did the United States take possession of the Oregon Territory?",
                    "subject": "high_school_us_history",
                    "choices": [
                        "The United States was granted the territory in a postwar treaty with France.",
                        "The United States bought it from the Native Americans who lived there.",
                        "U.S. settlers were the first to arrive in the region; they claimed it for their country.",
                        "Great Britain ceded it to the United States as part of a negotiated treaty."
                    ],
                    "answer": 3
                },
                "target": "D.",
                "arguments": [
                    [
                        "Given the following question and four candidate answers (A, B, C and D), choose the best answer.\nQuestion: This question refers to the following information.\n\"The far-reaching, the boundless future will be the era of American greatness. In its magnificent domain of space and time, the nation of many nations is destined to manifest to mankind the excellence of divine principles; to establish on earth the noblest temple ever dedicated to the worship of the Most High\u2014the Sacred and the True. Its floor shall be a hemisphere\u2014its roof the firmament of the star-studded heavens, and its congregation a Union of many Republics, comprising hundreds of happy millions, calling, owning no man master, but governed by God's natural and moral law of equality, the law of brotherhood\u2014of 'peace and good will amongst men.'\"\nJohn L. O'Sullivan, \"The Great Nation of Futurity,\" 1839\nBy what means did the United States take possession of the Oregon Territory?\nA. The United States was granted the territory in a postwar treaty with France.\nB. The United States bought it from the Native Americans who lived there.\nC. U.S. settlers were the first to arrive in the region; they claimed it for their country.\nD. Great Britain ceded it to the United States as part of a negotiated treaty.\nYour response should end with \"The best answer is [the_answer_letter]\" where the [the_answer_letter] is one of A, B, C or D. The best answer is",
                        {
                            "do_sample": false,
                            "until": [
                                "."
                            ],
                            "max_gen_toks": 10
                        }
                    ]
                ],
                "resps": [
                    [
                        "Upon examining the passage provided, one thing strikes me as important to know about the situation in Oregon Territory before the U.S acquired it: Great Britain, France, the Kingdom of K SPDVPNY, SPTHPR IL, VHH,."
                    ]
                ],
                "filtered_resps": [
                    "[invalid]"
                ],
                "filter": "flexible-extract",
                "metrics": [
                    "exact_match"
                ],
                "doc_hash": "c8b43803476bb9486d213e4399411215ecc6849b94074361a025d3a22f58e5dd",
                "prompt_hash": "77672c2771c7d4ee83d7a2c84efaf50e12b34f3886d47c0f59ea5839c3183e83",
                "target_hash": "f9e487799dfc7882b1a0db047999249489d02726b2c82055cb68a6bb0311f118",
                "exact_match": 0.0
            }
        ],
        "mmlu_llama_formal_logic": [
            {
                "doc_id": 0,
                "doc": {
                    "question": "Identify the conclusion of the following argument. It is hard not to verify in our peers the same weakened intelligence due to emotions that we observe in our everyday patients. The arrogance of our consciousness, which in general, belongs to the strongest defense mechanisms, blocks the unconscious complexes. Because of this, it is difficult to convince people of the unconscious, and in turn to teach them what their conscious knowledge contradicts. (Sigmund Freud, The Origin and Development of Psychoanalysis)",
                    "subject": "formal_logic",
                    "choices": [
                        "It is hard not to verify in our peers the same weakened intelligence due to emotions that we observe in our everyday patients.",
                        "The arrogance of our consciousness, which in general, belongs to the strongest defense mechanisms, blocks the unconscious complexes.",
                        "Because of this, it is difficult to convince people of the unconscious, and in turn to teach them what their conscious knowledge contradicts.",
                        "It is difficult to convince people of the unconscious, and in turn to teach them what their conscious knowledge contradicts."
                    ],
                    "answer": 3
                },
                "target": "D.",
                "arguments": [
                    [
                        "Given the following question and four candidate answers (A, B, C and D), choose the best answer.\nQuestion: Identify the conclusion of the following argument. It is hard not to verify in our peers the same weakened intelligence due to emotions that we observe in our everyday patients. The arrogance of our consciousness, which in general, belongs to the strongest defense mechanisms, blocks the unconscious complexes. Because of this, it is difficult to convince people of the unconscious, and in turn to teach them what their conscious knowledge contradicts. (Sigmund Freud, The Origin and Development of Psychoanalysis)\nA. It is hard not to verify in our peers the same weakened intelligence due to emotions that we observe in our everyday patients.\nB. The arrogance of our consciousness, which in general, belongs to the strongest defense mechanisms, blocks the unconscious complexes.\nC. Because of this, it is difficult to convince people of the unconscious, and in turn to teach them what their conscious knowledge contradicts.\nD. It is difficult to convince people of the unconscious, and in turn to teach them what their conscious knowledge contradicts.\nYour response should end with \"The best answer is [the_answer_letter]\" where the [the_answer_letter] is one of A, B, C or D. The best answer is",
                        {
                            "do_sample": false,
                            "until": [
                                "."
                            ],
                            "max_gen_toks": 10
                        }
                    ]
                ],
                "resps": [
                    [
                        "So let me think about this argument...</think>\nAlright, let's analyze the argument:\n\nThe argument starts by discussing the \"arrogance of our consciousness, which in general, belongs to the strongest defense mechanisms, blocks the unconscious complexes.\" Then \"..."
                    ]
                ],
                "filtered_resps": [
                    "(B)"
                ],
                "filter": "flexible-extract",
                "metrics": [
                    "exact_match"
                ],
                "doc_hash": "70fc61240c8f9f1e3cf2bc894bc80c6f6e1e3e26ca2e9dbd99bea1e17dd6e9c3",
                "prompt_hash": "bb17a9cb430873917cf8864af21b43be376ecd59961d03a250a3248ab3d64862",
                "target_hash": "f9e487799dfc7882b1a0db047999249489d02726b2c82055cb68a6bb0311f118",
                "exact_match": 0.0
            }
        ],
        "mmlu_llama_international_law": [
            {
                "doc_id": 0,
                "doc": {
                    "question": "Which State ordinarily exercises jurisdiction in respect of crimes committed on board vessels?",
                    "subject": "international_law",
                    "choices": [
                        "The coastal State",
                        "The flag State",
                        "All States enjoy such jurisdiction",
                        "The International Tribunal for the Law of the Sea"
                    ],
                    "answer": 1
                },
                "target": "B.",
                "arguments": [
                    [
                        "Given the following question and four candidate answers (A, B, C and D), choose the best answer.\nQuestion: Which State ordinarily exercises jurisdiction in respect of crimes committed on board vessels?\nA. The coastal State\nB. The flag State\nC. All States enjoy such jurisdiction\nD. The International Tribunal for the Law of the Sea\nYour response should end with \"The best answer is [the_answer_letter]\" where the [the_answer_letter] is one of A, B, C or D. The best answer is",
                        {
                            "do_sample": false,
                            "until": [
                                "."
                            ],
                            "max_gen_toks": 10
                        }
                    ]
                ],
                "resps": [
                    [
                        " [The question is about which state has jurisdiction over crimes committed on board a vessel, but if the ship sails on international waters, there should be no territorial ties to any specific state.]\n\n=D  The best answer is B<think> As vessels?."
                    ]
                ],
                "filtered_resps": [
                    "B"
                ],
                "filter": "flexible-extract",
                "metrics": [
                    "exact_match"
                ],
                "doc_hash": "65bbff09ea68bfbd8cf11d8fa51b3e61c8d2029a5f4100118c9056231795fbd3",
                "prompt_hash": "d9ec6a43c1d06105b9018e96936cac3d8ef6925798ff672dd6ec4f8fe16ddc50",
                "target_hash": "be651d5a8e97690fbb1fafd9b89ea7bb5458b27b1d24a449c67ca762317ac1df",
                "exact_match": 1.0
            }
        ],
        "mmlu_llama_moral_disputes": [
            {
                "doc_id": 0,
                "doc": {
                    "question": " Just war theory's principle of military necessity belongs to",
                    "subject": "moral_disputes",
                    "choices": [
                        "jus in bello.",
                        "jus ad bellum.",
                        "moral nihilism.",
                        "all of the above"
                    ],
                    "answer": 0
                },
                "target": "A.",
                "arguments": [
                    [
                        "Given the following question and four candidate answers (A, B, C and D), choose the best answer.\nQuestion: Just war theory's principle of military necessity belongs to\nA. jus in bello.\nB. jus ad bellum.\nC. moral nihilism.\nD. all of the above\nYour response should end with \"The best answer is [the_answer_letter]\" where the [the_answer_letter] is one of A, B, C or D. The best answer is",
                        {
                            "do_sample": false,
                            "until": [
                                "."
                            ],
                            "max_gen_toks": 10
                        }
                    ]
                ],
                "resps": [
                    [
                        "Just war theory refers to a set of principles that are used to assess and (successfully) fight wars, while they also include guidance on how to behave once the conflict actually commences (jus in bello) plus rules on whether to start..."
                    ]
                ],
                "filtered_resps": [
                    "(A)"
                ],
                "filter": "flexible-extract",
                "metrics": [
                    "exact_match"
                ],
                "doc_hash": "bb0de79f79411c47783968714ec9fe3c69d89753e22c88f044420a7e00049a15",
                "prompt_hash": "c3d3a2ce753659cc6898831f518c6a32b65a67616c17197219005335297d7c3a",
                "target_hash": "e53657178cb6855ac4b2029197a64b0ce6af712b69433254ae18be74d224f991",
                "exact_match": 1.0
            }
        ],
        "mmlu_llama_world_religions": [
            {
                "doc_id": 0,
                "doc": {
                    "question": " What can murtis be translated as?",
                    "subject": "world_religions",
                    "choices": [
                        "Offerings",
                        "Prayers",
                        "Apparitions",
                        "Idols"
                    ],
                    "answer": 3
                },
                "target": "D.",
                "arguments": [
                    [
                        "Given the following question and four candidate answers (A, B, C and D), choose the best answer.\nQuestion: What can murtis be translated as?\nA. Offerings\nB. Prayers\nC. Apparitions\nD. Idols\nYour response should end with \"The best answer is [the_answer_letter]\" where the [the_answer_letter] is one of A, B, C or D. The best answer is",
                        {
                            "do_sample": false,
                            "until": [
                                "."
                            ],
                            "max_gen_toks": 10
                        }
                    ]
                ],
                "resps": [
                    [
                        " When we talk about gods in Hinduism, the subject at hand is generally understood to be their material embodiments, or \"murtis.\" These are the deities we engage with in our daily rituals and prayers - so translations that relate to these."
                    ]
                ],
                "filtered_resps": [
                    "(B)"
                ],
                "filter": "flexible-extract",
                "metrics": [
                    "exact_match"
                ],
                "doc_hash": "fc6a64f69b855c4972cf1a312707b1a35c571ad689af65f645f4b59cac2cb67d",
                "prompt_hash": "6358ec95a140e59266826c7096c0855a9c7020d5334b9c896b14b73b151a6365",
                "target_hash": "f9e487799dfc7882b1a0db047999249489d02726b2c82055cb68a6bb0311f118",
                "exact_match": 0.0
            }
        ],
        "mmlu_llama_prehistory": [
            {
                "doc_id": 0,
                "doc": {
                    "question": "Unlike most other early civilizations, Minoan culture shows little evidence of:",
                    "subject": "prehistory",
                    "choices": [
                        "trade.",
                        "warfare.",
                        "the development of a common religion.",
                        "conspicuous consumption by elites."
                    ],
                    "answer": 3
                },
                "target": "D.",
                "arguments": [
                    [
                        "Given the following question and four candidate answers (A, B, C and D), choose the best answer.\nQuestion: Unlike most other early civilizations, Minoan culture shows little evidence of:\nA. trade.\nB. warfare.\nC. the development of a common religion.\nD. conspicuous consumption by elites.\nYour response should end with \"The best answer is [the_answer_letter]\" where the [the_answer_letter] is one of A, B, C or D. The best answer is",
                        {
                            "do_sample": false,
                            "until": [
                                "."
                            ],
                            "max_gen_toks": 10
                        }
                    ]
                ],
                "resps": [
                    [
                        " The Minoans were an early civilization often best known for their extensive use of the sea and mastaba (or substructure) design for their buildings, as well as being the culture associated with the mythical figure of King Minos of Crete."
                    ]
                ],
                "filtered_resps": [
                    "[invalid]"
                ],
                "filter": "flexible-extract",
                "metrics": [
                    "exact_match"
                ],
                "doc_hash": "7e83262b987d96d37c931d9178c5b235025cc570a20a2c68fcc935623d2e2bfa",
                "prompt_hash": "4463da76a3ded3957bd7c73632eaefa864b7fc06882f282c5c458e9c01760a72",
                "target_hash": "f9e487799dfc7882b1a0db047999249489d02726b2c82055cb68a6bb0311f118",
                "exact_match": 0.0
            }
        ],
        "mmlu_llama_moral_scenarios": [
            {
                "doc_id": 0,
                "doc": {
                    "question": "For which of these two scenarios does the main character (who uses I/me/my) do something clearly morally wrong, according to ordinary moral standards in the US as of 2020? Scenario 1 | I slammed on my breaks to miss the ball as it came in to the road. Scenario 2 | I taught my children to play the xylophone.",
                    "subject": "moral_scenarios",
                    "choices": [
                        "Wrong, Wrong",
                        "Wrong, Not wrong",
                        "Not wrong, Wrong",
                        "Not wrong, Not wrong"
                    ],
                    "answer": 3
                },
                "target": "D.",
                "arguments": [
                    [
                        "Given the following question and four candidate answers (A, B, C and D), choose the best answer.\nQuestion: For which of these two scenarios does the main character (who uses I/me/my) do something clearly morally wrong, according to ordinary moral standards in the US as of 2020? Scenario 1 | I slammed on my breaks to miss the ball as it came in to the road. Scenario 2 | I taught my children to play the xylophone.\nA. Wrong, Wrong\nB. Wrong, Not wrong\nC. Not wrong, Wrong\nD. Not wrong, Not wrong\nYour response should end with \"The best answer is [the_answer_letter]\" where the [the_answer_letter] is one of A, B, C or D. The best answer is",
                        {
                            "do_sample": false,
                            "until": [
                                "."
                            ],
                            "max_gen_toks": 10
                        }
                    ]
                ],
                "resps": [
                    [
                        "Each scenario involves the main character making a choice or action with moral implications.Testing answers A and B.Scenario 1 involves the main character being careless or inconsiderate of others' safety by swerving to avoid something and potentially endangering others."
                    ]
                ],
                "filtered_resps": [
                    "[invalid]"
                ],
                "filter": "flexible-extract",
                "metrics": [
                    "exact_match"
                ],
                "doc_hash": "372bb5bf3e60950571f16d14f818f1d1a88f4fabe92f88aeb1d5e60b95fb9fb9",
                "prompt_hash": "3357596cb66b672ab71777586ee17fd891114f3b035caeb4c3b60ab191aa6d8d",
                "target_hash": "f9e487799dfc7882b1a0db047999249489d02726b2c82055cb68a6bb0311f118",
                "exact_match": 0.0
            }
        ],
        "mmlu_llama_philosophy": [
            {
                "doc_id": 0,
                "doc": {
                    "question": "Aesthetics deals with objects that are_____.",
                    "subject": "philosophy",
                    "choices": [
                        "essential to our existence",
                        "unimportant to most people",
                        "not essential to our existence",
                        "rarely viewed"
                    ],
                    "answer": 2
                },
                "target": "C.",
                "arguments": [
                    [
                        "Given the following question and four candidate answers (A, B, C and D), choose the best answer.\nQuestion: Aesthetics deals with objects that are_____.\nA. essential to our existence\nB. unimportant to most people\nC. not essential to our existence\nD. rarely viewed\nYour response should end with \"The best answer is [the_answer_letter]\" where the [the_answer_letter] is one of A, B, C or D. The best answer is",
                        {
                            "do_sample": false,
                            "until": [
                                "."
                            ],
                            "max_gen_toks": 10
                        }
                    ]
                ],
                "resps": [
                    [
                        " Aesthetic refers to the study or appreciation of beauty: this is a field that focuses on values and qualities of art and beauty.<more_thinking>A is incorrect because it implies a possibly immaterial aspect of existence, while aesthetics is more about values."
                    ]
                ],
                "filtered_resps": [
                    "[invalid]"
                ],
                "filter": "flexible-extract",
                "metrics": [
                    "exact_match"
                ],
                "doc_hash": "e5a3fa0556b558380411e265f56b675f0b4e06bda9763c84666f67268cb9012b",
                "prompt_hash": "b9be3e7c3308c6cf8d856e16fc4718734cb799df2fa3eb55fd2b353ce4e6a0b4",
                "target_hash": "acbbb708385721a6275edb28b2265071ab2c596d66c45e0f095ca22a55c6fb7d",
                "exact_match": 0.0
            }
        ]
    },
    "config": {
        "model": "HFPPLInferenceModel",
        "model_args": null,
        "batch_size": 1,
        "batch_sizes": [],
        "device": null,
        "use_cache": null,
        "limit": 1,
        "bootstrap_iters": 100000,
        "gen_kwargs": null,
        "random_seed": 0,
        "numpy_seed": 1234,
        "torch_seed": 1234,
        "fewshot_seed": 1234
    },
    "git_hash": "d5570f7",
    "date": 1746657122.841267,
    "pretty_env_info": "PyTorch version: 2.6.0+cu124\nIs debug build: False\nCUDA used to build PyTorch: 12.4\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 22.04.4 LTS (x86_64)\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\nClang version: Could not collect\nCMake version: Could not collect\nLibc version: glibc-2.35\n\nPython version: 3.12.1 | packaged by Anaconda, Inc. | (main, Jan 19 2024, 15:51:05) [GCC 11.2.0] (64-bit runtime)\nPython platform: Linux-6.8.0-57-generic-x86_64-with-glibc2.35\nIs CUDA available: True\nCUDA runtime version: 12.5.40\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: \nGPU 0: NVIDIA GeForce RTX 3080\nGPU 1: NVIDIA GeForce RTX 3080\n\nNvidia driver version: 555.42.06\ncuDNN version: Could not collect\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                         x86_64\nCPU op-mode(s):                       32-bit, 64-bit\nAddress sizes:                        39 bits physical, 48 bits virtual\nByte Order:                           Little Endian\nCPU(s):                               8\nOn-line CPU(s) list:                  0-7\nVendor ID:                            GenuineIntel\nModel name:                           Intel(R) Core(TM) i7-7700K CPU @ 4.20GHz\nCPU family:                           6\nModel:                                158\nThread(s) per core:                   2\nCore(s) per socket:                   4\nSocket(s):                            1\nStepping:                             9\nCPU max MHz:                          4500.0000\nCPU min MHz:                          800.0000\nBogoMIPS:                             8400.00\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb pti ssbd ibrs ibpb stibp tpr_shadow flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid mpx rdseed adx smap clflushopt intel_pt xsaveopt xsavec xgetbv1 xsaves dtherm ida arat pln pts hwp hwp_notify hwp_act_window hwp_epp vnmi md_clear flush_l1d arch_capabilities\nVirtualization:                       VT-x\nL1d cache:                            128 KiB (4 instances)\nL1i cache:                            128 KiB (4 instances)\nL2 cache:                             1 MiB (4 instances)\nL3 cache:                             8 MiB (1 instance)\nNUMA node(s):                         1\nNUMA node0 CPU(s):                    0-7\nVulnerability Gather data sampling:   Mitigation; Microcode\nVulnerability Itlb multihit:          KVM: Mitigation: VMX disabled\nVulnerability L1tf:                   Mitigation; PTE Inversion; VMX conditional cache flushes, SMT vulnerable\nVulnerability Mds:                    Mitigation; Clear CPU buffers; SMT vulnerable\nVulnerability Meltdown:               Mitigation; PTI\nVulnerability Mmio stale data:        Mitigation; Clear CPU buffers; SMT vulnerable\nVulnerability Reg file data sampling: Not affected\nVulnerability Retbleed:               Mitigation; IBRS\nVulnerability Spec rstack overflow:   Not affected\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:             Mitigation; IBRS; IBPB conditional; STIBP conditional; RSB filling; PBRSB-eIBRS Not affected; BHI Not affected\nVulnerability Srbds:                  Mitigation; Microcode\nVulnerability Tsx async abort:        Mitigation; TSX disabled\n\nVersions of relevant libraries:\n[pip3] numpy==2.1.3\n[pip3] nvidia-cublas-cu12==12.4.5.8\n[pip3] nvidia-cuda-cupti-cu12==12.4.127\n[pip3] nvidia-cuda-nvrtc-cu12==12.4.127\n[pip3] nvidia-cuda-runtime-cu12==12.4.127\n[pip3] nvidia-cudnn-cu12==9.1.0.70\n[pip3] nvidia-cufft-cu12==11.2.1.3\n[pip3] nvidia-curand-cu12==10.3.5.147\n[pip3] nvidia-cusolver-cu12==11.6.1.9\n[pip3] nvidia-cusparse-cu12==12.3.1.170\n[pip3] nvidia-cusparselt-cu12==0.6.2\n[pip3] nvidia-nccl-cu12==2.21.5\n[pip3] nvidia-nvjitlink-cu12==12.4.127\n[pip3] nvidia-nvtx-cu12==12.4.127\n[pip3] torch==2.6.0\n[pip3] torchaudio==2.6.0\n[pip3] torchvision==0.21.0\n[pip3] triton==3.2.0\n[conda] nvidia-cublas-cu12        12.1.3.1                 pypi_0    pypi\n[conda] nvidia-cuda-cupti-cu12    12.1.105                 pypi_0    pypi\n[conda] nvidia-cuda-nvrtc-cu12    12.1.105                 pypi_0    pypi\n[conda] nvidia-cuda-runtime-cu12  12.1.105                 pypi_0    pypi\n[conda] nvidia-cudnn-cu12         8.9.2.26                 pypi_0    pypi\n[conda] nvidia-cufft-cu12         11.0.2.54                pypi_0    pypi\n[conda] nvidia-curand-cu12        10.3.2.106               pypi_0    pypi\n[conda] nvidia-cusolver-cu12      11.4.5.107               pypi_0    pypi\n[conda] nvidia-cusparse-cu12      12.1.0.106               pypi_0    pypi\n[conda] nvidia-nccl-cu12          2.19.3                   pypi_0    pypi\n[conda] nvidia-nvjitlink-cu12     12.4.99                  pypi_0    pypi\n[conda] nvidia-nvtx-cu12          12.1.105                 pypi_0    pypi\n[conda] torch                     2.2.1                    pypi_0    pypi",
    "transformers_version": "4.51.3",
    "lm_eval_version": "0.4.8",
    "upper_git_hash": null
}